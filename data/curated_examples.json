{
  "version": "1.0",
  "examples": [
    {
      "function_name": "get_user",
      "module_path": "api/endpoints.py",
      "function_signature": "def get_user(user_id: str, include_details: bool = False) -> dict[str, Any]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retrieve user information from the database.\n\nArgs:\n    user_id (str): Unique identifier for the user to retrieve.\n    include_details (bool): Whether to include additional user details\n        such as preferences and activity history. Defaults to False.\n\nReturns:\n    dict[str, Any]: User information including id, name, email, and\n        optionally detailed information if requested.\n\nRaises:\n    ValueError: If user_id is empty or invalid format.\n    UserNotFoundError: If no user exists with the given id.\n    DatabaseConnectionError: If unable to connect to the database.\n\nExample:\n    >>> user = get_user(\"12345\")\n    >>> print(user['name'])\n    'John Doe'\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_data",
      "module_path": "async/client.py",
      "function_signature": "async def fetch_data(url: str, timeout: float = 30.0, retries: int = 3) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Asynchronously fetch data from the specified URL.\n\nArgs:\n    url (str): The URL to fetch data from. Must be a valid HTTP/HTTPS URL.\n    timeout (float): Maximum time to wait for response in seconds.\n        Defaults to 30.0.\n    retries (int): Number of retry attempts for failed requests.\n        Defaults to 3.\n\nReturns:\n    Response: The response object containing status code, headers,\n        and body content.\n\nRaises:\n    InvalidURLError: If the URL format is invalid.\n    TimeoutError: If the request exceeds the timeout duration.\n    ConnectionError: If unable to establish connection after all retries.\n\nExample:\n    >>> response = await fetch_data(\"https://api.example.com/data\")\n    >>> data = response.json()\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "database_connection",
      "module_path": "db/context.py",
      "function_signature": "@contextmanager\ndef database_connection(host: str, port: int = 5432, database: str = \"main\") -> Connection",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Context manager for database connections.\n\nArgs:\n    host (str): Database server hostname or IP address.\n    port (int): Port number for database connection. Defaults to 5432.\n    database (str): Name of the database to connect to. Defaults to \"main\".\n\nYields:\n    Connection: Active database connection object that is automatically\n        closed when exiting the context.\n\nRaises:\n    ConnectionError: If unable to establish database connection.\n    AuthenticationError: If credentials are invalid.\n\nExample:\n    >>> with database_connection(\"localhost\") as conn:\n    ...     cursor = conn.cursor()\n    ...     cursor.execute(\"SELECT * FROM users\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "retry",
      "module_path": "utils/decorators.py",
      "function_signature": "def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0) -> Callable",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Decorator to retry function execution on failure.\n\nArgs:\n    max_attempts (int): Maximum number of retry attempts. Defaults to 3.\n    delay (float): Initial delay between retries in seconds. Defaults to 1.0.\n    backoff (float): Multiplier for delay after each retry. Defaults to 2.0.\n\nReturns:\n    Callable: Decorated function that will retry on exceptions.\n\nExample:\n    >>> @retry(max_attempts=5, delay=0.5)\n    ... def unstable_network_call():\n    ...     return requests.get(\"https://api.example.com\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "train_model",
      "module_path": "ml/training.py",
      "function_signature": "def train_model(X: np.ndarray, y: np.ndarray, model_type: str = \"random_forest\", **kwargs) -> Model",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Train a machine learning model on the provided dataset.\n\nParameters\n----------\nX : np.ndarray\n    Feature matrix with shape (n_samples, n_features).\ny : np.ndarray\n    Target values with shape (n_samples,) for regression\n    or classification.\nmodel_type : str, optional\n    Type of model to train. Options are 'random_forest',\n    'gradient_boosting', 'neural_network'. Default is 'random_forest'.\n**kwargs : dict\n    Additional keyword arguments passed to the model constructor.\n    Common options include n_estimators, max_depth, learning_rate.\n\nReturns\n-------\nModel\n    Trained model instance with fit() and predict() methods.\n\nRaises\n------\nValueError\n    If X and y have incompatible shapes.\nNotImplementedError\n    If model_type is not supported.\n\nExamples\n--------\n>>> X_train = np.random.randn(100, 10)\n>>> y_train = np.random.randint(0, 2, 100)\n>>> model = train_model(X_train, y_train, model_type='random_forest')\n>>> predictions = model.predict(X_test)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_multiple_urls",
      "module_path": "async/batch_client.py",
      "function_signature": "async def fetch_multiple_urls(urls: list[str], max_concurrent: int = 10) -> list[Result[dict[str, Any]]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Fetch multiple URLs concurrently with rate limiting.\n\nUses asyncio.Semaphore to limit concurrent requests and prevent\noverwhelming target servers.\n\nArgs:\n    urls: List of URLs to fetch. Each must be a valid HTTP/HTTPS URL.\n    max_concurrent: Maximum number of concurrent requests.\n        Defaults to 10. Must be between 1 and 100.\n\nReturns:\n    List of Result objects in the same order as input URLs.\n    Each Result contains either successful response data or error details.\n    Failed requests will have Result.is_error = True.\n\nRaises:\n    ValueError: If max_concurrent is outside valid range.\n    TypeError: If urls is not a list of strings.\n\nExample:\n    >>> urls = ['https://api1.com/data', 'https://api2.com/info']\n    >>> results = await fetch_multiple_urls(urls, max_concurrent=5)\n    >>> for url, result in zip(urls, results):\n    ...     if result.is_error:\n    ...         print(f'{url} failed: {result.error}')\n    ...     else:\n    ...         print(f'{url} returned: {len(result.data)} bytes')\n\nNote:\n    Connection timeout is 30 seconds per request.\n    Failed requests are not retried automatically.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "process_items_concurrently",
      "module_path": "async/processor.py",
      "function_signature": "async def process_items_concurrently(items: list[T], processor: Callable[[T], Awaitable[R]], batch_size: int = 50) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process items concurrently using asyncio.gather().\n\nSplits items into batches and processes each batch concurrently\nto avoid memory issues with large datasets.\n\nArgs:\n    items: List of items to process.\n    processor: Async function that processes a single item.\n    batch_size: Number of items to process concurrently.\n        Defaults to 50.\n\nReturns:\n    List of processed results in the same order as input items.\n\nRaises:\n    ProcessingError: If any item fails to process.\n    ValueError: If batch_size is less than 1.\n\nExample:\n    >>> async def transform_item(item: dict) -> dict:\n    ...     # Simulate async processing\n    ...     await asyncio.sleep(0.1)\n    ...     return {'id': item['id'], 'processed': True}\n    ...\n    >>> items = [{'id': i} for i in range(100)]\n    >>> results = await process_items_concurrently(items, transform_item)\n    >>> print(f'Processed {len(results)} items')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "rate_limited_api_client",
      "module_path": "async/rate_limiter.py",
      "function_signature": "async def rate_limited_api_client(endpoint: str, requests_per_second: float = 10.0) -> AsyncAPIClient",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create an API client with built-in rate limiting using Semaphore.\n\nImplements token bucket algorithm with async semaphore for\nsmooth rate limiting across concurrent requests.\n\nArgs:\n    endpoint: Base URL of the API endpoint.\n    requests_per_second: Maximum requests allowed per second.\n        Defaults to 10.0. Fractional values supported.\n\nReturns:\n    AsyncAPIClient instance with rate limiting enabled.\n\nRaises:\n    ValueError: If endpoint is invalid or requests_per_second <= 0.\n    ConnectionError: If unable to establish initial connection.\n\nExample:\n    >>> client = await rate_limited_api_client('https://api.example.com', 5.0)\n    >>> # Makes at most 5 requests per second\n    >>> tasks = [client.get(f'/user/{i}') for i in range(20)]\n    >>> results = await asyncio.gather(*tasks)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_database_pool",
      "module_path": "async/db_context.py",
      "function_signature": "@asynccontextmanager\nasync def async_database_pool(dsn: str, min_size: int = 10, max_size: int = 20) -> AsyncIterator[Pool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async context manager for database connection pool.\n\nCreates and manages a pool of database connections that can be\nreused across multiple async operations.\n\nArgs:\n    dsn: Database connection string (Data Source Name).\n    min_size: Minimum number of connections to maintain.\n        Defaults to 10.\n    max_size: Maximum number of connections allowed.\n        Defaults to 20.\n\nYields:\n    Pool: Async connection pool instance.\n\nRaises:\n    ConnectionError: If unable to create initial connections.\n    ValueError: If min_size > max_size.\n\nExample:\n    >>> async with async_database_pool('postgresql://localhost/mydb') as pool:\n    ...     async with pool.acquire() as conn:\n    ...         result = await conn.fetch('SELECT * FROM users')\n    ...         print(f'Found {len(result)} users')\n\nNote:\n    Pool is automatically closed when exiting context.\n    Connections are health-checked before reuse.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "stream_large_file",
      "module_path": "async/file_streaming.py",
      "function_signature": "async def stream_large_file(file_path: Path, chunk_size: int = 8192) -> AsyncIterator[bytes]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async generator for streaming large files in chunks.\n\nReads file in chunks to avoid loading entire file into memory,\nuseful for processing multi-GB files.\n\nArgs:\n    file_path: Path to the file to stream.\n    chunk_size: Size of each chunk in bytes.\n        Defaults to 8192 (8KB).\n\nYields:\n    bytes: File content in chunks.\n\nRaises:\n    FileNotFoundError: If file doesn't exist.\n    PermissionError: If file is not readable.\n    IOError: If read operation fails.\n\nExample:\n    >>> async for chunk in stream_large_file(Path('/data/large.csv')):\n    ...     # Process chunk without loading entire file\n    ...     lines = chunk.decode('utf-8').splitlines()\n    ...     for line in lines:\n    ...         await process_csv_line(line)\n\nNote:\n    File is automatically closed when generator is exhausted or cancelled.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "retry_with_exponential_backoff",
      "module_path": "async/retry_handler.py",
      "function_signature": "async def retry_with_exponential_backoff(coro: Callable[[], Awaitable[T]], max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0, exceptions: tuple[type[Exception], ...] = (Exception,)) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retry async operation with exponential backoff on failure.\n\nImplements exponential backoff with jitter to avoid thundering herd\nproblem when multiple clients retry simultaneously.\n\nArgs:\n    coro: Coroutine function to retry (called without arguments).\n    max_retries: Maximum number of retry attempts.\n        Defaults to 3.\n    base_delay: Initial delay in seconds.\n        Defaults to 1.0.\n    max_delay: Maximum delay between retries.\n        Defaults to 60.0.\n    exceptions: Tuple of exception types to retry on.\n        Defaults to catch all exceptions.\n\nReturns:\n    Result from successful coroutine execution.\n\nRaises:\n    The last exception if all retries fail.\n    ValueError: If max_retries < 0 or delays are invalid.\n\nExample:\n    >>> async def flaky_api_call():\n    ...     response = await http_client.get('/unstable-endpoint')\n    ...     return response.json()\n    ...\n    >>> data = await retry_with_exponential_backoff(\n    ...     flaky_api_call,\n    ...     max_retries=5,\n    ...     exceptions=(ConnectionError, TimeoutError)\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "wait_for_with_timeout",
      "module_path": "async/timeout_utils.py",
      "function_signature": "async def wait_for_with_timeout(coro: Coroutine[Any, Any, T], timeout: float, fallback: T | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute coroutine with timeout using asyncio.timeout().\n\nProvides a clean interface for timeout handling with optional\nfallback value on timeout.\n\nArgs:\n    coro: Coroutine to execute with timeout.\n    timeout: Maximum seconds to wait for completion.\n    fallback: Value to return if timeout occurs.\n        If None, raises TimeoutError.\n\nReturns:\n    Result from coroutine or fallback value on timeout.\n\nRaises:\n    TimeoutError: If timeout occurs and no fallback provided.\n    Any exception raised by the coroutine.\n\nExample:\n    >>> async def slow_computation():\n    ...     await asyncio.sleep(10)\n    ...     return 42\n    ...\n    >>> # Returns -1 if computation takes more than 5 seconds\n    >>> result = await wait_for_with_timeout(\n    ...     slow_computation(),\n    ...     timeout=5.0,\n    ...     fallback=-1\n    ... )\n\nNote:\n    Uses asyncio.timeout() context manager internally (Python 3.11+).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "create_background_tasks",
      "module_path": "async/task_manager.py",
      "function_signature": "async def create_background_tasks(task_specs: list[TaskSpec], supervisor: TaskSupervisor | None = None) -> list[asyncio.Task[Any]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create and manage background tasks using asyncio.create_task().\n\nCreates tasks that run concurrently in the background with optional\nsupervision for error handling and lifecycle management.\n\nArgs:\n    task_specs: List of task specifications containing coroutine\n        and metadata.\n    supervisor: Optional task supervisor for monitoring and\n        error handling.\n\nReturns:\n    List of created asyncio.Task instances.\n\nRaises:\n    ValueError: If task_specs is empty or contains invalid specs.\n    RuntimeError: If event loop is not running.\n\nExample:\n    >>> specs = [\n    ...     TaskSpec(coro=monitor_system(), name='system_monitor'),\n    ...     TaskSpec(coro=cleanup_old_files(), name='cleanup')\n    ... ]\n    >>> tasks = await create_background_tasks(specs)\n    >>> # Tasks now run in background\n    >>> await asyncio.sleep(60)  # Let them run\n    >>> # Cancel all tasks when done\n    >>> for task in tasks:\n    ...     task.cancel()\n\nNote:\n    Tasks continue running until explicitly cancelled or completed.\n    Use TaskSupervisor for automatic restart on failure.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "event_driven_processor",
      "module_path": "async/event_processor.py",
      "function_signature": "async def event_driven_processor(event_queue: asyncio.Queue[Event], handlers: dict[str, EventHandler], stop_event: asyncio.Event | None = None) -> ProcessorStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process events asynchronously using event-driven pattern.\n\nConsumes events from queue and dispatches to appropriate handlers\nbased on event type.\n\nArgs:\n    event_queue: Async queue containing events to process.\n    handlers: Mapping of event types to handler functions.\n    stop_event: Optional event to signal processor shutdown.\n\nReturns:\n    ProcessorStats with counts of processed and failed events.\n\nRaises:\n    ValueError: If handlers dict is empty.\n    ProcessorError: If critical error occurs during processing.\n\nExample:\n    >>> queue = asyncio.Queue()\n    >>> handlers = {\n    ...     'user_signup': handle_signup,\n    ...     'order_placed': handle_order,\n    ...     'payment_received': handle_payment\n    ... }\n    >>> stop = asyncio.Event()\n    >>> # Run processor in background\n    >>> processor_task = asyncio.create_task(\n    ...     event_driven_processor(queue, handlers, stop)\n    ... )\n    >>> # Add events to queue\n    >>> await queue.put(Event(type='user_signup', data={'user_id': 123}))\n    >>> # Stop processor\n    >>> stop.set()\n    >>> stats = await processor_task\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_paginate_results",
      "module_path": "async/pagination.py",
      "function_signature": "async def async_paginate_results(fetch_page: Callable[[int], Awaitable[Page[T]]], start_page: int = 1, max_pages: int | None = None) -> AsyncIterator[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"AsyncIterator for paginating through API results.\n\nAutomatically fetches subsequent pages as iteration continues,\nhandling pagination logic transparently.\n\nArgs:\n    fetch_page: Async function that fetches a specific page number.\n    start_page: Page number to start from. Defaults to 1.\n    max_pages: Maximum number of pages to fetch.\n        None for unlimited.\n\nYields:\n    Individual items from all pages.\n\nRaises:\n    PaginationError: If page fetching fails.\n    ValueError: If start_page < 1.\n\nExample:\n    >>> async def fetch_users_page(page: int) -> Page[User]:\n    ...     response = await api_client.get(f'/users?page={page}')\n    ...     return Page(\n    ...         items=response['users'],\n    ...         has_next=response['has_next']\n    ...     )\n    ...\n    >>> async for user in async_paginate_results(fetch_users_page):\n    ...     print(f'Processing user: {user.name}')\n    ...     if user.inactive:\n    ...         await deactivate_user(user)\n\nNote:\n    Stops iteration when fetch_page returns Page with has_next=False.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_cache_with_ttl",
      "module_path": "async/caching.py",
      "function_signature": "async def async_cache_with_ttl(key: str, fetcher: Callable[[], Awaitable[T]], ttl_seconds: float = 300.0, cache: AsyncCache | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Get value from async cache or fetch if missing/expired.\n\nImplements read-through caching pattern with TTL support for\nasync operations.\n\nArgs:\n    key: Cache key to lookup.\n    fetcher: Async function to fetch value if not cached.\n    ttl_seconds: Time-to-live in seconds. Defaults to 300 (5 minutes).\n    cache: Optional cache instance. Uses default if None.\n\nReturns:\n    Cached value or freshly fetched value.\n\nRaises:\n    CacheError: If cache operation fails.\n    Any exception raised by fetcher.\n\nExample:\n    >>> async def expensive_api_call():\n    ...     response = await external_api.get('/expensive-data')\n    ...     return response.json()\n    ...\n    >>> # First call fetches from API\n    >>> data = await async_cache_with_ttl(\n    ...     'api_data',\n    ...     expensive_api_call,\n    ...     ttl_seconds=600\n    ... )\n    >>> # Subsequent calls within 10 minutes use cache\n    >>> cached_data = await async_cache_with_ttl('api_data', expensive_api_call)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "parallel_map_async",
      "module_path": "async/parallel.py",
      "function_signature": "async def parallel_map_async(func: Callable[[T], Awaitable[R]], items: Iterable[T], max_workers: int = 10, ordered: bool = True) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Apply async function to items in parallel with worker limit.\n\nSimilar to map() but executes async function on multiple items\nconcurrently with controlled parallelism.\n\nArgs:\n    func: Async function to apply to each item.\n    items: Iterable of items to process.\n    max_workers: Maximum concurrent workers. Defaults to 10.\n    ordered: If True, preserve input order in results.\n        Defaults to True.\n\nReturns:\n    List of results from applying func to each item.\n\nRaises:\n    ParallelExecutionError: If any item fails to process.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def fetch_user_details(user_id: int) -> dict:\n    ...     response = await api_client.get(f'/users/{user_id}')\n    ...     return response.json()\n    ...\n    >>> user_ids = [101, 102, 103, 104, 105]\n    >>> details = await parallel_map_async(\n    ...     fetch_user_details,\n    ...     user_ids,\n    ...     max_workers=3\n    ... )\n    >>> print(f'Fetched details for {len(details)} users')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_circuit_breaker",
      "module_path": "async/resilience.py",
      "function_signature": "async def async_circuit_breaker(func: Callable[..., Awaitable[T]], failure_threshold: int = 5, timeout: float = 60.0, half_open_max_calls: int = 1) -> Callable[..., Awaitable[T]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Wrap async function with circuit breaker pattern.\n\nPrevents cascading failures by stopping calls to failing services\ntemporarily.\n\nArgs:\n    func: Async function to protect with circuit breaker.\n    failure_threshold: Failures before opening circuit.\n        Defaults to 5.\n    timeout: Seconds before attempting reset. Defaults to 60.0.\n    half_open_max_calls: Test calls in half-open state.\n        Defaults to 1.\n\nReturns:\n    Wrapped function with circuit breaker behavior.\n\nRaises:\n    CircuitOpenError: When circuit is open and calls are rejected.\n    Original exceptions from func when circuit is closed.\n\nExample:\n    >>> @async_circuit_breaker(failure_threshold=3, timeout=30.0)\n    ... async def unstable_service_call(endpoint: str) -> dict:\n    ...     response = await http_client.get(endpoint)\n    ...     return response.json()\n    ...\n    >>> try:\n    ...     result = await unstable_service_call('/api/data')\n    ... except CircuitOpenError:\n    ...     # Use fallback when circuit is open\n    ...     result = get_cached_data()\n\nNote:\n    Circuit states: CLOSED (normal) -> OPEN (failing) -> HALF_OPEN (testing).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_pub_sub",
      "module_path": "async/pubsub.py",
      "function_signature": "async def async_pub_sub(topic: str, message: Any | None = None, subscribe: bool = False) -> AsyncIterator[Message] | None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async publish-subscribe pattern implementation.\n\nAllows publishing messages to topics and subscribing to receive\nmessages asynchronously.\n\nArgs:\n    topic: Topic name for publish/subscribe.\n    message: Message to publish. None when subscribing.\n    subscribe: If True, returns subscription iterator.\n        If False, publishes message.\n\nReturns:\n    AsyncIterator[Message] when subscribing, None when publishing.\n\nRaises:\n    ValueError: If publishing with subscribe=True or vice versa.\n    TopicError: If topic doesn't exist when subscribing.\n\nExample:\n    >>> # Publisher\n    >>> await async_pub_sub('user_events', {'type': 'login', 'user_id': 123})\n    ...\n    >>> # Subscriber\n    >>> async for msg in async_pub_sub('user_events', subscribe=True):\n    ...     if msg.data['type'] == 'login':\n    ...         await track_user_login(msg.data['user_id'])\n    ...     elif msg.data['type'] == 'logout':\n    ...         await track_user_logout(msg.data['user_id'])\n\nNote:\n    Subscribers receive messages published after subscription starts.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_middleware_chain",
      "module_path": "async/middleware.py",
      "function_signature": "async def async_middleware_chain(request: Request, middlewares: list[Middleware], handler: RequestHandler) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute async middleware chain for request processing.\n\nProcesses request through multiple middleware layers before\nreaching the final handler.\n\nArgs:\n    request: Incoming request to process.\n    middlewares: Ordered list of middleware to apply.\n    handler: Final request handler after all middleware.\n\nReturns:\n    Response after processing through all layers.\n\nRaises:\n    MiddlewareError: If any middleware fails.\n    ValueError: If middlewares list is empty.\n\nExample:\n    >>> middlewares = [\n    ...     AuthenticationMiddleware(),\n    ...     RateLimitMiddleware(requests_per_minute=100),\n    ...     LoggingMiddleware(),\n    ...     CompressionMiddleware()\n    ... ]\n    >>> async def api_handler(request: Request) -> Response:\n    ...     data = await process_request(request)\n    ...     return Response(data=data)\n    ...\n    >>> response = await async_middleware_chain(\n    ...     request,\n    ...     middlewares,\n    ...     api_handler\n    ... )\n\nNote:\n    Middleware can modify request before and response after handler.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_batch_processor",
      "module_path": "async/batch.py",
      "function_signature": "async def async_batch_processor(items: AsyncIterator[T], batch_size: int = 100, processor: Callable[[list[T]], Awaitable[None]] | None = None) -> BatchStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process async iterator items in batches.\n\nAccumulates items into batches and processes them together\nfor improved efficiency.\n\nArgs:\n    items: Async iterator producing items to process.\n    batch_size: Number of items per batch. Defaults to 100.\n    processor: Optional batch processing function.\n        If None, uses default processor.\n\nReturns:\n    BatchStats with total items and batches processed.\n\nRaises:\n    BatchProcessingError: If batch processing fails.\n    ValueError: If batch_size < 1.\n\nExample:\n    >>> async def stream_records() -> AsyncIterator[Record]:\n    ...     async for record in database.stream('SELECT * FROM large_table'):\n    ...         yield record\n    ...\n    >>> async def save_batch(records: list[Record]) -> None:\n    ...     await bulk_insert(records)\n    ...\n    >>> stats = await async_batch_processor(\n    ...     stream_records(),\n    ...     batch_size=1000,\n    ...     processor=save_batch\n    ... )\n    >>> print(f'Processed {stats.total_items} items in {stats.total_batches} batches')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_resource_pool",
      "module_path": "async/resource_pool.py",
      "function_signature": "async def async_resource_pool(resource_factory: Callable[[], Awaitable[T]], min_size: int = 1, max_size: int = 10, timeout: float = 30.0) -> AsyncResourcePool[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create async pool for expensive resource management.\n\nManages a pool of reusable resources (connections, clients, etc.)\nwith automatic creation and cleanup.\n\nArgs:\n    resource_factory: Async function to create new resources.\n    min_size: Minimum resources to maintain. Defaults to 1.\n    max_size: Maximum resources allowed. Defaults to 10.\n    timeout: Seconds to wait for available resource.\n        Defaults to 30.0.\n\nReturns:\n    AsyncResourcePool instance for resource management.\n\nRaises:\n    PoolError: If unable to create minimum resources.\n    ValueError: If min_size > max_size or sizes < 1.\n\nExample:\n    >>> async def create_client() -> APIClient:\n    ...     client = APIClient()\n    ...     await client.connect()\n    ...     return client\n    ...\n    >>> pool = await async_resource_pool(\n    ...     create_client,\n    ...     min_size=5,\n    ...     max_size=20\n    ... )\n    >>> async with pool.acquire() as client:\n    ...     result = await client.make_request('/api/data')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_state_machine",
      "module_path": "async/state_machine.py",
      "function_signature": "async def async_state_machine(initial_state: State, transitions: dict[tuple[State, Event], State], event_stream: AsyncIterator[Event]) -> State",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async finite state machine for event processing.\n\nProcesses events from async stream and transitions between states\naccording to defined rules.\n\nArgs:\n    initial_state: Starting state of the machine.\n    transitions: Mapping of (state, event) tuples to next state.\n    event_stream: Async iterator of events to process.\n\nReturns:\n    Final state when event stream is exhausted.\n\nRaises:\n    InvalidTransitionError: If event has no valid transition from current state.\n    StateError: If state machine enters invalid state.\n\nExample:\n    >>> transitions = {\n    ...     (State.IDLE, Event.START): State.RUNNING,\n    ...     (State.RUNNING, Event.PAUSE): State.PAUSED,\n    ...     (State.PAUSED, Event.RESUME): State.RUNNING,\n    ...     (State.RUNNING, Event.COMPLETE): State.DONE\n    ... }\n    >>> async def job_events() -> AsyncIterator[Event]:\n    ...     yield Event.START\n    ...     await asyncio.sleep(1)\n    ...     yield Event.COMPLETE\n    ...\n    >>> final_state = await async_state_machine(\n    ...     State.IDLE,\n    ...     transitions,\n    ...     job_events()\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_health_monitor",
      "module_path": "async/monitoring.py",
      "function_signature": "async def async_health_monitor(services: list[Service], check_interval: float = 60.0, alert_handler: AlertHandler | None = None) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Monitor service health with async health checks.\n\nPeriodically checks health of multiple services and triggers\nalerts on failures.\n\nArgs:\n    services: List of services to monitor.\n    check_interval: Seconds between health checks.\n        Defaults to 60.0.\n    alert_handler: Optional handler for health alerts.\n\nRaises:\n    MonitoringError: If monitoring setup fails.\n    ValueError: If services list is empty or interval <= 0.\n\nExample:\n    >>> services = [\n    ...     Service(name='api', health_check=check_api_health),\n    ...     Service(name='database', health_check=check_db_health),\n    ...     Service(name='cache', health_check=check_cache_health)\n    ... ]\n    >>> async def send_alert(service: str, status: str):\n    ...     await notification_service.send(f'{service} is {status}')\n    ...\n    >>> # Run monitor in background\n    >>> monitor_task = asyncio.create_task(\n    ...     async_health_monitor(\n    ...         services,\n    ...         check_interval=30.0,\n    ...         alert_handler=send_alert\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Each service checked concurrently.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_data_pipeline",
      "module_path": "async/pipeline.py",
      "function_signature": "async def async_data_pipeline(source: AsyncIterator[T], transformers: list[Transformer[Any, Any]], sink: Sink[Any]) -> PipelineStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Build async data processing pipeline.\n\nStreams data through a series of transformations from source to sink\nwith backpressure handling.\n\nArgs:\n    source: Async iterator producing input data.\n    transformers: Ordered list of data transformers.\n    sink: Final destination for processed data.\n\nReturns:\n    PipelineStats with processing metrics.\n\nRaises:\n    PipelineError: If any stage fails.\n    ValueError: If transformers list is empty.\n\nExample:\n    >>> async def read_logs() -> AsyncIterator[str]:\n    ...     async for line in tail_file('/var/log/app.log'):\n    ...         yield line\n    ...\n    >>> transformers = [\n    ...     ParseJsonTransformer(),\n    ...     FilterErrorsTransformer(min_level='ERROR'),\n    ...     EnrichWithMetadataTransformer(),\n    ...     FormatForStorageTransformer()\n    ... ]\n    >>> sink = ElasticsearchSink(index='app-errors')\n    >>> stats = await async_data_pipeline(\n    ...     read_logs(),\n    ...     transformers,\n    ...     sink\n    ... )\n    >>> print(f'Processed {stats.total_records} records')\n\nNote:\n    Pipeline handles backpressure to prevent memory overflow.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_distributed_lock",
      "module_path": "async/distributed.py",
      "function_signature": "async def async_distributed_lock(resource_id: str, ttl: float = 30.0, redis_client: AsyncRedis | None = None) -> AsyncContextManager[bool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Acquire distributed lock using Redis for coordination.\n\nEnsures only one process can access a resource across multiple\nservers or instances.\n\nArgs:\n    resource_id: Unique identifier for the resource to lock.\n    ttl: Lock time-to-live in seconds. Defaults to 30.0.\n    redis_client: Optional Redis client. Uses default if None.\n\nYields:\n    bool: True if lock acquired, False otherwise.\n\nRaises:\n    LockError: If lock mechanism fails.\n    ConnectionError: If Redis connection fails.\n\nExample:\n    >>> async with async_distributed_lock('user:123:profile', ttl=60.0) as locked:\n    ...     if locked:\n    ...         # Exclusive access to update user profile\n    ...         await update_user_profile(123, new_data)\n    ...     else:\n    ...         # Another process has the lock\n    ...         raise ConcurrentModificationError('Profile is being updated')\n\nNote:\n    Lock is automatically released when context exits or TTL expires.\n    Uses Redis SET with NX and EX options for atomic acquisition.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_websocket_handler",
      "module_path": "async/websocket.py",
      "function_signature": "async def async_websocket_handler(websocket: WebSocket, message_handler: Callable[[Message], Awaitable[Response]], heartbeat_interval: float = 30.0) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Handle WebSocket connection with async message processing.\n\nManages WebSocket lifecycle including heartbeat, message handling,\nand graceful disconnection.\n\nArgs:\n    websocket: Active WebSocket connection.\n    message_handler: Async function to process incoming messages.\n    heartbeat_interval: Seconds between heartbeat pings.\n        Defaults to 30.0.\n\nRaises:\n    WebSocketError: If connection fails.\n    ValueError: If heartbeat_interval <= 0.\n\nExample:\n    >>> async def handle_chat_message(msg: Message) -> Response:\n    ...     if msg.type == 'chat':\n    ...         # Broadcast to other users\n    ...         await broadcast_message(msg.data)\n    ...         return Response(status='delivered')\n    ...     elif msg.type == 'typing':\n    ...         await notify_typing(msg.sender)\n    ...         return Response(status='ok')\n    ...\n    >>> @app.websocket('/ws/chat')\n    ... async def chat_endpoint(websocket: WebSocket):\n    ...     await websocket.accept()\n    ...     await async_websocket_handler(\n    ...         websocket,\n    ...         handle_chat_message\n    ...     )\n\nNote:\n    Automatically handles ping/pong for connection keep-alive.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_queue_consumer",
      "module_path": "async/queue.py",
      "function_signature": "async def async_queue_consumer(queue_name: str, handler: Callable[[QueueMessage], Awaitable[None]], max_workers: int = 5, auto_ack: bool = True) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Consume messages from async queue with concurrent processing.\n\nContinuously pulls messages from queue and processes them using\nmultiple concurrent workers.\n\nArgs:\n    queue_name: Name of the queue to consume from.\n    handler: Async function to process each message.\n    max_workers: Maximum concurrent message processors.\n        Defaults to 5.\n    auto_ack: Whether to auto-acknowledge messages.\n        Defaults to True.\n\nRaises:\n    QueueError: If queue connection fails.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def process_order(message: QueueMessage) -> None:\n    ...     order = json.loads(message.body)\n    ...     await validate_order(order)\n    ...     await charge_payment(order['payment_info'])\n    ...     await ship_order(order['items'])\n    ...     if not message.auto_acked:\n    ...         await message.ack()\n    ...\n    >>> # Run consumer in background\n    >>> consumer_task = asyncio.create_task(\n    ...     async_queue_consumer(\n    ...         'order_queue',\n    ...         process_order,\n    ...         max_workers=10\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Failed messages are requeued.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_parallel_aggregate",
      "module_path": "async/aggregation.py",
      "function_signature": "async def async_parallel_aggregate(data_sources: list[DataSource], aggregator: Aggregator, timeout_per_source: float = 10.0) -> AggregateResult",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Aggregate data from multiple async sources in parallel.\n\nFetches data from multiple sources concurrently and combines\nresults using specified aggregation strategy.\n\nArgs:\n    data_sources: List of async data sources to query.\n    aggregator: Strategy for combining results.\n    timeout_per_source: Max seconds per source.\n        Defaults to 10.0.\n\nReturns:\n    AggregateResult containing combined data and metadata.\n\nRaises:\n    AggregationError: If aggregation fails.\n    PartialResultError: If some sources fail but partial_ok=True.\n\nExample:\n    >>> sources = [\n    ...     MetricsAPISource('cpu_usage'),\n    ...     MetricsAPISource('memory_usage'),\n    ...     MetricsAPISource('disk_usage')\n    ... ]\n    >>> aggregator = AverageAggregator(time_window='5m')\n    >>> result = await async_parallel_aggregate(\n    ...     sources,\n    ...     aggregator,\n    ...     timeout_per_source=5.0\n    ... )\n    >>> print(f'System health score: {result.value}')\n    >>> if result.failed_sources:\n    ...     print(f'Warning: {len(result.failed_sources)} sources failed')\n\nNote:\n    Failed sources don't stop aggregation if aggregator supports partial results.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_scheduled_task",
      "module_path": "async/scheduler.py",
      "function_signature": "async def async_scheduled_task(task_func: Callable[[], Awaitable[None]], schedule: str | CronSchedule, timezone: str = 'UTC', max_instances: int = 1) -> ScheduledTask",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Schedule async task execution with cron-like syntax.\n\nRuns async tasks on a schedule with instance limiting to prevent\noverlapping executions.\n\nArgs:\n    task_func: Async function to execute on schedule.\n    schedule: Cron expression or CronSchedule object.\n        Examples: '0 * * * *' (hourly), '*/5 * * * *' (every 5 min).\n    timezone: Timezone for schedule. Defaults to 'UTC'.\n    max_instances: Maximum concurrent executions.\n        Defaults to 1.\n\nReturns:\n    ScheduledTask instance for management.\n\nRaises:\n    ScheduleError: If schedule syntax is invalid.\n    ValueError: If max_instances < 1.\n\nExample:\n    >>> async def cleanup_old_files():\n    ...     cutoff = datetime.now() - timedelta(days=7)\n    ...     async for file in scan_temp_directory():\n    ...         if file.modified < cutoff:\n    ...             await file.delete()\n    ...\n    >>> # Run cleanup daily at 2 AM\n    >>> task = await async_scheduled_task(\n    ...     cleanup_old_files,\n    ...     schedule='0 2 * * *',\n    ...     timezone='America/New_York'\n    ... )\n    >>> # Task now runs automatically\n    >>> # Stop scheduling\n    >>> await task.stop()\n\nNote:\n    Task continues running until explicitly stopped.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "update_user",
      "module_path": "api/users.py",
      "function_signature": "def update_user(user_id: str, user_data: UserUpdateSchema, current_user: User = Depends(get_current_user)) -> UserResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Update user information.\n\nPartially updates user data based on provided fields. Only non-null\nfields in user_data will be updated.\n\nArgs:\n    user_id: Unique identifier of the user to update.\n    user_data: Schema containing fields to update.\n        Optional fields: name, email, phone, preferences.\n    current_user: Authenticated user making the request.\n        Automatically injected by FastAPI.\n\nReturns:\n    UserResponse: Updated user data with all fields.\n\nRaises:\n    HTTPException: Status 404 if user not found.\n    HTTPException: Status 403 if current user lacks permission.\n    HTTPException: Status 400 if email already taken.\n    ValidationError: If user_data contains invalid fields.\n\nExample:\n    Request:\n        PATCH /users/123\n        Body: {\"name\": \"Jane Doe\", \"email\": \"jane@example.com\"}\n    \n    Response:\n        200 OK\n        {\n            \"id\": \"123\",\n            \"name\": \"Jane Doe\",\n            \"email\": \"jane@example.com\",\n            \"created_at\": \"2024-01-01T00:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "list_products",
      "module_path": "api/products.py",
      "function_signature": "def list_products(page: int = Query(1, ge=1), limit: int = Query(20, ge=1, le=100), category: str | None = Query(None), sort_by: str = Query('created_at'), order: str = Query('desc', regex='^(asc|desc)$')) -> PaginatedResponse[Product]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"List products with pagination, filtering, and sorting.\n\nReturns paginated list of products with optional filtering by category\nand configurable sorting.\n\nArgs:\n    page: Page number starting from 1. Defaults to 1.\n    limit: Number of items per page. Defaults to 20, max 100.\n    category: Optional category filter. Case-insensitive partial match.\n    sort_by: Field to sort by. Options: 'created_at', 'price', 'name'.\n        Defaults to 'created_at'.\n    order: Sort order. Must be 'asc' or 'desc'. Defaults to 'desc'.\n\nReturns:\n    PaginatedResponse containing:\n        - items: List of Product objects for current page\n        - total: Total number of products matching filters\n        - page: Current page number\n        - pages: Total number of pages\n        - has_next: Whether next page exists\n        - has_prev: Whether previous page exists\n\nRaises:\n    HTTPException: Status 400 if sort_by field is invalid.\n    ValidationError: If query parameters fail validation.\n\nExample:\n    Request:\n        GET /products?page=2&limit=10&category=electronics&sort_by=price&order=asc\n    \n    Response:\n        200 OK\n        {\n            \"items\": [{\"id\": \"123\", \"name\": \"USB Cable\", \"price\": 9.99}],\n            \"total\": 45,\n            \"page\": 2,\n            \"pages\": 5,\n            \"has_next\": true,\n            \"has_prev\": true\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "create_order",
      "module_path": "api/orders.py",
      "function_signature": "def create_order(order_data: CreateOrderSchema, user: User = Depends(get_current_user), db: Session = Depends(get_db)) -> OrderResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create a new order.\n\nValidates order data, checks inventory, calculates totals, and creates\norder with line items in a transaction.\n\nArgs:\n    order_data: Order creation data containing:\n        - items: List of {product_id, quantity} objects\n        - shipping_address: Address information\n        - payment_method: Payment method identifier\n        - coupon_code: Optional discount code\n    user: Current authenticated user creating the order.\n    db: Database session for transaction management.\n\nReturns:\n    OrderResponse with:\n        - order_id: Unique order identifier\n        - status: Order status ('pending', 'processing', etc.)\n        - total: Total amount including tax and shipping\n        - items: Detailed line items with prices\n        - estimated_delivery: Delivery date estimate\n\nRaises:\n    HTTPException: Status 400 if insufficient inventory.\n    HTTPException: Status 400 if invalid coupon code.\n    HTTPException: Status 402 if payment validation fails.\n    HTTPException: Status 422 if order validation fails.\n\nExample:\n    Request:\n        POST /orders\n        Body: {\n            \"items\": [{\"product_id\": \"ABC123\", \"quantity\": 2}],\n            \"shipping_address\": {\"street\": \"123 Main St\", \"city\": \"Boston\"},\n            \"payment_method\": \"pm_1234567890\"\n        }\n    \n    Response:\n        201 Created\n        {\n            \"order_id\": \"ORD-2024-0001\",\n            \"status\": \"pending\",\n            \"total\": 59.98,\n            \"estimated_delivery\": \"2024-01-15\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "delete_resource",
      "module_path": "api/resources.py",
      "function_signature": "def delete_resource(resource_id: UUID, soft_delete: bool = Query(True), reason: str | None = Query(None, max_length=500), admin_user: AdminUser = Depends(require_admin)) -> DeleteResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Delete a resource with soft/hard delete options.\n\nSupports both soft delete (marking as deleted) and hard delete\n(permanent removal) with audit logging.\n\nArgs:\n    resource_id: UUID of the resource to delete.\n    soft_delete: If True, marks as deleted but retains data.\n        If False, permanently removes. Defaults to True.\n    reason: Optional deletion reason for audit log.\n        Max 500 characters.\n    admin_user: Admin user performing deletion.\n        Must have delete permissions.\n\nReturns:\n    DeleteResponse containing:\n        - resource_id: Deleted resource ID\n        - deleted_at: Timestamp of deletion\n        - soft_deleted: Whether soft delete was used\n        - recoverable: Whether resource can be restored\n\nRaises:\n    HTTPException: Status 404 if resource not found.\n    HTTPException: Status 403 if user lacks delete permission.\n    HTTPException: Status 409 if resource has dependencies.\n    HTTPException: Status 410 if resource already deleted.\n\nExample:\n    Request:\n        DELETE /resources/550e8400-e29b-41d4-a716-446655440000?soft_delete=true&reason=User%20requested\n    \n    Response:\n        200 OK\n        {\n            \"resource_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n            \"deleted_at\": \"2024-01-10T15:30:00Z\",\n            \"soft_deleted\": true,\n            \"recoverable\": true\n        }\n\nNote:\n    Soft deleted resources can be restored within 30 days.\n    Hard delete requires additional confirmation header.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "upload_file",
      "module_path": "api/files.py",
      "function_signature": "async def upload_file(file: UploadFile = File(...), purpose: str = Form(...), metadata: str | None = Form(None), user: User = Depends(get_current_user)) -> FileUploadResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Upload a file with validation and metadata.\n\nHandles file upload with type validation, virus scanning, and\nmetadata storage. Supports images, documents, and data files.\n\nArgs:\n    file: File to upload. Max size 100MB.\n        Allowed types: jpg, png, pdf, csv, xlsx.\n    purpose: Purpose of upload. Options: 'avatar', 'document',\n        'import', 'attachment'.\n    metadata: Optional JSON metadata string.\n        Max 1KB when parsed.\n    user: User uploading the file.\n\nReturns:\n    FileUploadResponse with:\n        - file_id: Unique file identifier\n        - filename: Original filename\n        - size: File size in bytes\n        - content_type: MIME type\n        - url: Download URL\n        - thumbnail_url: Thumbnail URL for images\n        - expires_at: URL expiration time\n\nRaises:\n    HTTPException: Status 400 if file type not allowed.\n    HTTPException: Status 413 if file too large.\n    HTTPException: Status 422 if file is corrupted.\n    HTTPException: Status 507 if storage quota exceeded.\n\nExample:\n    Request:\n        POST /files/upload\n        Content-Type: multipart/form-data\n        \n        ------WebKitFormBoundary\n        Content-Disposition: form-data; name=\"file\"; filename=\"report.pdf\"\n        Content-Type: application/pdf\n        \n        [PDF binary data]\n        ------WebKitFormBoundary\n        Content-Disposition: form-data; name=\"purpose\"\n        \n        document\n        ------WebKitFormBoundary--\n    \n    Response:\n        201 Created\n        {\n            \"file_id\": \"file_ABC123\",\n            \"filename\": \"report.pdf\",\n            \"size\": 1048576,\n            \"content_type\": \"application/pdf\",\n            \"url\": \"https://api.example.com/files/file_ABC123/download\",\n            \"expires_at\": \"2024-01-11T00:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "search_users",
      "module_path": "api/search.py",
      "function_signature": "def search_users(q: str = Query(..., min_length=2), fields: list[str] = Query(['name', 'email']), filters: UserFilters = Depends(), limit: int = Query(20, le=100)) -> SearchResponse[UserSearchResult]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Search users with full-text search and filtering.\n\nPerforms fuzzy search across specified fields with optional filters\nand relevance scoring.\n\nArgs:\n    q: Search query string. Minimum 2 characters.\n        Supports wildcards (*) and quoted phrases.\n    fields: Fields to search in. Defaults to ['name', 'email'].\n        Available: 'name', 'email', 'bio', 'skills'.\n    filters: Additional filters injected via dependency:\n        - role: Filter by user role\n        - active: Filter by active status\n        - created_after: Users created after date\n        - department: Filter by department\n    limit: Maximum results to return. Default 20, max 100.\n\nReturns:\n    SearchResponse containing:\n        - results: List of UserSearchResult with relevance scores\n        - total_hits: Total number of matching users\n        - query_time_ms: Search execution time\n        - facets: Result counts by category\n\nRaises:\n    HTTPException: Status 400 if invalid field specified.\n    HTTPException: Status 400 if query syntax invalid.\n\nExample:\n    Request:\n        GET /users/search?q=john*&fields=name&fields=email&filters[role]=admin&limit=10\n    \n    Response:\n        200 OK\n        {\n            \"results\": [\n                {\n                    \"user\": {\"id\": \"123\", \"name\": \"John Smith\", \"email\": \"john@example.com\"},\n                    \"score\": 0.95,\n                    \"highlights\": {\"name\": \"<em>John</em> Smith\"}\n                }\n            ],\n            \"total_hits\": 3,\n            \"query_time_ms\": 12,\n            \"facets\": {\"role\": {\"admin\": 1, \"user\": 2}}\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "authenticate",
      "module_path": "api/auth.py",
      "function_signature": "def authenticate(credentials: OAuth2PasswordRequestForm = Depends(), client_info: ClientInfo = Depends(get_client_info), db: Session = Depends(get_db)) -> TokenResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Authenticate user and issue access tokens.\n\nValidates credentials, checks account status, and issues JWT tokens\nwith refresh token rotation.\n\nArgs:\n    credentials: OAuth2 form data containing:\n        - username: User's email or username\n        - password: User's password\n        - scope: Optional requested scopes\n        - grant_type: Must be 'password'\n    client_info: Client information from headers:\n        - user_agent: Client application info\n        - ip_address: Client IP for security logging\n    db: Database session for user lookup.\n\nReturns:\n    TokenResponse with:\n        - access_token: JWT access token\n        - refresh_token: Opaque refresh token\n        - token_type: Always 'bearer'\n        - expires_in: Access token lifetime in seconds\n        - scope: Granted scopes\n\nRaises:\n    HTTPException: Status 401 if credentials invalid.\n    HTTPException: Status 401 if account locked/disabled.\n    HTTPException: Status 429 if too many failed attempts.\n    HTTPException: Status 503 if auth service unavailable.\n\nExample:\n    Request:\n        POST /auth/token\n        Content-Type: application/x-www-form-urlencoded\n        \n        username=user@example.com&password=SecurePass123&grant_type=password\n    \n    Response:\n        200 OK\n        {\n            \"access_token\": \"eyJhbGciOiJIUzI1NiIs...\",\n            \"refresh_token\": \"7f3a9c2d5e1b4a8f...\",\n            \"token_type\": \"bearer\",\n            \"expires_in\": 3600,\n            \"scope\": \"read write\"\n        }\n\nNote:\n    Failed attempts are rate-limited per IP and username.\n    Refresh tokens expire after 30 days of inactivity.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "bulk_create_records",
      "module_path": "api/bulk.py",
      "function_signature": "async def bulk_create_records(records: list[RecordCreateSchema], validate_all: bool = Query(True), transaction_mode: str = Query('all_or_none'), background_tasks: BackgroundTasks = BackgroundTasks()) -> BulkCreateResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Bulk create multiple records in a single request.\n\nEfficiently creates multiple records with validation and transaction\ncontrol. Supports partial success modes.\n\nArgs:\n    records: List of records to create. Max 1000 items.\n        Each must pass individual validation.\n    validate_all: If True, validate all records before creating any.\n        If False, create valid records and report errors.\n        Defaults to True.\n    transaction_mode: Transaction behavior:\n        - 'all_or_none': Rollback if any record fails\n        - 'best_effort': Create valid records, skip invalid\n        Defaults to 'all_or_none'.\n    background_tasks: FastAPI background tasks for post-processing.\n\nReturns:\n    BulkCreateResponse containing:\n        - created: List of successfully created record IDs\n        - failed: List of failed records with error details\n        - total_processed: Number of records processed\n        - transaction_id: Unique ID for this bulk operation\n\nRaises:\n    HTTPException: Status 400 if validate_all=True and any validation fails.\n    HTTPException: Status 413 if more than 1000 records.\n    HTTPException: Status 507 if database capacity exceeded.\n\nExample:\n    Request:\n        POST /records/bulk?validate_all=false&transaction_mode=best_effort\n        Body: [\n            {\"name\": \"Record 1\", \"value\": 100},\n            {\"name\": \"Record 2\", \"value\": \"invalid\"},\n            {\"name\": \"Record 3\", \"value\": 300}\n        ]\n    \n    Response:\n        207 Multi-Status\n        {\n            \"created\": [\"id_1\", \"id_3\"],\n            \"failed\": [\n                {\n                    \"index\": 1,\n                    \"error\": \"value must be numeric\",\n                    \"record\": {\"name\": \"Record 2\", \"value\": \"invalid\"}\n                }\n            ],\n            \"total_processed\": 3,\n            \"transaction_id\": \"bulk_20240110_abc123\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "get_user_profile",
      "module_path": "api/profiles.py",
      "function_signature": "def get_user_profile(user_id: int = Path(..., ge=1), include_stats: bool = Query(False), cache_control: str | None = Header(None)) -> UserProfileResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Get detailed user profile by ID.\n\nRetrieves user profile with optional statistics and proper cache headers.\n\nArgs:\n    user_id: User ID to retrieve. Must be positive integer.\n    include_stats: Whether to include usage statistics.\n        Adds extra database queries. Defaults to False.\n    cache_control: Client cache control header.\n        Respects 'no-cache' and 'max-age' directives.\n\nReturns:\n    UserProfileResponse with:\n        - user: Core user information\n        - profile: Extended profile data\n        - stats: Usage statistics (if requested)\n        - last_updated: Profile modification timestamp\n\nRaises:\n    HTTPException: Status 404 if user not found.\n    HTTPException: Status 403 if profile is private.\n\nExample:\n    Request:\n        GET /users/123/profile?include_stats=true\n        Headers:\n            Cache-Control: max-age=300\n    \n    Response:\n        200 OK\n        Headers:\n            Cache-Control: public, max-age=300\n            ETag: \"33a64df551\"\n            Last-Modified: Mon, 10 Jan 2024 10:00:00 GMT\n        Body:\n        {\n            \"user\": {\"id\": 123, \"username\": \"johndoe\"},\n            \"profile\": {\"bio\": \"Software developer\", \"location\": \"NYC\"},\n            \"stats\": {\"posts_count\": 42, \"followers_count\": 150},\n            \"last_updated\": \"2024-01-10T10:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "update_settings",
      "module_path": "api/settings.py",
      "function_signature": "def update_settings(settings: SettingsUpdateSchema, setting_type: str = Path(..., regex='^(user|app|notification)$'), user: User = Depends(get_current_user), if_match: str | None = Header(None)) -> SettingsResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Update user or application settings.\n\nApplies partial updates to settings with optimistic locking support\nvia ETag headers.\n\nArgs:\n    settings: Settings to update. Only provided fields are modified.\n        Supports nested updates via dot notation.\n    setting_type: Type of settings to update.\n        Must be 'user', 'app', or 'notification'.\n    user: Current authenticated user.\n    if_match: Optional ETag for optimistic locking.\n        Prevents concurrent modification conflicts.\n\nReturns:\n    SettingsResponse containing:\n        - settings: Updated settings object\n        - version: New version number\n        - etag: New ETag for future updates\n        - applied_changes: List of fields that were modified\n\nRaises:\n    HTTPException: Status 400 if invalid setting key.\n    HTTPException: Status 409 if ETag mismatch (concurrent update).\n    HTTPException: Status 422 if setting value fails validation.\n\nExample:\n    Request:\n        PUT /settings/notification\n        Headers:\n            If-Match: \"a3c9f021\"\n        Body:\n        {\n            \"email_notifications\": true,\n            \"notification_frequency\": \"daily\",\n            \"channels.slack.enabled\": false\n        }\n    \n    Response:\n        200 OK\n        Headers:\n            ETag: \"b4d8e132\"\n        Body:\n        {\n            \"settings\": {\n                \"email_notifications\": true,\n                \"notification_frequency\": \"daily\",\n                \"channels\": {\"slack\": {\"enabled\": false}, \"email\": {\"enabled\": true}}\n            },\n            \"version\": 5,\n            \"etag\": \"b4d8e132\",\n            \"applied_changes\": [\"email_notifications\", \"notification_frequency\", \"channels.slack.enabled\"]\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "stream_events",
      "module_path": "api/events.py",
      "function_signature": "async def stream_events(event_types: list[str] = Query(['*']), since: datetime | None = Query(None), user: User = Depends(get_current_user)) -> EventSourceResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Stream real-time events using Server-Sent Events (SSE).\n\nEstablishes long-lived connection for streaming events to client\nwith automatic reconnection support.\n\nArgs:\n    event_types: List of event types to subscribe to.\n        Use ['*'] for all events. Defaults to all.\n        Options: 'user_update', 'message', 'notification', 'system'.\n    since: Optional timestamp to replay events from.\n        Useful for reconnection. Max 1 hour in past.\n    user: Authenticated user receiving events.\n\nReturns:\n    EventSourceResponse streaming events in SSE format:\n        - id: Event ID for resumption\n        - event: Event type name\n        - data: JSON event payload\n        - retry: Reconnection delay in milliseconds\n\nRaises:\n    HTTPException: Status 400 if invalid event type.\n    HTTPException: Status 400 if 'since' too far in past.\n\nExample:\n    Request:\n        GET /events/stream?event_types=notification&event_types=message\n        Headers:\n            Accept: text/event-stream\n    \n    Response:\n        200 OK\n        Content-Type: text/event-stream\n        Cache-Control: no-cache\n        \n        id: 1704891234567\n        event: notification\n        data: {\"type\": \"mention\", \"from_user\": \"alice\", \"message\": \"Check this out!\"}\n        \n        id: 1704891235001\n        event: message\n        data: {\"chat_id\": \"room_123\", \"text\": \"Hello everyone!\", \"timestamp\": \"2024-01-10T12:00:35Z\"}\n        \n        :heartbeat\n\nNote:\n    Sends ':heartbeat' every 30 seconds to keep connection alive.\n    Client should reconnect with last received ID on disconnect.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "export_data",
      "module_path": "api/export.py",
      "function_signature": "def export_data(export_format: str = Query(..., regex='^(csv|json|excel|pdf)$'), filters: ExportFilters = Depends(), columns: list[str] | None = Query(None), async_export: bool = Query(False)) -> ExportResponse | TaskResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Export data in various formats with filtering.\n\nSupports synchronous and asynchronous export with progress tracking\nfor large datasets.\n\nArgs:\n    export_format: Output format.\n        Options: 'csv', 'json', 'excel', 'pdf'.\n    filters: Export filters injected via dependency:\n        - date_from: Start date for data range\n        - date_to: End date for data range\n        - categories: List of categories to include\n        - include_archived: Whether to include archived items\n    columns: Specific columns to include in export.\n        None exports all available columns.\n    async_export: If True, returns task ID for background processing.\n        Recommended for exports over 10k records.\n\nReturns:\n    If async_export=False: ExportResponse with download URL\n    If async_export=True: TaskResponse with task tracking info\n\nRaises:\n    HTTPException: Status 400 if invalid column specified.\n    HTTPException: Status 413 if export too large for sync mode.\n    HTTPException: Status 429 if export rate limit exceeded.\n\nExample:\n    Sync Request:\n        GET /data/export?export_format=csv&filters[date_from]=2024-01-01&columns=id&columns=name\n    \n    Response:\n        200 OK\n        {\n            \"download_url\": \"https://downloads.example.com/exports/abc123.csv\",\n            \"expires_at\": \"2024-01-11T00:00:00Z\",\n            \"size_bytes\": 1048576,\n            \"row_count\": 5000\n        }\n    \n    Async Request:\n        GET /data/export?export_format=excel&async_export=true\n    \n    Response:\n        202 Accepted\n        {\n            \"task_id\": \"task_xyz789\",\n            \"status\": \"pending\",\n            \"progress_url\": \"/tasks/task_xyz789/progress\",\n            \"estimated_completion\": \"2024-01-10T12:05:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "handle_webhook",
      "module_path": "api/webhooks.py",
      "function_signature": "async def handle_webhook(webhook_id: str, request: Request, signature: str = Header(..., alias='X-Webhook-Signature'), timestamp: str = Header(..., alias='X-Webhook-Timestamp')) -> WebhookResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Handle incoming webhook notifications.\n\nValidates webhook signatures, processes payloads, and handles retries\nwith idempotency support.\n\nArgs:\n    webhook_id: Unique identifier for webhook configuration.\n    request: Raw request object containing webhook payload.\n    signature: HMAC signature for payload verification.\n        Must match expected signature.\n    timestamp: Request timestamp for replay protection.\n        Must be within 5 minutes of server time.\n\nReturns:\n    WebhookResponse with:\n        - status: Processing status ('success', 'queued', 'duplicate')\n        - message_id: Unique ID for this webhook delivery\n        - processed_at: Processing timestamp\n\nRaises:\n    HTTPException: Status 401 if signature validation fails.\n    HTTPException: Status 400 if timestamp too old (replay attack).\n    HTTPException: Status 404 if webhook_id not registered.\n    HTTPException: Status 409 if duplicate delivery (already processed).\n\nExample:\n    Request:\n        POST /webhooks/wh_payment_provider\n        Headers:\n            X-Webhook-Signature: sha256=3b5c3a8f9d2e1a0c...\n            X-Webhook-Timestamp: 1704891600\n            Content-Type: application/json\n        Body:\n        {\n            \"event\": \"payment.completed\",\n            \"payment_id\": \"pay_123\",\n            \"amount\": 99.99,\n            \"currency\": \"USD\"\n        }\n    \n    Response:\n        200 OK\n        {\n            \"status\": \"success\",\n            \"message_id\": \"msg_abc123xyz\",\n            \"processed_at\": \"2024-01-10T12:00:00Z\"\n        }\n\nNote:\n    Webhook handlers should be idempotent.\n    Failed webhooks are automatically retried with exponential backoff.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "graphql_resolver",
      "module_path": "api/graphql.py",
      "function_signature": "@strawberry.field\nasync def get_user_posts(self, first: int = 10, after: str | None = None, order_by: PostOrderBy = PostOrderBy.CREATED_AT_DESC) -> Connection[Post]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"GraphQL resolver for paginated user posts.\n\nImplements Relay-style cursor pagination with sorting options for\nefficient data fetching in GraphQL queries.\n\nArgs:\n    first: Number of posts to return. Max 100.\n        Defaults to 10.\n    after: Cursor for pagination. Opaque string from previous\n        response's endCursor.\n    order_by: Sort order for posts. Enum with options:\n        CREATED_AT_DESC, CREATED_AT_ASC, POPULARITY_DESC.\n        Defaults to CREATED_AT_DESC.\n\nReturns:\n    Connection[Post] containing:\n        - edges: List of PostEdge with node and cursor\n        - pageInfo: Pagination metadata\n            - hasNextPage: More results available\n            - hasPreviousPage: Can paginate backwards\n            - startCursor: First item's cursor\n            - endCursor: Last item's cursor\n        - totalCount: Total posts for this user\n\nRaises:\n    GraphQLError: If first > 100 or < 1.\n    GraphQLError: If after cursor is invalid.\n\nExample:\n    Query:\n        query GetUserPosts($userId: ID!, $first: Int, $after: String) {\n            user(id: $userId) {\n                posts(first: $first, after: $after, orderBy: CREATED_AT_DESC) {\n                    edges {\n                        node {\n                            id\n                            title\n                            content\n                            createdAt\n                        }\n                        cursor\n                    }\n                    pageInfo {\n                        hasNextPage\n                        endCursor\n                    }\n                    totalCount\n                }\n            }\n        }\n    \n    Variables:\n        {\n            \"userId\": \"123\",\n            \"first\": 20,\n            \"after\": \"eyJpZCI6NDV9\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "batch_update",
      "module_path": "api/batch.py",
      "function_signature": "def batch_update(updates: list[ResourceUpdate], validation_mode: str = Query('strict', regex='^(strict|lenient)$'), partial: bool = Query(True)) -> BatchUpdateResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Batch update multiple resources in a single request.\n\nEfficiently updates multiple resources with configurable validation\nand partial update support.\n\nArgs:\n    updates: List of update operations. Each contains:\n        - resource_id: ID of resource to update\n        - fields: Dictionary of fields to update\n        - version: Optional version for optimistic locking\n        Max 500 updates per request.\n    validation_mode: How to handle validation:\n        - 'strict': Fail entire batch on any validation error\n        - 'lenient': Skip invalid updates, process valid ones\n        Defaults to 'strict'.\n    partial: If True, only update provided fields.\n        If False, replace entire resource. Defaults to True.\n\nReturns:\n    BatchUpdateResponse containing:\n        - updated: List of successfully updated resource IDs\n        - failed: List of failed updates with reasons\n        - warnings: List of non-fatal issues\n        - batch_version: Version identifier for this batch\n\nRaises:\n    HTTPException: Status 400 if validation_mode='strict' and any update invalid.\n    HTTPException: Status 413 if more than 500 updates.\n    HTTPException: Status 409 if version conflicts detected.\n\nExample:\n    Request:\n        PATCH /resources/batch?validation_mode=lenient\n        Body:\n        [\n            {\n                \"resource_id\": \"res_001\",\n                \"fields\": {\"status\": \"active\", \"priority\": \"high\"},\n                \"version\": 3\n            },\n            {\n                \"resource_id\": \"res_002\",\n                \"fields\": {\"status\": \"invalid_status\"}\n            }\n        ]\n    \n    Response:\n        207 Multi-Status\n        {\n            \"updated\": [\"res_001\"],\n            \"failed\": [\n                {\n                    \"resource_id\": \"res_002\",\n                    \"reason\": \"Invalid status value\",\n                    \"code\": \"VALIDATION_ERROR\"\n                }\n            ],\n            \"warnings\": [],\n            \"batch_version\": \"batch_v4_20240110\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "health_check",
      "module_path": "api/health.py",
      "function_signature": "async def health_check(include_details: bool = Query(False), check_dependencies: bool = Query(True)) -> HealthCheckResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Health check endpoint for monitoring and load balancers.\n\nReturns service health status with optional detailed diagnostics\nand dependency checks.\n\nArgs:\n    include_details: If True, include detailed component status.\n        May increase response time. Defaults to False.\n    check_dependencies: If True, check external dependencies.\n        Set False for faster checks. Defaults to True.\n\nReturns:\n    HealthCheckResponse with:\n        - status: Overall status ('healthy', 'degraded', 'unhealthy')\n        - timestamp: Check timestamp\n        - version: API version\n        - uptime_seconds: Service uptime\n        - details: Component details (if requested)\n        - dependencies: External service status (if checked)\n\nRaises:\n    Never raises exceptions - returns unhealthy status instead.\n\nExample:\n    Simple check:\n        GET /health\n    \n    Response:\n        200 OK\n        {\n            \"status\": \"healthy\",\n            \"timestamp\": \"2024-01-10T12:00:00Z\",\n            \"version\": \"1.2.3\",\n            \"uptime_seconds\": 86400\n        }\n    \n    Detailed check:\n        GET /health?include_details=true&check_dependencies=true\n    \n    Response:\n        200 OK (degraded) or 503 Service Unavailable (unhealthy)\n        {\n            \"status\": \"degraded\",\n            \"timestamp\": \"2024-01-10T12:00:00Z\",\n            \"version\": \"1.2.3\",\n            \"uptime_seconds\": 86400,\n            \"details\": {\n                \"api\": \"healthy\",\n                \"database_pool\": \"healthy\",\n                \"cache\": \"degraded\",\n                \"queue\": \"healthy\"\n            },\n            \"dependencies\": {\n                \"auth_service\": \"healthy\",\n                \"payment_gateway\": \"timeout\",\n                \"email_service\": \"healthy\"\n            }\n        }\n\nNote:\n    Returns 200 for healthy/degraded, 503 for unhealthy.\n    Degraded means non-critical components have issues.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "websocket_chat",
      "module_path": "api/websocket.py",
      "function_signature": "async def websocket_chat(websocket: WebSocket, room_id: str = Path(...), token: str = Query(...)) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"WebSocket endpoint for real-time chat functionality.\n\nHandles bidirectional communication for chat rooms with authentication,\nmessage broadcasting, and presence tracking.\n\nArgs:\n    websocket: WebSocket connection from client.\n    room_id: Chat room identifier to join.\n    token: Authentication token for user verification.\n\nRaises:\n    WebSocketException: If authentication fails.\n    WebSocketException: If room doesn't exist or user lacks access.\n    ConnectionError: If WebSocket connection fails.\n\nExample:\n    Client connection:\n        const ws = new WebSocket('wss://api.example.com/ws/chat/room123?token=abc...');\n        \n        ws.onopen = () => {\n            // Send message\n            ws.send(JSON.stringify({\n                type: 'message',\n                content: 'Hello everyone!'\n            }));\n        };\n        \n        ws.onmessage = (event) => {\n            const data = JSON.parse(event.data);\n            switch(data.type) {\n                case 'message':\n                    console.log(`${data.user}: ${data.content}`);\n                    break;\n                case 'user_joined':\n                    console.log(`${data.user} joined the room`);\n                    break;\n                case 'user_left':\n                    console.log(`${data.user} left the room`);\n                    break;\n                case 'error':\n                    console.error(`Error: ${data.message}`);\n                    break;\n            }\n        };\n    \n    Server messages:\n        {\"type\": \"message\", \"user\": \"alice\", \"content\": \"Hi!\", \"timestamp\": \"2024-01-10T12:00:00Z\"}\n        {\"type\": \"user_joined\", \"user\": \"bob\", \"timestamp\": \"2024-01-10T12:01:00Z\"}\n        {\"type\": \"typing\", \"user\": \"alice\", \"is_typing\": true}\n        {\"type\": \"error\", \"message\": \"Rate limit exceeded\", \"code\": \"RATE_LIMIT\"}\n\nNote:\n    - Automatically sends 'user_joined' and 'user_left' events\n    - Implements rate limiting (max 100 messages/minute per user)\n    - Disconnects idle connections after 5 minutes\n    - Messages are persisted for history retrieval\n\"\"\"",
      "has_params": true,
      "has_returns": false,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "create_checkout_session",
      "module_path": "api/payments.py",
      "function_signature": "async def create_checkout_session(checkout_data: CheckoutSessionCreate, user: User = Depends(get_current_user), idempotency_key: str | None = Header(None)) -> CheckoutSessionResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create a payment checkout session.\n\nInitiates a payment flow with support for multiple payment methods\nand idempotency for safe retries.\n\nArgs:\n    checkout_data: Checkout session configuration:\n        - items: List of {product_id, quantity, price_override?}\n        - success_url: URL to redirect after success\n        - cancel_url: URL to redirect on cancellation\n        - payment_methods: Allowed methods ['card', 'bank', 'wallet']\n        - metadata: Optional key-value pairs for reference\n    user: Authenticated user creating the session.\n    idempotency_key: Optional key to prevent duplicate charges\n        on retry. Recommended for all payment operations.\n\nReturns:\n    CheckoutSessionResponse containing:\n        - session_id: Unique checkout session identifier\n        - payment_url: URL to redirect user for payment\n        - expires_at: Session expiration time (30 minutes)\n        - amount_total: Total amount including tax\n        - currency: Payment currency code\n\nRaises:\n    HTTPException: Status 400 if invalid product or pricing.\n    HTTPException: Status 402 if payment method not available.\n    HTTPException: Status 409 if idempotency key already used.\n    HTTPException: Status 503 if payment service unavailable.\n\nExample:\n    Request:\n        POST /checkout/sessions\n        Headers:\n            Idempotency-Key: order_12345_attempt_1\n        Body:\n        {\n            \"items\": [\n                {\"product_id\": \"prod_abc\", \"quantity\": 2},\n                {\"product_id\": \"prod_xyz\", \"quantity\": 1, \"price_override\": 49.99}\n            ],\n            \"success_url\": \"https://myapp.com/success?session_id={CHECKOUT_SESSION_ID}\",\n            \"cancel_url\": \"https://myapp.com/cart\",\n            \"payment_methods\": [\"card\", \"wallet\"],\n            \"metadata\": {\"order_id\": \"12345\"}\n        }\n    \n    Response:\n        201 Created\n        {\n            \"session_id\": \"cs_live_abc123xyz\",\n            \"payment_url\": \"https://checkout.example.com/pay/cs_live_abc123xyz\",\n            \"expires_at\": \"2024-01-10T12:30:00Z\",\n            \"amount_total\": 199.97,\n            \"currency\": \"USD\"\n        }\n\nNote:\n    Sessions expire after 30 minutes. Monitor webhook for completion.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "analyze_logs",
      "module_path": "api/analytics.py",
      "function_signature": "def analyze_logs(time_range: TimeRange = Depends(), aggregations: list[str] = Query(['count', 'errors']), group_by: str | None = Query(None), filters: LogFilters = Depends()) -> AnalyticsResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Analyze application logs with aggregations and filtering.\n\nProvides real-time log analysis with various aggregation functions\nand flexible filtering options.\n\nArgs:\n    time_range: Time range for analysis (injected):\n        - start: Beginning of range\n        - end: End of range (defaults to now)\n        - timezone: Timezone for bucketing\n    aggregations: List of aggregation functions:\n        Options: 'count', 'errors', 'response_time_p50',\n        'response_time_p95', 'response_time_p99',\n        'unique_users', 'requests_per_second'.\n        Defaults to ['count', 'errors'].\n    group_by: Optional grouping field:\n        Options: 'endpoint', 'status_code', 'user_id',\n        'hour', 'day', 'error_type'.\n    filters: Log filters (injected):\n        - level: Minimum log level\n        - endpoint_prefix: Filter by endpoint\n        - status_codes: List of HTTP status codes\n        - user_ids: Specific users to analyze\n\nReturns:\n    AnalyticsResponse containing:\n        - aggregations: Dict of aggregation results\n        - time_series: Time-bucketed data (if applicable)\n        - groups: Grouped results (if group_by specified)\n        - query_time_ms: Analysis execution time\n        - total_events: Number of events analyzed\n\nRaises:\n    HTTPException: Status 400 if invalid aggregation or group_by.\n    HTTPException: Status 400 if time range exceeds 30 days.\n\nExample:\n    Request:\n        GET /logs/analyze?time_range[start]=2024-01-10T00:00:00Z&aggregations=errors&aggregations=response_time_p95&group_by=endpoint\n    \n    Response:\n        200 OK\n        {\n            \"aggregations\": {\n                \"errors\": 42,\n                \"response_time_p95\": 234.5\n            },\n            \"groups\": [\n                {\n                    \"endpoint\": \"/api/users\",\n                    \"errors\": 10,\n                    \"response_time_p95\": 150.2\n                },\n                {\n                    \"endpoint\": \"/api/orders\",\n                    \"errors\": 32,\n                    \"response_time_p95\": 450.8\n                }\n            ],\n            \"query_time_ms\": 45,\n            \"total_events\": 50000\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "refresh_token",
      "module_path": "api/auth.py",
      "function_signature": "def refresh_token(refresh_token: str = Body(...), grant_type: str = Body('refresh_token', regex='^refresh_token$')) -> TokenResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Refresh access token using refresh token.\n\nExchanges a valid refresh token for a new access token and optionally\na new refresh token (rotation).\n\nArgs:\n    refresh_token: Valid refresh token from previous authentication.\n        Must not be expired or revoked.\n    grant_type: OAuth2 grant type. Must be 'refresh_token'.\n\nReturns:\n    TokenResponse with:\n        - access_token: New JWT access token\n        - refresh_token: New refresh token (if rotation enabled)\n        - token_type: Always 'bearer'\n        - expires_in: Access token lifetime in seconds\n        - scope: Granted scopes (may be subset of original)\n\nRaises:\n    HTTPException: Status 401 if refresh token invalid/expired.\n    HTTPException: Status 401 if refresh token revoked.\n    HTTPException: Status 429 if too many refresh attempts.\n\nExample:\n    Request:\n        POST /auth/refresh\n        Content-Type: application/json\n        \n        {\n            \"refresh_token\": \"7f3a9c2d5e1b4a8f...\",\n            \"grant_type\": \"refresh_token\"\n        }\n    \n    Response:\n        200 OK\n        {\n            \"access_token\": \"eyJhbGciOiJIUzI1NiIs...\",\n            \"refresh_token\": \"8g4b0d3e6f2c5b9g...\",\n            \"token_type\": \"bearer\",\n            \"expires_in\": 3600,\n            \"scope\": \"read write\"\n        }\n\nNote:\n    - Old refresh token is invalidated after successful rotation\n    - Refresh tokens have longer expiry (30 days) than access tokens\n    - Each refresh token can only be used once when rotation is enabled\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    }
  ]
}
