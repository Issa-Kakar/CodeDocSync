{
  "version": "1.0",
  "examples": [
    {
      "function_name": "get_user",
      "module_path": "api/endpoints.py",
      "function_signature": "def get_user(user_id: str, include_details: bool = False) -> dict[str, Any]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retrieve user information from the database.\n\nArgs:\n    user_id (str): Unique identifier for the user to retrieve.\n    include_details (bool): Whether to include additional user details\n        such as preferences and activity history. Defaults to False.\n\nReturns:\n    dict[str, Any]: User information including id, name, email, and\n        optionally detailed information if requested.\n\nRaises:\n    ValueError: If user_id is empty or invalid format.\n    UserNotFoundError: If no user exists with the given id.\n    DatabaseConnectionError: If unable to connect to the database.\n\nExample:\n    >>> user = get_user(\"12345\")\n    >>> print(user['name'])\n    'John Doe'\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_data",
      "module_path": "async/client.py",
      "function_signature": "async def fetch_data(url: str, timeout: float = 30.0, retries: int = 3) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Asynchronously fetch data from the specified URL.\n\nArgs:\n    url (str): The URL to fetch data from. Must be a valid HTTP/HTTPS URL.\n    timeout (float): Maximum time to wait for response in seconds.\n        Defaults to 30.0.\n    retries (int): Number of retry attempts for failed requests.\n        Defaults to 3.\n\nReturns:\n    Response: The response object containing status code, headers,\n        and body content.\n\nRaises:\n    InvalidURLError: If the URL format is invalid.\n    TimeoutError: If the request exceeds the timeout duration.\n    ConnectionError: If unable to establish connection after all retries.\n\nExample:\n    >>> response = await fetch_data(\"https://api.example.com/data\")\n    >>> data = response.json()\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "database_connection",
      "module_path": "db/context.py",
      "function_signature": "@contextmanager\ndef database_connection(host: str, port: int = 5432, database: str = \"main\") -> Connection",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Context manager for database connections.\n\nArgs:\n    host (str): Database server hostname or IP address.\n    port (int): Port number for database connection. Defaults to 5432.\n    database (str): Name of the database to connect to. Defaults to \"main\".\n\nYields:\n    Connection: Active database connection object that is automatically\n        closed when exiting the context.\n\nRaises:\n    ConnectionError: If unable to establish database connection.\n    AuthenticationError: If credentials are invalid.\n\nExample:\n    >>> with database_connection(\"localhost\") as conn:\n    ...     cursor = conn.cursor()\n    ...     cursor.execute(\"SELECT * FROM users\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "retry",
      "module_path": "utils/decorators.py",
      "function_signature": "def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0) -> Callable",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Decorator to retry function execution on failure.\n\nArgs:\n    max_attempts (int): Maximum number of retry attempts. Defaults to 3.\n    delay (float): Initial delay between retries in seconds. Defaults to 1.0.\n    backoff (float): Multiplier for delay after each retry. Defaults to 2.0.\n\nReturns:\n    Callable: Decorated function that will retry on exceptions.\n\nExample:\n    >>> @retry(max_attempts=5, delay=0.5)\n    ... def unstable_network_call():\n    ...     return requests.get(\"https://api.example.com\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "train_model",
      "module_path": "ml/training.py",
      "function_signature": "def train_model(X: np.ndarray, y: np.ndarray, model_type: str = \"random_forest\", **kwargs) -> Model",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Train a machine learning model on the provided dataset.\n\nParameters\n----------\nX : np.ndarray\n    Feature matrix with shape (n_samples, n_features).\ny : np.ndarray\n    Target values with shape (n_samples,) for regression\n    or classification.\nmodel_type : str, optional\n    Type of model to train. Options are 'random_forest',\n    'gradient_boosting', 'neural_network'. Default is 'random_forest'.\n**kwargs : dict\n    Additional keyword arguments passed to the model constructor.\n    Common options include n_estimators, max_depth, learning_rate.\n\nReturns\n-------\nModel\n    Trained model instance with fit() and predict() methods.\n\nRaises\n------\nValueError\n    If X and y have incompatible shapes.\nNotImplementedError\n    If model_type is not supported.\n\nExamples\n--------\n>>> X_train = np.random.randn(100, 10)\n>>> y_train = np.random.randint(0, 2, 100)\n>>> model = train_model(X_train, y_train, model_type='random_forest')\n>>> predictions = model.predict(X_test)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_multiple_urls",
      "module_path": "async/batch_client.py",
      "function_signature": "async def fetch_multiple_urls(urls: list[str], max_concurrent: int = 10) -> list[Result[dict[str, Any]]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Fetch multiple URLs concurrently with rate limiting.\n\nUses asyncio.Semaphore to limit concurrent requests and prevent\noverwhelming target servers.\n\nArgs:\n    urls: List of URLs to fetch. Each must be a valid HTTP/HTTPS URL.\n    max_concurrent: Maximum number of concurrent requests.\n        Defaults to 10. Must be between 1 and 100.\n\nReturns:\n    List of Result objects in the same order as input URLs.\n    Each Result contains either successful response data or error details.\n    Failed requests will have Result.is_error = True.\n\nRaises:\n    ValueError: If max_concurrent is outside valid range.\n    TypeError: If urls is not a list of strings.\n\nExample:\n    >>> urls = ['https://api1.com/data', 'https://api2.com/info']\n    >>> results = await fetch_multiple_urls(urls, max_concurrent=5)\n    >>> for url, result in zip(urls, results):\n    ...     if result.is_error:\n    ...         print(f'{url} failed: {result.error}')\n    ...     else:\n    ...         print(f'{url} returned: {len(result.data)} bytes')\n\nNote:\n    Connection timeout is 30 seconds per request.\n    Failed requests are not retried automatically.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "process_items_concurrently",
      "module_path": "async/processor.py",
      "function_signature": "async def process_items_concurrently(items: list[T], processor: Callable[[T], Awaitable[R]], batch_size: int = 50) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process items concurrently using asyncio.gather().\n\nSplits items into batches and processes each batch concurrently\nto avoid memory issues with large datasets.\n\nArgs:\n    items: List of items to process.\n    processor: Async function that processes a single item.\n    batch_size: Number of items to process concurrently.\n        Defaults to 50.\n\nReturns:\n    List of processed results in the same order as input items.\n\nRaises:\n    ProcessingError: If any item fails to process.\n    ValueError: If batch_size is less than 1.\n\nExample:\n    >>> async def transform_item(item: dict) -> dict:\n    ...     # Simulate async processing\n    ...     await asyncio.sleep(0.1)\n    ...     return {'id': item['id'], 'processed': True}\n    ...\n    >>> items = [{'id': i} for i in range(100)]\n    >>> results = await process_items_concurrently(items, transform_item)\n    >>> print(f'Processed {len(results)} items')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "rate_limited_api_client",
      "module_path": "async/rate_limiter.py",
      "function_signature": "async def rate_limited_api_client(endpoint: str, requests_per_second: float = 10.0) -> AsyncAPIClient",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create an API client with built-in rate limiting using Semaphore.\n\nImplements token bucket algorithm with async semaphore for\nsmooth rate limiting across concurrent requests.\n\nArgs:\n    endpoint: Base URL of the API endpoint.\n    requests_per_second: Maximum requests allowed per second.\n        Defaults to 10.0. Fractional values supported.\n\nReturns:\n    AsyncAPIClient instance with rate limiting enabled.\n\nRaises:\n    ValueError: If endpoint is invalid or requests_per_second <= 0.\n    ConnectionError: If unable to establish initial connection.\n\nExample:\n    >>> client = await rate_limited_api_client('https://api.example.com', 5.0)\n    >>> # Makes at most 5 requests per second\n    >>> tasks = [client.get(f'/user/{i}') for i in range(20)]\n    >>> results = await asyncio.gather(*tasks)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_database_pool",
      "module_path": "async/db_context.py",
      "function_signature": "@asynccontextmanager\nasync def async_database_pool(dsn: str, min_size: int = 10, max_size: int = 20) -> AsyncIterator[Pool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async context manager for database connection pool.\n\nCreates and manages a pool of database connections that can be\nreused across multiple async operations.\n\nArgs:\n    dsn: Database connection string (Data Source Name).\n    min_size: Minimum number of connections to maintain.\n        Defaults to 10.\n    max_size: Maximum number of connections allowed.\n        Defaults to 20.\n\nYields:\n    Pool: Async connection pool instance.\n\nRaises:\n    ConnectionError: If unable to create initial connections.\n    ValueError: If min_size > max_size.\n\nExample:\n    >>> async with async_database_pool('postgresql://localhost/mydb') as pool:\n    ...     async with pool.acquire() as conn:\n    ...         result = await conn.fetch('SELECT * FROM users')\n    ...         print(f'Found {len(result)} users')\n\nNote:\n    Pool is automatically closed when exiting context.\n    Connections are health-checked before reuse.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "stream_large_file",
      "module_path": "async/file_streaming.py",
      "function_signature": "async def stream_large_file(file_path: Path, chunk_size: int = 8192) -> AsyncIterator[bytes]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async generator for streaming large files in chunks.\n\nReads file in chunks to avoid loading entire file into memory,\nuseful for processing multi-GB files.\n\nArgs:\n    file_path: Path to the file to stream.\n    chunk_size: Size of each chunk in bytes.\n        Defaults to 8192 (8KB).\n\nYields:\n    bytes: File content in chunks.\n\nRaises:\n    FileNotFoundError: If file doesn't exist.\n    PermissionError: If file is not readable.\n    IOError: If read operation fails.\n\nExample:\n    >>> async for chunk in stream_large_file(Path('/data/large.csv')):\n    ...     # Process chunk without loading entire file\n    ...     lines = chunk.decode('utf-8').splitlines()\n    ...     for line in lines:\n    ...         await process_csv_line(line)\n\nNote:\n    File is automatically closed when generator is exhausted or cancelled.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "retry_with_exponential_backoff",
      "module_path": "async/retry_handler.py",
      "function_signature": "async def retry_with_exponential_backoff(coro: Callable[[], Awaitable[T]], max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0, exceptions: tuple[type[Exception], ...] = (Exception,)) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retry async operation with exponential backoff on failure.\n\nImplements exponential backoff with jitter to avoid thundering herd\nproblem when multiple clients retry simultaneously.\n\nArgs:\n    coro: Coroutine function to retry (called without arguments).\n    max_retries: Maximum number of retry attempts.\n        Defaults to 3.\n    base_delay: Initial delay in seconds.\n        Defaults to 1.0.\n    max_delay: Maximum delay between retries.\n        Defaults to 60.0.\n    exceptions: Tuple of exception types to retry on.\n        Defaults to catch all exceptions.\n\nReturns:\n    Result from successful coroutine execution.\n\nRaises:\n    The last exception if all retries fail.\n    ValueError: If max_retries < 0 or delays are invalid.\n\nExample:\n    >>> async def flaky_api_call():\n    ...     response = await http_client.get('/unstable-endpoint')\n    ...     return response.json()\n    ...\n    >>> data = await retry_with_exponential_backoff(\n    ...     flaky_api_call,\n    ...     max_retries=5,\n    ...     exceptions=(ConnectionError, TimeoutError)\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "wait_for_with_timeout",
      "module_path": "async/timeout_utils.py",
      "function_signature": "async def wait_for_with_timeout(coro: Coroutine[Any, Any, T], timeout: float, fallback: T | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute coroutine with timeout using asyncio.timeout().\n\nProvides a clean interface for timeout handling with optional\nfallback value on timeout.\n\nArgs:\n    coro: Coroutine to execute with timeout.\n    timeout: Maximum seconds to wait for completion.\n    fallback: Value to return if timeout occurs.\n        If None, raises TimeoutError.\n\nReturns:\n    Result from coroutine or fallback value on timeout.\n\nRaises:\n    TimeoutError: If timeout occurs and no fallback provided.\n    Any exception raised by the coroutine.\n\nExample:\n    >>> async def slow_computation():\n    ...     await asyncio.sleep(10)\n    ...     return 42\n    ...\n    >>> # Returns -1 if computation takes more than 5 seconds\n    >>> result = await wait_for_with_timeout(\n    ...     slow_computation(),\n    ...     timeout=5.0,\n    ...     fallback=-1\n    ... )\n\nNote:\n    Uses asyncio.timeout() context manager internally (Python 3.11+).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "create_background_tasks",
      "module_path": "async/task_manager.py",
      "function_signature": "async def create_background_tasks(task_specs: list[TaskSpec], supervisor: TaskSupervisor | None = None) -> list[asyncio.Task[Any]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create and manage background tasks using asyncio.create_task().\n\nCreates tasks that run concurrently in the background with optional\nsupervision for error handling and lifecycle management.\n\nArgs:\n    task_specs: List of task specifications containing coroutine\n        and metadata.\n    supervisor: Optional task supervisor for monitoring and\n        error handling.\n\nReturns:\n    List of created asyncio.Task instances.\n\nRaises:\n    ValueError: If task_specs is empty or contains invalid specs.\n    RuntimeError: If event loop is not running.\n\nExample:\n    >>> specs = [\n    ...     TaskSpec(coro=monitor_system(), name='system_monitor'),\n    ...     TaskSpec(coro=cleanup_old_files(), name='cleanup')\n    ... ]\n    >>> tasks = await create_background_tasks(specs)\n    >>> # Tasks now run in background\n    >>> await asyncio.sleep(60)  # Let them run\n    >>> # Cancel all tasks when done\n    >>> for task in tasks:\n    ...     task.cancel()\n\nNote:\n    Tasks continue running until explicitly cancelled or completed.\n    Use TaskSupervisor for automatic restart on failure.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "event_driven_processor",
      "module_path": "async/event_processor.py",
      "function_signature": "async def event_driven_processor(event_queue: asyncio.Queue[Event], handlers: dict[str, EventHandler], stop_event: asyncio.Event | None = None) -> ProcessorStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process events asynchronously using event-driven pattern.\n\nConsumes events from queue and dispatches to appropriate handlers\nbased on event type.\n\nArgs:\n    event_queue: Async queue containing events to process.\n    handlers: Mapping of event types to handler functions.\n    stop_event: Optional event to signal processor shutdown.\n\nReturns:\n    ProcessorStats with counts of processed and failed events.\n\nRaises:\n    ValueError: If handlers dict is empty.\n    ProcessorError: If critical error occurs during processing.\n\nExample:\n    >>> queue = asyncio.Queue()\n    >>> handlers = {\n    ...     'user_signup': handle_signup,\n    ...     'order_placed': handle_order,\n    ...     'payment_received': handle_payment\n    ... }\n    >>> stop = asyncio.Event()\n    >>> # Run processor in background\n    >>> processor_task = asyncio.create_task(\n    ...     event_driven_processor(queue, handlers, stop)\n    ... )\n    >>> # Add events to queue\n    >>> await queue.put(Event(type='user_signup', data={'user_id': 123}))\n    >>> # Stop processor\n    >>> stop.set()\n    >>> stats = await processor_task\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_paginate_results",
      "module_path": "async/pagination.py",
      "function_signature": "async def async_paginate_results(fetch_page: Callable[[int], Awaitable[Page[T]]], start_page: int = 1, max_pages: int | None = None) -> AsyncIterator[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"AsyncIterator for paginating through API results.\n\nAutomatically fetches subsequent pages as iteration continues,\nhandling pagination logic transparently.\n\nArgs:\n    fetch_page: Async function that fetches a specific page number.\n    start_page: Page number to start from. Defaults to 1.\n    max_pages: Maximum number of pages to fetch.\n        None for unlimited.\n\nYields:\n    Individual items from all pages.\n\nRaises:\n    PaginationError: If page fetching fails.\n    ValueError: If start_page < 1.\n\nExample:\n    >>> async def fetch_users_page(page: int) -> Page[User]:\n    ...     response = await api_client.get(f'/users?page={page}')\n    ...     return Page(\n    ...         items=response['users'],\n    ...         has_next=response['has_next']\n    ...     )\n    ...\n    >>> async for user in async_paginate_results(fetch_users_page):\n    ...     print(f'Processing user: {user.name}')\n    ...     if user.inactive:\n    ...         await deactivate_user(user)\n\nNote:\n    Stops iteration when fetch_page returns Page with has_next=False.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_cache_with_ttl",
      "module_path": "async/caching.py",
      "function_signature": "async def async_cache_with_ttl(key: str, fetcher: Callable[[], Awaitable[T]], ttl_seconds: float = 300.0, cache: AsyncCache | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Get value from async cache or fetch if missing/expired.\n\nImplements read-through caching pattern with TTL support for\nasync operations.\n\nArgs:\n    key: Cache key to lookup.\n    fetcher: Async function to fetch value if not cached.\n    ttl_seconds: Time-to-live in seconds. Defaults to 300 (5 minutes).\n    cache: Optional cache instance. Uses default if None.\n\nReturns:\n    Cached value or freshly fetched value.\n\nRaises:\n    CacheError: If cache operation fails.\n    Any exception raised by fetcher.\n\nExample:\n    >>> async def expensive_api_call():\n    ...     response = await external_api.get('/expensive-data')\n    ...     return response.json()\n    ...\n    >>> # First call fetches from API\n    >>> data = await async_cache_with_ttl(\n    ...     'api_data',\n    ...     expensive_api_call,\n    ...     ttl_seconds=600\n    ... )\n    >>> # Subsequent calls within 10 minutes use cache\n    >>> cached_data = await async_cache_with_ttl('api_data', expensive_api_call)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "parallel_map_async",
      "module_path": "async/parallel.py",
      "function_signature": "async def parallel_map_async(func: Callable[[T], Awaitable[R]], items: Iterable[T], max_workers: int = 10, ordered: bool = True) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Apply async function to items in parallel with worker limit.\n\nSimilar to map() but executes async function on multiple items\nconcurrently with controlled parallelism.\n\nArgs:\n    func: Async function to apply to each item.\n    items: Iterable of items to process.\n    max_workers: Maximum concurrent workers. Defaults to 10.\n    ordered: If True, preserve input order in results.\n        Defaults to True.\n\nReturns:\n    List of results from applying func to each item.\n\nRaises:\n    ParallelExecutionError: If any item fails to process.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def fetch_user_details(user_id: int) -> dict:\n    ...     response = await api_client.get(f'/users/{user_id}')\n    ...     return response.json()\n    ...\n    >>> user_ids = [101, 102, 103, 104, 105]\n    >>> details = await parallel_map_async(\n    ...     fetch_user_details,\n    ...     user_ids,\n    ...     max_workers=3\n    ... )\n    >>> print(f'Fetched details for {len(details)} users')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_circuit_breaker",
      "module_path": "async/resilience.py",
      "function_signature": "async def async_circuit_breaker(func: Callable[..., Awaitable[T]], failure_threshold: int = 5, timeout: float = 60.0, half_open_max_calls: int = 1) -> Callable[..., Awaitable[T]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Wrap async function with circuit breaker pattern.\n\nPrevents cascading failures by stopping calls to failing services\ntemporarily.\n\nArgs:\n    func: Async function to protect with circuit breaker.\n    failure_threshold: Failures before opening circuit.\n        Defaults to 5.\n    timeout: Seconds before attempting reset. Defaults to 60.0.\n    half_open_max_calls: Test calls in half-open state.\n        Defaults to 1.\n\nReturns:\n    Wrapped function with circuit breaker behavior.\n\nRaises:\n    CircuitOpenError: When circuit is open and calls are rejected.\n    Original exceptions from func when circuit is closed.\n\nExample:\n    >>> @async_circuit_breaker(failure_threshold=3, timeout=30.0)\n    ... async def unstable_service_call(endpoint: str) -> dict:\n    ...     response = await http_client.get(endpoint)\n    ...     return response.json()\n    ...\n    >>> try:\n    ...     result = await unstable_service_call('/api/data')\n    ... except CircuitOpenError:\n    ...     # Use fallback when circuit is open\n    ...     result = get_cached_data()\n\nNote:\n    Circuit states: CLOSED (normal) -> OPEN (failing) -> HALF_OPEN (testing).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_pub_sub",
      "module_path": "async/pubsub.py",
      "function_signature": "async def async_pub_sub(topic: str, message: Any | None = None, subscribe: bool = False) -> AsyncIterator[Message] | None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async publish-subscribe pattern implementation.\n\nAllows publishing messages to topics and subscribing to receive\nmessages asynchronously.\n\nArgs:\n    topic: Topic name for publish/subscribe.\n    message: Message to publish. None when subscribing.\n    subscribe: If True, returns subscription iterator.\n        If False, publishes message.\n\nReturns:\n    AsyncIterator[Message] when subscribing, None when publishing.\n\nRaises:\n    ValueError: If publishing with subscribe=True or vice versa.\n    TopicError: If topic doesn't exist when subscribing.\n\nExample:\n    >>> # Publisher\n    >>> await async_pub_sub('user_events', {'type': 'login', 'user_id': 123})\n    ...\n    >>> # Subscriber\n    >>> async for msg in async_pub_sub('user_events', subscribe=True):\n    ...     if msg.data['type'] == 'login':\n    ...         await track_user_login(msg.data['user_id'])\n    ...     elif msg.data['type'] == 'logout':\n    ...         await track_user_logout(msg.data['user_id'])\n\nNote:\n    Subscribers receive messages published after subscription starts.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_middleware_chain",
      "module_path": "async/middleware.py",
      "function_signature": "async def async_middleware_chain(request: Request, middlewares: list[Middleware], handler: RequestHandler) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute async middleware chain for request processing.\n\nProcesses request through multiple middleware layers before\nreaching the final handler.\n\nArgs:\n    request: Incoming request to process.\n    middlewares: Ordered list of middleware to apply.\n    handler: Final request handler after all middleware.\n\nReturns:\n    Response after processing through all layers.\n\nRaises:\n    MiddlewareError: If any middleware fails.\n    ValueError: If middlewares list is empty.\n\nExample:\n    >>> middlewares = [\n    ...     AuthenticationMiddleware(),\n    ...     RateLimitMiddleware(requests_per_minute=100),\n    ...     LoggingMiddleware(),\n    ...     CompressionMiddleware()\n    ... ]\n    >>> async def api_handler(request: Request) -> Response:\n    ...     data = await process_request(request)\n    ...     return Response(data=data)\n    ...\n    >>> response = await async_middleware_chain(\n    ...     request,\n    ...     middlewares,\n    ...     api_handler\n    ... )\n\nNote:\n    Middleware can modify request before and response after handler.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_batch_processor",
      "module_path": "async/batch.py",
      "function_signature": "async def async_batch_processor(items: AsyncIterator[T], batch_size: int = 100, processor: Callable[[list[T]], Awaitable[None]] | None = None) -> BatchStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process async iterator items in batches.\n\nAccumulates items into batches and processes them together\nfor improved efficiency.\n\nArgs:\n    items: Async iterator producing items to process.\n    batch_size: Number of items per batch. Defaults to 100.\n    processor: Optional batch processing function.\n        If None, uses default processor.\n\nReturns:\n    BatchStats with total items and batches processed.\n\nRaises:\n    BatchProcessingError: If batch processing fails.\n    ValueError: If batch_size < 1.\n\nExample:\n    >>> async def stream_records() -> AsyncIterator[Record]:\n    ...     async for record in database.stream('SELECT * FROM large_table'):\n    ...         yield record\n    ...\n    >>> async def save_batch(records: list[Record]) -> None:\n    ...     await bulk_insert(records)\n    ...\n    >>> stats = await async_batch_processor(\n    ...     stream_records(),\n    ...     batch_size=1000,\n    ...     processor=save_batch\n    ... )\n    >>> print(f'Processed {stats.total_items} items in {stats.total_batches} batches')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_resource_pool",
      "module_path": "async/resource_pool.py",
      "function_signature": "async def async_resource_pool(resource_factory: Callable[[], Awaitable[T]], min_size: int = 1, max_size: int = 10, timeout: float = 30.0) -> AsyncResourcePool[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create async pool for expensive resource management.\n\nManages a pool of reusable resources (connections, clients, etc.)\nwith automatic creation and cleanup.\n\nArgs:\n    resource_factory: Async function to create new resources.\n    min_size: Minimum resources to maintain. Defaults to 1.\n    max_size: Maximum resources allowed. Defaults to 10.\n    timeout: Seconds to wait for available resource.\n        Defaults to 30.0.\n\nReturns:\n    AsyncResourcePool instance for resource management.\n\nRaises:\n    PoolError: If unable to create minimum resources.\n    ValueError: If min_size > max_size or sizes < 1.\n\nExample:\n    >>> async def create_client() -> APIClient:\n    ...     client = APIClient()\n    ...     await client.connect()\n    ...     return client\n    ...\n    >>> pool = await async_resource_pool(\n    ...     create_client,\n    ...     min_size=5,\n    ...     max_size=20\n    ... )\n    >>> async with pool.acquire() as client:\n    ...     result = await client.make_request('/api/data')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_state_machine",
      "module_path": "async/state_machine.py",
      "function_signature": "async def async_state_machine(initial_state: State, transitions: dict[tuple[State, Event], State], event_stream: AsyncIterator[Event]) -> State",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async finite state machine for event processing.\n\nProcesses events from async stream and transitions between states\naccording to defined rules.\n\nArgs:\n    initial_state: Starting state of the machine.\n    transitions: Mapping of (state, event) tuples to next state.\n    event_stream: Async iterator of events to process.\n\nReturns:\n    Final state when event stream is exhausted.\n\nRaises:\n    InvalidTransitionError: If event has no valid transition from current state.\n    StateError: If state machine enters invalid state.\n\nExample:\n    >>> transitions = {\n    ...     (State.IDLE, Event.START): State.RUNNING,\n    ...     (State.RUNNING, Event.PAUSE): State.PAUSED,\n    ...     (State.PAUSED, Event.RESUME): State.RUNNING,\n    ...     (State.RUNNING, Event.COMPLETE): State.DONE\n    ... }\n    >>> async def job_events() -> AsyncIterator[Event]:\n    ...     yield Event.START\n    ...     await asyncio.sleep(1)\n    ...     yield Event.COMPLETE\n    ...\n    >>> final_state = await async_state_machine(\n    ...     State.IDLE,\n    ...     transitions,\n    ...     job_events()\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_health_monitor",
      "module_path": "async/monitoring.py",
      "function_signature": "async def async_health_monitor(services: list[Service], check_interval: float = 60.0, alert_handler: AlertHandler | None = None) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Monitor service health with async health checks.\n\nPeriodically checks health of multiple services and triggers\nalerts on failures.\n\nArgs:\n    services: List of services to monitor.\n    check_interval: Seconds between health checks.\n        Defaults to 60.0.\n    alert_handler: Optional handler for health alerts.\n\nRaises:\n    MonitoringError: If monitoring setup fails.\n    ValueError: If services list is empty or interval <= 0.\n\nExample:\n    >>> services = [\n    ...     Service(name='api', health_check=check_api_health),\n    ...     Service(name='database', health_check=check_db_health),\n    ...     Service(name='cache', health_check=check_cache_health)\n    ... ]\n    >>> async def send_alert(service: str, status: str):\n    ...     await notification_service.send(f'{service} is {status}')\n    ...\n    >>> # Run monitor in background\n    >>> monitor_task = asyncio.create_task(\n    ...     async_health_monitor(\n    ...         services,\n    ...         check_interval=30.0,\n    ...         alert_handler=send_alert\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Each service checked concurrently.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_data_pipeline",
      "module_path": "async/pipeline.py",
      "function_signature": "async def async_data_pipeline(source: AsyncIterator[T], transformers: list[Transformer[Any, Any]], sink: Sink[Any]) -> PipelineStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Build async data processing pipeline.\n\nStreams data through a series of transformations from source to sink\nwith backpressure handling.\n\nArgs:\n    source: Async iterator producing input data.\n    transformers: Ordered list of data transformers.\n    sink: Final destination for processed data.\n\nReturns:\n    PipelineStats with processing metrics.\n\nRaises:\n    PipelineError: If any stage fails.\n    ValueError: If transformers list is empty.\n\nExample:\n    >>> async def read_logs() -> AsyncIterator[str]:\n    ...     async for line in tail_file('/var/log/app.log'):\n    ...         yield line\n    ...\n    >>> transformers = [\n    ...     ParseJsonTransformer(),\n    ...     FilterErrorsTransformer(min_level='ERROR'),\n    ...     EnrichWithMetadataTransformer(),\n    ...     FormatForStorageTransformer()\n    ... ]\n    >>> sink = ElasticsearchSink(index='app-errors')\n    >>> stats = await async_data_pipeline(\n    ...     read_logs(),\n    ...     transformers,\n    ...     sink\n    ... )\n    >>> print(f'Processed {stats.total_records} records')\n\nNote:\n    Pipeline handles backpressure to prevent memory overflow.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_distributed_lock",
      "module_path": "async/distributed.py",
      "function_signature": "async def async_distributed_lock(resource_id: str, ttl: float = 30.0, redis_client: AsyncRedis | None = None) -> AsyncContextManager[bool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Acquire distributed lock using Redis for coordination.\n\nEnsures only one process can access a resource across multiple\nservers or instances.\n\nArgs:\n    resource_id: Unique identifier for the resource to lock.\n    ttl: Lock time-to-live in seconds. Defaults to 30.0.\n    redis_client: Optional Redis client. Uses default if None.\n\nYields:\n    bool: True if lock acquired, False otherwise.\n\nRaises:\n    LockError: If lock mechanism fails.\n    ConnectionError: If Redis connection fails.\n\nExample:\n    >>> async with async_distributed_lock('user:123:profile', ttl=60.0) as locked:\n    ...     if locked:\n    ...         # Exclusive access to update user profile\n    ...         await update_user_profile(123, new_data)\n    ...     else:\n    ...         # Another process has the lock\n    ...         raise ConcurrentModificationError('Profile is being updated')\n\nNote:\n    Lock is automatically released when context exits or TTL expires.\n    Uses Redis SET with NX and EX options for atomic acquisition.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_websocket_handler",
      "module_path": "async/websocket.py",
      "function_signature": "async def async_websocket_handler(websocket: WebSocket, message_handler: Callable[[Message], Awaitable[Response]], heartbeat_interval: float = 30.0) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Handle WebSocket connection with async message processing.\n\nManages WebSocket lifecycle including heartbeat, message handling,\nand graceful disconnection.\n\nArgs:\n    websocket: Active WebSocket connection.\n    message_handler: Async function to process incoming messages.\n    heartbeat_interval: Seconds between heartbeat pings.\n        Defaults to 30.0.\n\nRaises:\n    WebSocketError: If connection fails.\n    ValueError: If heartbeat_interval <= 0.\n\nExample:\n    >>> async def handle_chat_message(msg: Message) -> Response:\n    ...     if msg.type == 'chat':\n    ...         # Broadcast to other users\n    ...         await broadcast_message(msg.data)\n    ...         return Response(status='delivered')\n    ...     elif msg.type == 'typing':\n    ...         await notify_typing(msg.sender)\n    ...         return Response(status='ok')\n    ...\n    >>> @app.websocket('/ws/chat')\n    ... async def chat_endpoint(websocket: WebSocket):\n    ...     await websocket.accept()\n    ...     await async_websocket_handler(\n    ...         websocket,\n    ...         handle_chat_message\n    ...     )\n\nNote:\n    Automatically handles ping/pong for connection keep-alive.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_queue_consumer",
      "module_path": "async/queue.py",
      "function_signature": "async def async_queue_consumer(queue_name: str, handler: Callable[[QueueMessage], Awaitable[None]], max_workers: int = 5, auto_ack: bool = True) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Consume messages from async queue with concurrent processing.\n\nContinuously pulls messages from queue and processes them using\nmultiple concurrent workers.\n\nArgs:\n    queue_name: Name of the queue to consume from.\n    handler: Async function to process each message.\n    max_workers: Maximum concurrent message processors.\n        Defaults to 5.\n    auto_ack: Whether to auto-acknowledge messages.\n        Defaults to True.\n\nRaises:\n    QueueError: If queue connection fails.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def process_order(message: QueueMessage) -> None:\n    ...     order = json.loads(message.body)\n    ...     await validate_order(order)\n    ...     await charge_payment(order['payment_info'])\n    ...     await ship_order(order['items'])\n    ...     if not message.auto_acked:\n    ...         await message.ack()\n    ...\n    >>> # Run consumer in background\n    >>> consumer_task = asyncio.create_task(\n    ...     async_queue_consumer(\n    ...         'order_queue',\n    ...         process_order,\n    ...         max_workers=10\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Failed messages are requeued.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_parallel_aggregate",
      "module_path": "async/aggregation.py",
      "function_signature": "async def async_parallel_aggregate(data_sources: list[DataSource], aggregator: Aggregator, timeout_per_source: float = 10.0) -> AggregateResult",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Aggregate data from multiple async sources in parallel.\n\nFetches data from multiple sources concurrently and combines\nresults using specified aggregation strategy.\n\nArgs:\n    data_sources: List of async data sources to query.\n    aggregator: Strategy for combining results.\n    timeout_per_source: Max seconds per source.\n        Defaults to 10.0.\n\nReturns:\n    AggregateResult containing combined data and metadata.\n\nRaises:\n    AggregationError: If aggregation fails.\n    PartialResultError: If some sources fail but partial_ok=True.\n\nExample:\n    >>> sources = [\n    ...     MetricsAPISource('cpu_usage'),\n    ...     MetricsAPISource('memory_usage'),\n    ...     MetricsAPISource('disk_usage')\n    ... ]\n    >>> aggregator = AverageAggregator(time_window='5m')\n    >>> result = await async_parallel_aggregate(\n    ...     sources,\n    ...     aggregator,\n    ...     timeout_per_source=5.0\n    ... )\n    >>> print(f'System health score: {result.value}')\n    >>> if result.failed_sources:\n    ...     print(f'Warning: {len(result.failed_sources)} sources failed')\n\nNote:\n    Failed sources don't stop aggregation if aggregator supports partial results.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_scheduled_task",
      "module_path": "async/scheduler.py",
      "function_signature": "async def async_scheduled_task(task_func: Callable[[], Awaitable[None]], schedule: str | CronSchedule, timezone: str = 'UTC', max_instances: int = 1) -> ScheduledTask",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Schedule async task execution with cron-like syntax.\n\nRuns async tasks on a schedule with instance limiting to prevent\noverlapping executions.\n\nArgs:\n    task_func: Async function to execute on schedule.\n    schedule: Cron expression or CronSchedule object.\n        Examples: '0 * * * *' (hourly), '*/5 * * * *' (every 5 min).\n    timezone: Timezone for schedule. Defaults to 'UTC'.\n    max_instances: Maximum concurrent executions.\n        Defaults to 1.\n\nReturns:\n    ScheduledTask instance for management.\n\nRaises:\n    ScheduleError: If schedule syntax is invalid.\n    ValueError: If max_instances < 1.\n\nExample:\n    >>> async def cleanup_old_files():\n    ...     cutoff = datetime.now() - timedelta(days=7)\n    ...     async for file in scan_temp_directory():\n    ...         if file.modified < cutoff:\n    ...             await file.delete()\n    ...\n    >>> # Run cleanup daily at 2 AM\n    >>> task = await async_scheduled_task(\n    ...     cleanup_old_files,\n    ...     schedule='0 2 * * *',\n    ...     timezone='America/New_York'\n    ... )\n    >>> # Task now runs automatically\n    >>> # Stop scheduling\n    >>> await task.stop()\n\nNote:\n    Task continues running until explicitly stopped.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    }
  ]
}
