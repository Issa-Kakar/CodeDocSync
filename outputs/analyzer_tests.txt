============================= test session starts =============================
platform win32 -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0 -- C:\Users\issak\CodeDocSync\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\issak\CodeDocSync
configfile: pyproject.toml
plugins: anyio-4.9.0, asyncio-1.1.0, mock-3.14.1
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 31 items

tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_behavioral_consistency_check ERROR [  3%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_semantic_parameter_analysis ERROR [  6%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_return_value_semantic_check ERROR [  9%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_exception_flow_analysis ERROR [ 12%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_example_correctness_check ERROR [ 16%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_deprecation_consistency ERROR [ 19%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_llm_retry_logic ERROR [ 22%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_rate_limit_handling ERROR [ 25%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_cache_identical_analyses ERROR [ 29%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_fallback_to_rules_on_llm_failure ERROR [ 32%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_analysis_time_under_2s_per_function ERROR [ 35%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_cache_effectiveness_above_80_percent ERROR [ 38%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_batch_analysis_with_concurrency ERROR [ 41%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_circuit_breaker_protection ERROR [ 45%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_configuration_validation FAILED [ 48%]
tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_performance_monitoring ERROR [ 51%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_parameter_name_mismatch PASSED [ 54%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_parameter_count_mismatch PASSED [ 58%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_parameter_type_mismatch PASSED [ 61%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_return_type_mismatch PASSED [ 64%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_missing_raises_documentation PASSED [ 67%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_undocumented_exceptions PASSED [ 70%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_behavioral_description_accuracy PASSED [ 74%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_example_code_validity PASSED [ 77%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_version_deprecation_info PASSED [ 80%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_missing_params PASSED [ 83%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_parameter_order_different PASSED [ 87%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_optional_parameters PASSED [ 90%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_analyze_single_function_under_5ms PASSED [ 93%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_analyze_100_functions_under_500ms PASSED [ 96%]
tests/analyzer/test_rule_engine.py::TestRuleEngine::test_zero_false_positives_for_perfect_match PASSED [100%]

=================================== ERRORS ====================================
_____ ERROR at setup of TestLLMAnalyzer.test_behavioral_consistency_check _____

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F4E5950>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_____ ERROR at setup of TestLLMAnalyzer.test_semantic_parameter_analysis ______

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F4E5E50>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_____ ERROR at setup of TestLLMAnalyzer.test_return_value_semantic_check ______

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F515BA0>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_______ ERROR at setup of TestLLMAnalyzer.test_exception_flow_analysis ________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F515E00>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
______ ERROR at setup of TestLLMAnalyzer.test_example_correctness_check _______

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F4D7770>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_______ ERROR at setup of TestLLMAnalyzer.test_deprecation_consistency ________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F265F20>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
___________ ERROR at setup of TestLLMAnalyzer.test_llm_retry_logic ____________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F266250>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_________ ERROR at setup of TestLLMAnalyzer.test_rate_limit_handling __________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F568550>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_______ ERROR at setup of TestLLMAnalyzer.test_cache_identical_analyses _______

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F568750>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
___ ERROR at setup of TestLLMAnalyzer.test_fallback_to_rules_on_llm_failure ___

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F564500>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_ ERROR at setup of TestLLMAnalyzer.test_analysis_time_under_2s_per_function __

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F5646E0>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
_ ERROR at setup of TestLLMAnalyzer.test_cache_effectiveness_above_80_percent _

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F571FD0>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
___ ERROR at setup of TestLLMAnalyzer.test_batch_analysis_with_concurrency ____

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F5725F0>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
______ ERROR at setup of TestLLMAnalyzer.test_circuit_breaker_protection ______

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F52F790>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
________ ERROR at setup of TestLLMAnalyzer.test_performance_monitoring ________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F2768D0>

    @pytest.fixture
    def llm_config(self) -> LLMConfig:
        """Create a test LLM configuration."""
>       return LLMConfig(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=1000,
            timeout_seconds=30,
            max_retries=3,
            cache_ttl_days=7,
            requests_per_second=10,
            burst_size=20,
        )

tests\analyzer\test_llm_analyzer.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=1000, timeout_seconds=30, max_retries=3, cache_ttl_days=7, requests_per_second=10, burst_size=20, max_context_tokens=2000, max_functions_in_context=3)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
================================== FAILURES ===================================
________________ TestLLMAnalyzer.test_configuration_validation ________________

self = <test_llm_analyzer.TestLLMAnalyzer object at 0x0000018D7F276750>
mock_env = None

    @pytest.mark.asyncio
    async def test_configuration_validation(self, mock_env: Any) -> None:
        """Test configuration validation and error handling."""
        # Test with invalid configuration
        with pytest.raises(ValueError):
            config = LLMConfig(
                model="invalid-model", temperature=2.0
            )  # Invalid temperature

        # Test without API key
        with patch.dict(os.environ, {}, clear=True):
            with pytest.raises(LLMAPIKeyError):
                config = LLMConfig()

        # Test with valid configuration
>       config = LLMConfig.create_fast_config()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\analyzer\test_llm_analyzer.py:1135:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
codedocsync\analyzer\llm_config.py:147: in create_fast_config
    return cls(
<string>:14: in __init__
    ???
codedocsync\analyzer\llm_config.py:49: in __post_init__
    self._validate_api_key()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LLMConfig(provider='openai', model='gpt-4o-mini', temperature=0.0, max_tokens=500, timeout_seconds=15, max_retries=1, cache_ttl_days=7, requests_per_second=15.0, burst_size=20, max_context_tokens=1000, max_functions_in_context=1)

    def _validate_api_key(self) -> None:
        """Validate that required API key is available."""
        if self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise LLMAPIKeyError(
                    "OPENAI_API_KEY environment variable is required for OpenAI provider"
                )
            if not api_key.startswith("sk-"):
>               raise LLMAPIKeyError(
                    "OPENAI_API_KEY must start with 'sk-', invalid format detected"
                )
E               codedocsync.analyzer.llm_errors.LLMAPIKeyError: OPENAI_API_KEY must start with 'sk-', invalid format detected

codedocsync\analyzer\llm_config.py:140: LLMAPIKeyError
=========================== short test summary info ===========================
FAILED tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_configuration_validation
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_behavioral_consistency_check
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_semantic_parameter_analysis
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_return_value_semantic_check
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_exception_flow_analysis
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_example_correctness_check
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_deprecation_consistency
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_llm_retry_logic
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_rate_limit_handling
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_cache_identical_analyses
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_fallback_to_rules_on_llm_failure
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_analysis_time_under_2s_per_function
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_cache_effectiveness_above_80_percent
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_batch_analysis_with_concurrency
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_circuit_breaker_protection
ERROR tests/analyzer/test_llm_analyzer.py::TestLLMAnalyzer::test_performance_monitoring
=================== 1 failed, 15 passed, 15 errors in 2.57s ===================
