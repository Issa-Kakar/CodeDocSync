{
  "version": "1.0",
  "examples": [
    {
      "function_name": "get_user",
      "module_path": "api/endpoints.py",
      "function_signature": "def get_user(user_id: str, include_details: bool = False) -> dict[str, Any]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retrieve user information from the database.\n\nArgs:\n    user_id (str): Unique identifier for the user to retrieve.\n    include_details (bool): Whether to include additional user details\n        such as preferences and activity history. Defaults to False.\n\nReturns:\n    dict[str, Any]: User information including id, name, email, and\n        optionally detailed information if requested.\n\nRaises:\n    ValueError: If user_id is empty or invalid format.\n    UserNotFoundError: If no user exists with the given id.\n    DatabaseConnectionError: If unable to connect to the database.\n\nExample:\n    >>> user = get_user(\"12345\")\n    >>> print(user['name'])\n    'John Doe'\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_data",
      "module_path": "async/client.py",
      "function_signature": "async def fetch_data(url: str, timeout: float = 30.0, retries: int = 3) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Asynchronously fetch data from the specified URL.\n\nArgs:\n    url (str): The URL to fetch data from. Must be a valid HTTP/HTTPS URL.\n    timeout (float): Maximum time to wait for response in seconds.\n        Defaults to 30.0.\n    retries (int): Number of retry attempts for failed requests.\n        Defaults to 3.\n\nReturns:\n    Response: The response object containing status code, headers,\n        and body content.\n\nRaises:\n    InvalidURLError: If the URL format is invalid.\n    TimeoutError: If the request exceeds the timeout duration.\n    ConnectionError: If unable to establish connection after all retries.\n\nExample:\n    >>> response = await fetch_data(\"https://api.example.com/data\")\n    >>> data = response.json()\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "database_connection",
      "module_path": "db/context.py",
      "function_signature": "@contextmanager\ndef database_connection(host: str, port: int = 5432, database: str = \"main\") -> Connection",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Context manager for database connections.\n\nArgs:\n    host (str): Database server hostname or IP address.\n    port (int): Port number for database connection. Defaults to 5432.\n    database (str): Name of the database to connect to. Defaults to \"main\".\n\nYields:\n    Connection: Active database connection object that is automatically\n        closed when exiting the context.\n\nRaises:\n    ConnectionError: If unable to establish database connection.\n    AuthenticationError: If credentials are invalid.\n\nExample:\n    >>> with database_connection(\"localhost\") as conn:\n    ...     cursor = conn.cursor()\n    ...     cursor.execute(\"SELECT * FROM users\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "retry",
      "module_path": "utils/decorators.py",
      "function_signature": "def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0) -> Callable",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Decorator to retry function execution on failure.\n\nArgs:\n    max_attempts (int): Maximum number of retry attempts. Defaults to 3.\n    delay (float): Initial delay between retries in seconds. Defaults to 1.0.\n    backoff (float): Multiplier for delay after each retry. Defaults to 2.0.\n\nReturns:\n    Callable: Decorated function that will retry on exceptions.\n\nExample:\n    >>> @retry(max_attempts=5, delay=0.5)\n    ... def unstable_network_call():\n    ...     return requests.get(\"https://api.example.com\")\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "train_model",
      "module_path": "ml/training.py",
      "function_signature": "def train_model(X: np.ndarray, y: np.ndarray, model_type: str = \"random_forest\", **kwargs) -> Model",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Train a machine learning model on the provided dataset.\n\nParameters\n----------\nX : np.ndarray\n    Feature matrix with shape (n_samples, n_features).\ny : np.ndarray\n    Target values with shape (n_samples,) for regression\n    or classification.\nmodel_type : str, optional\n    Type of model to train. Options are 'random_forest',\n    'gradient_boosting', 'neural_network'. Default is 'random_forest'.\n**kwargs : dict\n    Additional keyword arguments passed to the model constructor.\n    Common options include n_estimators, max_depth, learning_rate.\n\nReturns\n-------\nModel\n    Trained model instance with fit() and predict() methods.\n\nRaises\n------\nValueError\n    If X and y have incompatible shapes.\nNotImplementedError\n    If model_type is not supported.\n\nExamples\n--------\n>>> X_train = np.random.randn(100, 10)\n>>> y_train = np.random.randint(0, 2, 100)\n>>> model = train_model(X_train, y_train, model_type='random_forest')\n>>> predictions = model.predict(X_test)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated"
    },
    {
      "function_name": "fetch_multiple_urls",
      "module_path": "async/batch_client.py",
      "function_signature": "async def fetch_multiple_urls(urls: list[str], max_concurrent: int = 10) -> list[Result[dict[str, Any]]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Fetch multiple URLs concurrently with rate limiting.\n\nUses asyncio.Semaphore to limit concurrent requests and prevent\noverwhelming target servers.\n\nArgs:\n    urls: List of URLs to fetch. Each must be a valid HTTP/HTTPS URL.\n    max_concurrent: Maximum number of concurrent requests.\n        Defaults to 10. Must be between 1 and 100.\n\nReturns:\n    List of Result objects in the same order as input URLs.\n    Each Result contains either successful response data or error details.\n    Failed requests will have Result.is_error = True.\n\nRaises:\n    ValueError: If max_concurrent is outside valid range.\n    TypeError: If urls is not a list of strings.\n\nExample:\n    >>> urls = ['https://api1.com/data', 'https://api2.com/info']\n    >>> results = await fetch_multiple_urls(urls, max_concurrent=5)\n    >>> for url, result in zip(urls, results):\n    ...     if result.is_error:\n    ...         print(f'{url} failed: {result.error}')\n    ...     else:\n    ...         print(f'{url} returned: {len(result.data)} bytes')\n\nNote:\n    Connection timeout is 30 seconds per request.\n    Failed requests are not retried automatically.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "process_items_concurrently",
      "module_path": "async/processor.py",
      "function_signature": "async def process_items_concurrently(items: list[T], processor: Callable[[T], Awaitable[R]], batch_size: int = 50) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process items concurrently using asyncio.gather().\n\nSplits items into batches and processes each batch concurrently\nto avoid memory issues with large datasets.\n\nArgs:\n    items: List of items to process.\n    processor: Async function that processes a single item.\n    batch_size: Number of items to process concurrently.\n        Defaults to 50.\n\nReturns:\n    List of processed results in the same order as input items.\n\nRaises:\n    ProcessingError: If any item fails to process.\n    ValueError: If batch_size is less than 1.\n\nExample:\n    >>> async def transform_item(item: dict) -> dict:\n    ...     # Simulate async processing\n    ...     await asyncio.sleep(0.1)\n    ...     return {'id': item['id'], 'processed': True}\n    ...\n    >>> items = [{'id': i} for i in range(100)]\n    >>> results = await process_items_concurrently(items, transform_item)\n    >>> print(f'Processed {len(results)} items')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "rate_limited_api_client",
      "module_path": "async/rate_limiter.py",
      "function_signature": "async def rate_limited_api_client(endpoint: str, requests_per_second: float = 10.0) -> AsyncAPIClient",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create an API client with built-in rate limiting using Semaphore.\n\nImplements token bucket algorithm with async semaphore for\nsmooth rate limiting across concurrent requests.\n\nArgs:\n    endpoint: Base URL of the API endpoint.\n    requests_per_second: Maximum requests allowed per second.\n        Defaults to 10.0. Fractional values supported.\n\nReturns:\n    AsyncAPIClient instance with rate limiting enabled.\n\nRaises:\n    ValueError: If endpoint is invalid or requests_per_second <= 0.\n    ConnectionError: If unable to establish initial connection.\n\nExample:\n    >>> client = await rate_limited_api_client('https://api.example.com', 5.0)\n    >>> # Makes at most 5 requests per second\n    >>> tasks = [client.get(f'/user/{i}') for i in range(20)]\n    >>> results = await asyncio.gather(*tasks)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_database_pool",
      "module_path": "async/db_context.py",
      "function_signature": "@asynccontextmanager\nasync def async_database_pool(dsn: str, min_size: int = 10, max_size: int = 20) -> AsyncIterator[Pool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async context manager for database connection pool.\n\nCreates and manages a pool of database connections that can be\nreused across multiple async operations.\n\nArgs:\n    dsn: Database connection string (Data Source Name).\n    min_size: Minimum number of connections to maintain.\n        Defaults to 10.\n    max_size: Maximum number of connections allowed.\n        Defaults to 20.\n\nYields:\n    Pool: Async connection pool instance.\n\nRaises:\n    ConnectionError: If unable to create initial connections.\n    ValueError: If min_size > max_size.\n\nExample:\n    >>> async with async_database_pool('postgresql://localhost/mydb') as pool:\n    ...     async with pool.acquire() as conn:\n    ...         result = await conn.fetch('SELECT * FROM users')\n    ...         print(f'Found {len(result)} users')\n\nNote:\n    Pool is automatically closed when exiting context.\n    Connections are health-checked before reuse.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "stream_large_file",
      "module_path": "async/file_streaming.py",
      "function_signature": "async def stream_large_file(file_path: Path, chunk_size: int = 8192) -> AsyncIterator[bytes]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async generator for streaming large files in chunks.\n\nReads file in chunks to avoid loading entire file into memory,\nuseful for processing multi-GB files.\n\nArgs:\n    file_path: Path to the file to stream.\n    chunk_size: Size of each chunk in bytes.\n        Defaults to 8192 (8KB).\n\nYields:\n    bytes: File content in chunks.\n\nRaises:\n    FileNotFoundError: If file doesn't exist.\n    PermissionError: If file is not readable.\n    IOError: If read operation fails.\n\nExample:\n    >>> async for chunk in stream_large_file(Path('/data/large.csv')):\n    ...     # Process chunk without loading entire file\n    ...     lines = chunk.decode('utf-8').splitlines()\n    ...     for line in lines:\n    ...         await process_csv_line(line)\n\nNote:\n    File is automatically closed when generator is exhausted or cancelled.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "retry_with_exponential_backoff",
      "module_path": "async/retry_handler.py",
      "function_signature": "async def retry_with_exponential_backoff(coro: Callable[[], Awaitable[T]], max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0, exceptions: tuple[type[Exception], ...] = (Exception,)) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Retry async operation with exponential backoff on failure.\n\nImplements exponential backoff with jitter to avoid thundering herd\nproblem when multiple clients retry simultaneously.\n\nArgs:\n    coro: Coroutine function to retry (called without arguments).\n    max_retries: Maximum number of retry attempts.\n        Defaults to 3.\n    base_delay: Initial delay in seconds.\n        Defaults to 1.0.\n    max_delay: Maximum delay between retries.\n        Defaults to 60.0.\n    exceptions: Tuple of exception types to retry on.\n        Defaults to catch all exceptions.\n\nReturns:\n    Result from successful coroutine execution.\n\nRaises:\n    The last exception if all retries fail.\n    ValueError: If max_retries < 0 or delays are invalid.\n\nExample:\n    >>> async def flaky_api_call():\n    ...     response = await http_client.get('/unstable-endpoint')\n    ...     return response.json()\n    ...\n    >>> data = await retry_with_exponential_backoff(\n    ...     flaky_api_call,\n    ...     max_retries=5,\n    ...     exceptions=(ConnectionError, TimeoutError)\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "wait_for_with_timeout",
      "module_path": "async/timeout_utils.py",
      "function_signature": "async def wait_for_with_timeout(coro: Coroutine[Any, Any, T], timeout: float, fallback: T | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute coroutine with timeout using asyncio.timeout().\n\nProvides a clean interface for timeout handling with optional\nfallback value on timeout.\n\nArgs:\n    coro: Coroutine to execute with timeout.\n    timeout: Maximum seconds to wait for completion.\n    fallback: Value to return if timeout occurs.\n        If None, raises TimeoutError.\n\nReturns:\n    Result from coroutine or fallback value on timeout.\n\nRaises:\n    TimeoutError: If timeout occurs and no fallback provided.\n    Any exception raised by the coroutine.\n\nExample:\n    >>> async def slow_computation():\n    ...     await asyncio.sleep(10)\n    ...     return 42\n    ...\n    >>> # Returns -1 if computation takes more than 5 seconds\n    >>> result = await wait_for_with_timeout(\n    ...     slow_computation(),\n    ...     timeout=5.0,\n    ...     fallback=-1\n    ... )\n\nNote:\n    Uses asyncio.timeout() context manager internally (Python 3.11+).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "create_background_tasks",
      "module_path": "async/task_manager.py",
      "function_signature": "async def create_background_tasks(task_specs: list[TaskSpec], supervisor: TaskSupervisor | None = None) -> list[asyncio.Task[Any]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create and manage background tasks using asyncio.create_task().\n\nCreates tasks that run concurrently in the background with optional\nsupervision for error handling and lifecycle management.\n\nArgs:\n    task_specs: List of task specifications containing coroutine\n        and metadata.\n    supervisor: Optional task supervisor for monitoring and\n        error handling.\n\nReturns:\n    List of created asyncio.Task instances.\n\nRaises:\n    ValueError: If task_specs is empty or contains invalid specs.\n    RuntimeError: If event loop is not running.\n\nExample:\n    >>> specs = [\n    ...     TaskSpec(coro=monitor_system(), name='system_monitor'),\n    ...     TaskSpec(coro=cleanup_old_files(), name='cleanup')\n    ... ]\n    >>> tasks = await create_background_tasks(specs)\n    >>> # Tasks now run in background\n    >>> await asyncio.sleep(60)  # Let them run\n    >>> # Cancel all tasks when done\n    >>> for task in tasks:\n    ...     task.cancel()\n\nNote:\n    Tasks continue running until explicitly cancelled or completed.\n    Use TaskSupervisor for automatic restart on failure.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "event_driven_processor",
      "module_path": "async/event_processor.py",
      "function_signature": "async def event_driven_processor(event_queue: asyncio.Queue[Event], handlers: dict[str, EventHandler], stop_event: asyncio.Event | None = None) -> ProcessorStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process events asynchronously using event-driven pattern.\n\nConsumes events from queue and dispatches to appropriate handlers\nbased on event type.\n\nArgs:\n    event_queue: Async queue containing events to process.\n    handlers: Mapping of event types to handler functions.\n    stop_event: Optional event to signal processor shutdown.\n\nReturns:\n    ProcessorStats with counts of processed and failed events.\n\nRaises:\n    ValueError: If handlers dict is empty.\n    ProcessorError: If critical error occurs during processing.\n\nExample:\n    >>> queue = asyncio.Queue()\n    >>> handlers = {\n    ...     'user_signup': handle_signup,\n    ...     'order_placed': handle_order,\n    ...     'payment_received': handle_payment\n    ... }\n    >>> stop = asyncio.Event()\n    >>> # Run processor in background\n    >>> processor_task = asyncio.create_task(\n    ...     event_driven_processor(queue, handlers, stop)\n    ... )\n    >>> # Add events to queue\n    >>> await queue.put(Event(type='user_signup', data={'user_id': 123}))\n    >>> # Stop processor\n    >>> stop.set()\n    >>> stats = await processor_task\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_paginate_results",
      "module_path": "async/pagination.py",
      "function_signature": "async def async_paginate_results(fetch_page: Callable[[int], Awaitable[Page[T]]], start_page: int = 1, max_pages: int | None = None) -> AsyncIterator[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"AsyncIterator for paginating through API results.\n\nAutomatically fetches subsequent pages as iteration continues,\nhandling pagination logic transparently.\n\nArgs:\n    fetch_page: Async function that fetches a specific page number.\n    start_page: Page number to start from. Defaults to 1.\n    max_pages: Maximum number of pages to fetch.\n        None for unlimited.\n\nYields:\n    Individual items from all pages.\n\nRaises:\n    PaginationError: If page fetching fails.\n    ValueError: If start_page < 1.\n\nExample:\n    >>> async def fetch_users_page(page: int) -> Page[User]:\n    ...     response = await api_client.get(f'/users?page={page}')\n    ...     return Page(\n    ...         items=response['users'],\n    ...         has_next=response['has_next']\n    ...     )\n    ...\n    >>> async for user in async_paginate_results(fetch_users_page):\n    ...     print(f'Processing user: {user.name}')\n    ...     if user.inactive:\n    ...         await deactivate_user(user)\n\nNote:\n    Stops iteration when fetch_page returns Page with has_next=False.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_cache_with_ttl",
      "module_path": "async/caching.py",
      "function_signature": "async def async_cache_with_ttl(key: str, fetcher: Callable[[], Awaitable[T]], ttl_seconds: float = 300.0, cache: AsyncCache | None = None) -> T",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Get value from async cache or fetch if missing/expired.\n\nImplements read-through caching pattern with TTL support for\nasync operations.\n\nArgs:\n    key: Cache key to lookup.\n    fetcher: Async function to fetch value if not cached.\n    ttl_seconds: Time-to-live in seconds. Defaults to 300 (5 minutes).\n    cache: Optional cache instance. Uses default if None.\n\nReturns:\n    Cached value or freshly fetched value.\n\nRaises:\n    CacheError: If cache operation fails.\n    Any exception raised by fetcher.\n\nExample:\n    >>> async def expensive_api_call():\n    ...     response = await external_api.get('/expensive-data')\n    ...     return response.json()\n    ...\n    >>> # First call fetches from API\n    >>> data = await async_cache_with_ttl(\n    ...     'api_data',\n    ...     expensive_api_call,\n    ...     ttl_seconds=600\n    ... )\n    >>> # Subsequent calls within 10 minutes use cache\n    >>> cached_data = await async_cache_with_ttl('api_data', expensive_api_call)\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "parallel_map_async",
      "module_path": "async/parallel.py",
      "function_signature": "async def parallel_map_async(func: Callable[[T], Awaitable[R]], items: Iterable[T], max_workers: int = 10, ordered: bool = True) -> list[R]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Apply async function to items in parallel with worker limit.\n\nSimilar to map() but executes async function on multiple items\nconcurrently with controlled parallelism.\n\nArgs:\n    func: Async function to apply to each item.\n    items: Iterable of items to process.\n    max_workers: Maximum concurrent workers. Defaults to 10.\n    ordered: If True, preserve input order in results.\n        Defaults to True.\n\nReturns:\n    List of results from applying func to each item.\n\nRaises:\n    ParallelExecutionError: If any item fails to process.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def fetch_user_details(user_id: int) -> dict:\n    ...     response = await api_client.get(f'/users/{user_id}')\n    ...     return response.json()\n    ...\n    >>> user_ids = [101, 102, 103, 104, 105]\n    >>> details = await parallel_map_async(\n    ...     fetch_user_details,\n    ...     user_ids,\n    ...     max_workers=3\n    ... )\n    >>> print(f'Fetched details for {len(details)} users')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_circuit_breaker",
      "module_path": "async/resilience.py",
      "function_signature": "async def async_circuit_breaker(func: Callable[..., Awaitable[T]], failure_threshold: int = 5, timeout: float = 60.0, half_open_max_calls: int = 1) -> Callable[..., Awaitable[T]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Wrap async function with circuit breaker pattern.\n\nPrevents cascading failures by stopping calls to failing services\ntemporarily.\n\nArgs:\n    func: Async function to protect with circuit breaker.\n    failure_threshold: Failures before opening circuit.\n        Defaults to 5.\n    timeout: Seconds before attempting reset. Defaults to 60.0.\n    half_open_max_calls: Test calls in half-open state.\n        Defaults to 1.\n\nReturns:\n    Wrapped function with circuit breaker behavior.\n\nRaises:\n    CircuitOpenError: When circuit is open and calls are rejected.\n    Original exceptions from func when circuit is closed.\n\nExample:\n    >>> @async_circuit_breaker(failure_threshold=3, timeout=30.0)\n    ... async def unstable_service_call(endpoint: str) -> dict:\n    ...     response = await http_client.get(endpoint)\n    ...     return response.json()\n    ...\n    >>> try:\n    ...     result = await unstable_service_call('/api/data')\n    ... except CircuitOpenError:\n    ...     # Use fallback when circuit is open\n    ...     result = get_cached_data()\n\nNote:\n    Circuit states: CLOSED (normal) -> OPEN (failing) -> HALF_OPEN (testing).\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_pub_sub",
      "module_path": "async/pubsub.py",
      "function_signature": "async def async_pub_sub(topic: str, message: Any | None = None, subscribe: bool = False) -> AsyncIterator[Message] | None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async publish-subscribe pattern implementation.\n\nAllows publishing messages to topics and subscribing to receive\nmessages asynchronously.\n\nArgs:\n    topic: Topic name for publish/subscribe.\n    message: Message to publish. None when subscribing.\n    subscribe: If True, returns subscription iterator.\n        If False, publishes message.\n\nReturns:\n    AsyncIterator[Message] when subscribing, None when publishing.\n\nRaises:\n    ValueError: If publishing with subscribe=True or vice versa.\n    TopicError: If topic doesn't exist when subscribing.\n\nExample:\n    >>> # Publisher\n    >>> await async_pub_sub('user_events', {'type': 'login', 'user_id': 123})\n    ...\n    >>> # Subscriber\n    >>> async for msg in async_pub_sub('user_events', subscribe=True):\n    ...     if msg.data['type'] == 'login':\n    ...         await track_user_login(msg.data['user_id'])\n    ...     elif msg.data['type'] == 'logout':\n    ...         await track_user_logout(msg.data['user_id'])\n\nNote:\n    Subscribers receive messages published after subscription starts.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_middleware_chain",
      "module_path": "async/middleware.py",
      "function_signature": "async def async_middleware_chain(request: Request, middlewares: list[Middleware], handler: RequestHandler) -> Response",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Execute async middleware chain for request processing.\n\nProcesses request through multiple middleware layers before\nreaching the final handler.\n\nArgs:\n    request: Incoming request to process.\n    middlewares: Ordered list of middleware to apply.\n    handler: Final request handler after all middleware.\n\nReturns:\n    Response after processing through all layers.\n\nRaises:\n    MiddlewareError: If any middleware fails.\n    ValueError: If middlewares list is empty.\n\nExample:\n    >>> middlewares = [\n    ...     AuthenticationMiddleware(),\n    ...     RateLimitMiddleware(requests_per_minute=100),\n    ...     LoggingMiddleware(),\n    ...     CompressionMiddleware()\n    ... ]\n    >>> async def api_handler(request: Request) -> Response:\n    ...     data = await process_request(request)\n    ...     return Response(data=data)\n    ...\n    >>> response = await async_middleware_chain(\n    ...     request,\n    ...     middlewares,\n    ...     api_handler\n    ... )\n\nNote:\n    Middleware can modify request before and response after handler.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_batch_processor",
      "module_path": "async/batch.py",
      "function_signature": "async def async_batch_processor(items: AsyncIterator[T], batch_size: int = 100, processor: Callable[[list[T]], Awaitable[None]] | None = None) -> BatchStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Process async iterator items in batches.\n\nAccumulates items into batches and processes them together\nfor improved efficiency.\n\nArgs:\n    items: Async iterator producing items to process.\n    batch_size: Number of items per batch. Defaults to 100.\n    processor: Optional batch processing function.\n        If None, uses default processor.\n\nReturns:\n    BatchStats with total items and batches processed.\n\nRaises:\n    BatchProcessingError: If batch processing fails.\n    ValueError: If batch_size < 1.\n\nExample:\n    >>> async def stream_records() -> AsyncIterator[Record]:\n    ...     async for record in database.stream('SELECT * FROM large_table'):\n    ...         yield record\n    ...\n    >>> async def save_batch(records: list[Record]) -> None:\n    ...     await bulk_insert(records)\n    ...\n    >>> stats = await async_batch_processor(\n    ...     stream_records(),\n    ...     batch_size=1000,\n    ...     processor=save_batch\n    ... )\n    >>> print(f'Processed {stats.total_items} items in {stats.total_batches} batches')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_resource_pool",
      "module_path": "async/resource_pool.py",
      "function_signature": "async def async_resource_pool(resource_factory: Callable[[], Awaitable[T]], min_size: int = 1, max_size: int = 10, timeout: float = 30.0) -> AsyncResourcePool[T]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create async pool for expensive resource management.\n\nManages a pool of reusable resources (connections, clients, etc.)\nwith automatic creation and cleanup.\n\nArgs:\n    resource_factory: Async function to create new resources.\n    min_size: Minimum resources to maintain. Defaults to 1.\n    max_size: Maximum resources allowed. Defaults to 10.\n    timeout: Seconds to wait for available resource.\n        Defaults to 30.0.\n\nReturns:\n    AsyncResourcePool instance for resource management.\n\nRaises:\n    PoolError: If unable to create minimum resources.\n    ValueError: If min_size > max_size or sizes < 1.\n\nExample:\n    >>> async def create_client() -> APIClient:\n    ...     client = APIClient()\n    ...     await client.connect()\n    ...     return client\n    ...\n    >>> pool = await async_resource_pool(\n    ...     create_client,\n    ...     min_size=5,\n    ...     max_size=20\n    ... )\n    >>> async with pool.acquire() as client:\n    ...     result = await client.make_request('/api/data')\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_state_machine",
      "module_path": "async/state_machine.py",
      "function_signature": "async def async_state_machine(initial_state: State, transitions: dict[tuple[State, Event], State], event_stream: AsyncIterator[Event]) -> State",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Async finite state machine for event processing.\n\nProcesses events from async stream and transitions between states\naccording to defined rules.\n\nArgs:\n    initial_state: Starting state of the machine.\n    transitions: Mapping of (state, event) tuples to next state.\n    event_stream: Async iterator of events to process.\n\nReturns:\n    Final state when event stream is exhausted.\n\nRaises:\n    InvalidTransitionError: If event has no valid transition from current state.\n    StateError: If state machine enters invalid state.\n\nExample:\n    >>> transitions = {\n    ...     (State.IDLE, Event.START): State.RUNNING,\n    ...     (State.RUNNING, Event.PAUSE): State.PAUSED,\n    ...     (State.PAUSED, Event.RESUME): State.RUNNING,\n    ...     (State.RUNNING, Event.COMPLETE): State.DONE\n    ... }\n    >>> async def job_events() -> AsyncIterator[Event]:\n    ...     yield Event.START\n    ...     await asyncio.sleep(1)\n    ...     yield Event.COMPLETE\n    ...\n    >>> final_state = await async_state_machine(\n    ...     State.IDLE,\n    ...     transitions,\n    ...     job_events()\n    ... )\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_health_monitor",
      "module_path": "async/monitoring.py",
      "function_signature": "async def async_health_monitor(services: list[Service], check_interval: float = 60.0, alert_handler: AlertHandler | None = None) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Monitor service health with async health checks.\n\nPeriodically checks health of multiple services and triggers\nalerts on failures.\n\nArgs:\n    services: List of services to monitor.\n    check_interval: Seconds between health checks.\n        Defaults to 60.0.\n    alert_handler: Optional handler for health alerts.\n\nRaises:\n    MonitoringError: If monitoring setup fails.\n    ValueError: If services list is empty or interval <= 0.\n\nExample:\n    >>> services = [\n    ...     Service(name='api', health_check=check_api_health),\n    ...     Service(name='database', health_check=check_db_health),\n    ...     Service(name='cache', health_check=check_cache_health)\n    ... ]\n    >>> async def send_alert(service: str, status: str):\n    ...     await notification_service.send(f'{service} is {status}')\n    ...\n    >>> # Run monitor in background\n    >>> monitor_task = asyncio.create_task(\n    ...     async_health_monitor(\n    ...         services,\n    ...         check_interval=30.0,\n    ...         alert_handler=send_alert\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Each service checked concurrently.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_data_pipeline",
      "module_path": "async/pipeline.py",
      "function_signature": "async def async_data_pipeline(source: AsyncIterator[T], transformers: list[Transformer[Any, Any]], sink: Sink[Any]) -> PipelineStats",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Build async data processing pipeline.\n\nStreams data through a series of transformations from source to sink\nwith backpressure handling.\n\nArgs:\n    source: Async iterator producing input data.\n    transformers: Ordered list of data transformers.\n    sink: Final destination for processed data.\n\nReturns:\n    PipelineStats with processing metrics.\n\nRaises:\n    PipelineError: If any stage fails.\n    ValueError: If transformers list is empty.\n\nExample:\n    >>> async def read_logs() -> AsyncIterator[str]:\n    ...     async for line in tail_file('/var/log/app.log'):\n    ...         yield line\n    ...\n    >>> transformers = [\n    ...     ParseJsonTransformer(),\n    ...     FilterErrorsTransformer(min_level='ERROR'),\n    ...     EnrichWithMetadataTransformer(),\n    ...     FormatForStorageTransformer()\n    ... ]\n    >>> sink = ElasticsearchSink(index='app-errors')\n    >>> stats = await async_data_pipeline(\n    ...     read_logs(),\n    ...     transformers,\n    ...     sink\n    ... )\n    >>> print(f'Processed {stats.total_records} records')\n\nNote:\n    Pipeline handles backpressure to prevent memory overflow.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_distributed_lock",
      "module_path": "async/distributed.py",
      "function_signature": "async def async_distributed_lock(resource_id: str, ttl: float = 30.0, redis_client: AsyncRedis | None = None) -> AsyncContextManager[bool]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Acquire distributed lock using Redis for coordination.\n\nEnsures only one process can access a resource across multiple\nservers or instances.\n\nArgs:\n    resource_id: Unique identifier for the resource to lock.\n    ttl: Lock time-to-live in seconds. Defaults to 30.0.\n    redis_client: Optional Redis client. Uses default if None.\n\nYields:\n    bool: True if lock acquired, False otherwise.\n\nRaises:\n    LockError: If lock mechanism fails.\n    ConnectionError: If Redis connection fails.\n\nExample:\n    >>> async with async_distributed_lock('user:123:profile', ttl=60.0) as locked:\n    ...     if locked:\n    ...         # Exclusive access to update user profile\n    ...         await update_user_profile(123, new_data)\n    ...     else:\n    ...         # Another process has the lock\n    ...         raise ConcurrentModificationError('Profile is being updated')\n\nNote:\n    Lock is automatically released when context exits or TTL expires.\n    Uses Redis SET with NX and EX options for atomic acquisition.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_websocket_handler",
      "module_path": "async/websocket.py",
      "function_signature": "async def async_websocket_handler(websocket: WebSocket, message_handler: Callable[[Message], Awaitable[Response]], heartbeat_interval: float = 30.0) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Handle WebSocket connection with async message processing.\n\nManages WebSocket lifecycle including heartbeat, message handling,\nand graceful disconnection.\n\nArgs:\n    websocket: Active WebSocket connection.\n    message_handler: Async function to process incoming messages.\n    heartbeat_interval: Seconds between heartbeat pings.\n        Defaults to 30.0.\n\nRaises:\n    WebSocketError: If connection fails.\n    ValueError: If heartbeat_interval <= 0.\n\nExample:\n    >>> async def handle_chat_message(msg: Message) -> Response:\n    ...     if msg.type == 'chat':\n    ...         # Broadcast to other users\n    ...         await broadcast_message(msg.data)\n    ...         return Response(status='delivered')\n    ...     elif msg.type == 'typing':\n    ...         await notify_typing(msg.sender)\n    ...         return Response(status='ok')\n    ...\n    >>> @app.websocket('/ws/chat')\n    ... async def chat_endpoint(websocket: WebSocket):\n    ...     await websocket.accept()\n    ...     await async_websocket_handler(\n    ...         websocket,\n    ...         handle_chat_message\n    ...     )\n\nNote:\n    Automatically handles ping/pong for connection keep-alive.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_queue_consumer",
      "module_path": "async/queue.py",
      "function_signature": "async def async_queue_consumer(queue_name: str, handler: Callable[[QueueMessage], Awaitable[None]], max_workers: int = 5, auto_ack: bool = True) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Consume messages from async queue with concurrent processing.\n\nContinuously pulls messages from queue and processes them using\nmultiple concurrent workers.\n\nArgs:\n    queue_name: Name of the queue to consume from.\n    handler: Async function to process each message.\n    max_workers: Maximum concurrent message processors.\n        Defaults to 5.\n    auto_ack: Whether to auto-acknowledge messages.\n        Defaults to True.\n\nRaises:\n    QueueError: If queue connection fails.\n    ValueError: If max_workers < 1.\n\nExample:\n    >>> async def process_order(message: QueueMessage) -> None:\n    ...     order = json.loads(message.body)\n    ...     await validate_order(order)\n    ...     await charge_payment(order['payment_info'])\n    ...     await ship_order(order['items'])\n    ...     if not message.auto_acked:\n    ...         await message.ack()\n    ...\n    >>> # Run consumer in background\n    >>> consumer_task = asyncio.create_task(\n    ...     async_queue_consumer(\n    ...         'order_queue',\n    ...         process_order,\n    ...         max_workers=10\n    ...     )\n    ... )\n\nNote:\n    Runs indefinitely until cancelled. Failed messages are requeued.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_parallel_aggregate",
      "module_path": "async/aggregation.py",
      "function_signature": "async def async_parallel_aggregate(data_sources: list[DataSource], aggregator: Aggregator, timeout_per_source: float = 10.0) -> AggregateResult",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Aggregate data from multiple async sources in parallel.\n\nFetches data from multiple sources concurrently and combines\nresults using specified aggregation strategy.\n\nArgs:\n    data_sources: List of async data sources to query.\n    aggregator: Strategy for combining results.\n    timeout_per_source: Max seconds per source.\n        Defaults to 10.0.\n\nReturns:\n    AggregateResult containing combined data and metadata.\n\nRaises:\n    AggregationError: If aggregation fails.\n    PartialResultError: If some sources fail but partial_ok=True.\n\nExample:\n    >>> sources = [\n    ...     MetricsAPISource('cpu_usage'),\n    ...     MetricsAPISource('memory_usage'),\n    ...     MetricsAPISource('disk_usage')\n    ... ]\n    >>> aggregator = AverageAggregator(time_window='5m')\n    >>> result = await async_parallel_aggregate(\n    ...     sources,\n    ...     aggregator,\n    ...     timeout_per_source=5.0\n    ... )\n    >>> print(f'System health score: {result.value}')\n    >>> if result.failed_sources:\n    ...     print(f'Warning: {len(result.failed_sources)} sources failed')\n\nNote:\n    Failed sources don't stop aggregation if aggregator supports partial results.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "async_scheduled_task",
      "module_path": "async/scheduler.py",
      "function_signature": "async def async_scheduled_task(task_func: Callable[[], Awaitable[None]], schedule: str | CronSchedule, timezone: str = 'UTC', max_instances: int = 1) -> ScheduledTask",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Schedule async task execution with cron-like syntax.\n\nRuns async tasks on a schedule with instance limiting to prevent\noverlapping executions.\n\nArgs:\n    task_func: Async function to execute on schedule.\n    schedule: Cron expression or CronSchedule object.\n        Examples: '0 * * * *' (hourly), '*/5 * * * *' (every 5 min).\n    timezone: Timezone for schedule. Defaults to 'UTC'.\n    max_instances: Maximum concurrent executions.\n        Defaults to 1.\n\nReturns:\n    ScheduledTask instance for management.\n\nRaises:\n    ScheduleError: If schedule syntax is invalid.\n    ValueError: If max_instances < 1.\n\nExample:\n    >>> async def cleanup_old_files():\n    ...     cutoff = datetime.now() - timedelta(days=7)\n    ...     async for file in scan_temp_directory():\n    ...         if file.modified < cutoff:\n    ...             await file.delete()\n    ...\n    >>> # Run cleanup daily at 2 AM\n    >>> task = await async_scheduled_task(\n    ...     cleanup_old_files,\n    ...     schedule='0 2 * * *',\n    ...     timezone='America/New_York'\n    ... )\n    >>> # Task now runs automatically\n    >>> # Stop scheduling\n    >>> await task.stop()\n\nNote:\n    Task continues running until explicitly stopped.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "async_patterns"
    },
    {
      "function_name": "update_user",
      "module_path": "api/users.py",
      "function_signature": "def update_user(user_id: str, user_data: UserUpdateSchema, current_user: User = Depends(get_current_user)) -> UserResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Update user information.\n\nPartially updates user data based on provided fields. Only non-null\nfields in user_data will be updated.\n\nArgs:\n    user_id: Unique identifier of the user to update.\n    user_data: Schema containing fields to update.\n        Optional fields: name, email, phone, preferences.\n    current_user: Authenticated user making the request.\n        Automatically injected by FastAPI.\n\nReturns:\n    UserResponse: Updated user data with all fields.\n\nRaises:\n    HTTPException: Status 404 if user not found.\n    HTTPException: Status 403 if current user lacks permission.\n    HTTPException: Status 400 if email already taken.\n    ValidationError: If user_data contains invalid fields.\n\nExample:\n    Request:\n        PATCH /users/123\n        Body: {\"name\": \"Jane Doe\", \"email\": \"jane@example.com\"}\n    \n    Response:\n        200 OK\n        {\n            \"id\": \"123\",\n            \"name\": \"Jane Doe\",\n            \"email\": \"jane@example.com\",\n            \"created_at\": \"2024-01-01T00:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "list_products",
      "module_path": "api/products.py",
      "function_signature": "def list_products(page: int = Query(1, ge=1), limit: int = Query(20, ge=1, le=100), category: str | None = Query(None), sort_by: str = Query('created_at'), order: str = Query('desc', regex='^(asc|desc)$')) -> PaginatedResponse[Product]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"List products with pagination, filtering, and sorting.\n\nReturns paginated list of products with optional filtering by category\nand configurable sorting.\n\nArgs:\n    page: Page number starting from 1. Defaults to 1.\n    limit: Number of items per page. Defaults to 20, max 100.\n    category: Optional category filter. Case-insensitive partial match.\n    sort_by: Field to sort by. Options: 'created_at', 'price', 'name'.\n        Defaults to 'created_at'.\n    order: Sort order. Must be 'asc' or 'desc'. Defaults to 'desc'.\n\nReturns:\n    PaginatedResponse containing:\n        - items: List of Product objects for current page\n        - total: Total number of products matching filters\n        - page: Current page number\n        - pages: Total number of pages\n        - has_next: Whether next page exists\n        - has_prev: Whether previous page exists\n\nRaises:\n    HTTPException: Status 400 if sort_by field is invalid.\n    ValidationError: If query parameters fail validation.\n\nExample:\n    Request:\n        GET /products?page=2&limit=10&category=electronics&sort_by=price&order=asc\n    \n    Response:\n        200 OK\n        {\n            \"items\": [{\"id\": \"123\", \"name\": \"USB Cable\", \"price\": 9.99}],\n            \"total\": 45,\n            \"page\": 2,\n            \"pages\": 5,\n            \"has_next\": true,\n            \"has_prev\": true\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "create_order",
      "module_path": "api/orders.py",
      "function_signature": "def create_order(order_data: CreateOrderSchema, user: User = Depends(get_current_user), db: Session = Depends(get_db)) -> OrderResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create a new order.\n\nValidates order data, checks inventory, calculates totals, and creates\norder with line items in a transaction.\n\nArgs:\n    order_data: Order creation data containing:\n        - items: List of {product_id, quantity} objects\n        - shipping_address: Address information\n        - payment_method: Payment method identifier\n        - coupon_code: Optional discount code\n    user: Current authenticated user creating the order.\n    db: Database session for transaction management.\n\nReturns:\n    OrderResponse with:\n        - order_id: Unique order identifier\n        - status: Order status ('pending', 'processing', etc.)\n        - total: Total amount including tax and shipping\n        - items: Detailed line items with prices\n        - estimated_delivery: Delivery date estimate\n\nRaises:\n    HTTPException: Status 400 if insufficient inventory.\n    HTTPException: Status 400 if invalid coupon code.\n    HTTPException: Status 402 if payment validation fails.\n    HTTPException: Status 422 if order validation fails.\n\nExample:\n    Request:\n        POST /orders\n        Body: {\n            \"items\": [{\"product_id\": \"ABC123\", \"quantity\": 2}],\n            \"shipping_address\": {\"street\": \"123 Main St\", \"city\": \"Boston\"},\n            \"payment_method\": \"pm_1234567890\"\n        }\n    \n    Response:\n        201 Created\n        {\n            \"order_id\": \"ORD-2024-0001\",\n            \"status\": \"pending\",\n            \"total\": 59.98,\n            \"estimated_delivery\": \"2024-01-15\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "delete_resource",
      "module_path": "api/resources.py",
      "function_signature": "def delete_resource(resource_id: UUID, soft_delete: bool = Query(True), reason: str | None = Query(None, max_length=500), admin_user: AdminUser = Depends(require_admin)) -> DeleteResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Delete a resource with soft/hard delete options.\n\nSupports both soft delete (marking as deleted) and hard delete\n(permanent removal) with audit logging.\n\nArgs:\n    resource_id: UUID of the resource to delete.\n    soft_delete: If True, marks as deleted but retains data.\n        If False, permanently removes. Defaults to True.\n    reason: Optional deletion reason for audit log.\n        Max 500 characters.\n    admin_user: Admin user performing deletion.\n        Must have delete permissions.\n\nReturns:\n    DeleteResponse containing:\n        - resource_id: Deleted resource ID\n        - deleted_at: Timestamp of deletion\n        - soft_deleted: Whether soft delete was used\n        - recoverable: Whether resource can be restored\n\nRaises:\n    HTTPException: Status 404 if resource not found.\n    HTTPException: Status 403 if user lacks delete permission.\n    HTTPException: Status 409 if resource has dependencies.\n    HTTPException: Status 410 if resource already deleted.\n\nExample:\n    Request:\n        DELETE /resources/550e8400-e29b-41d4-a716-446655440000?soft_delete=true&reason=User%20requested\n    \n    Response:\n        200 OK\n        {\n            \"resource_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n            \"deleted_at\": \"2024-01-10T15:30:00Z\",\n            \"soft_deleted\": true,\n            \"recoverable\": true\n        }\n\nNote:\n    Soft deleted resources can be restored within 30 days.\n    Hard delete requires additional confirmation header.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "upload_file",
      "module_path": "api/files.py",
      "function_signature": "async def upload_file(file: UploadFile = File(...), purpose: str = Form(...), metadata: str | None = Form(None), user: User = Depends(get_current_user)) -> FileUploadResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Upload a file with validation and metadata.\n\nHandles file upload with type validation, virus scanning, and\nmetadata storage. Supports images, documents, and data files.\n\nArgs:\n    file: File to upload. Max size 100MB.\n        Allowed types: jpg, png, pdf, csv, xlsx.\n    purpose: Purpose of upload. Options: 'avatar', 'document',\n        'import', 'attachment'.\n    metadata: Optional JSON metadata string.\n        Max 1KB when parsed.\n    user: User uploading the file.\n\nReturns:\n    FileUploadResponse with:\n        - file_id: Unique file identifier\n        - filename: Original filename\n        - size: File size in bytes\n        - content_type: MIME type\n        - url: Download URL\n        - thumbnail_url: Thumbnail URL for images\n        - expires_at: URL expiration time\n\nRaises:\n    HTTPException: Status 400 if file type not allowed.\n    HTTPException: Status 413 if file too large.\n    HTTPException: Status 422 if file is corrupted.\n    HTTPException: Status 507 if storage quota exceeded.\n\nExample:\n    Request:\n        POST /files/upload\n        Content-Type: multipart/form-data\n        \n        ------WebKitFormBoundary\n        Content-Disposition: form-data; name=\"file\"; filename=\"report.pdf\"\n        Content-Type: application/pdf\n        \n        [PDF binary data]\n        ------WebKitFormBoundary\n        Content-Disposition: form-data; name=\"purpose\"\n        \n        document\n        ------WebKitFormBoundary--\n    \n    Response:\n        201 Created\n        {\n            \"file_id\": \"file_ABC123\",\n            \"filename\": \"report.pdf\",\n            \"size\": 1048576,\n            \"content_type\": \"application/pdf\",\n            \"url\": \"https://api.example.com/files/file_ABC123/download\",\n            \"expires_at\": \"2024-01-11T00:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "search_users",
      "module_path": "api/search.py",
      "function_signature": "def search_users(q: str = Query(..., min_length=2), fields: list[str] = Query(['name', 'email']), filters: UserFilters = Depends(), limit: int = Query(20, le=100)) -> SearchResponse[UserSearchResult]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Search users with full-text search and filtering.\n\nPerforms fuzzy search across specified fields with optional filters\nand relevance scoring.\n\nArgs:\n    q: Search query string. Minimum 2 characters.\n        Supports wildcards (*) and quoted phrases.\n    fields: Fields to search in. Defaults to ['name', 'email'].\n        Available: 'name', 'email', 'bio', 'skills'.\n    filters: Additional filters injected via dependency:\n        - role: Filter by user role\n        - active: Filter by active status\n        - created_after: Users created after date\n        - department: Filter by department\n    limit: Maximum results to return. Default 20, max 100.\n\nReturns:\n    SearchResponse containing:\n        - results: List of UserSearchResult with relevance scores\n        - total_hits: Total number of matching users\n        - query_time_ms: Search execution time\n        - facets: Result counts by category\n\nRaises:\n    HTTPException: Status 400 if invalid field specified.\n    HTTPException: Status 400 if query syntax invalid.\n\nExample:\n    Request:\n        GET /users/search?q=john*&fields=name&fields=email&filters[role]=admin&limit=10\n    \n    Response:\n        200 OK\n        {\n            \"results\": [\n                {\n                    \"user\": {\"id\": \"123\", \"name\": \"John Smith\", \"email\": \"john@example.com\"},\n                    \"score\": 0.95,\n                    \"highlights\": {\"name\": \"<em>John</em> Smith\"}\n                }\n            ],\n            \"total_hits\": 3,\n            \"query_time_ms\": 12,\n            \"facets\": {\"role\": {\"admin\": 1, \"user\": 2}}\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "authenticate",
      "module_path": "api/auth.py",
      "function_signature": "def authenticate(credentials: OAuth2PasswordRequestForm = Depends(), client_info: ClientInfo = Depends(get_client_info), db: Session = Depends(get_db)) -> TokenResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Authenticate user and issue access tokens.\n\nValidates credentials, checks account status, and issues JWT tokens\nwith refresh token rotation.\n\nArgs:\n    credentials: OAuth2 form data containing:\n        - username: User's email or username\n        - password: User's password\n        - scope: Optional requested scopes\n        - grant_type: Must be 'password'\n    client_info: Client information from headers:\n        - user_agent: Client application info\n        - ip_address: Client IP for security logging\n    db: Database session for user lookup.\n\nReturns:\n    TokenResponse with:\n        - access_token: JWT access token\n        - refresh_token: Opaque refresh token\n        - token_type: Always 'bearer'\n        - expires_in: Access token lifetime in seconds\n        - scope: Granted scopes\n\nRaises:\n    HTTPException: Status 401 if credentials invalid.\n    HTTPException: Status 401 if account locked/disabled.\n    HTTPException: Status 429 if too many failed attempts.\n    HTTPException: Status 503 if auth service unavailable.\n\nExample:\n    Request:\n        POST /auth/token\n        Content-Type: application/x-www-form-urlencoded\n        \n        username=user@example.com&password=SecurePass123&grant_type=password\n    \n    Response:\n        200 OK\n        {\n            \"access_token\": \"eyJhbGciOiJIUzI1NiIs...\",\n            \"refresh_token\": \"7f3a9c2d5e1b4a8f...\",\n            \"token_type\": \"bearer\",\n            \"expires_in\": 3600,\n            \"scope\": \"read write\"\n        }\n\nNote:\n    Failed attempts are rate-limited per IP and username.\n    Refresh tokens expire after 30 days of inactivity.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "bulk_create_records",
      "module_path": "api/bulk.py",
      "function_signature": "async def bulk_create_records(records: list[RecordCreateSchema], validate_all: bool = Query(True), transaction_mode: str = Query('all_or_none'), background_tasks: BackgroundTasks = BackgroundTasks()) -> BulkCreateResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Bulk create multiple records in a single request.\n\nEfficiently creates multiple records with validation and transaction\ncontrol. Supports partial success modes.\n\nArgs:\n    records: List of records to create. Max 1000 items.\n        Each must pass individual validation.\n    validate_all: If True, validate all records before creating any.\n        If False, create valid records and report errors.\n        Defaults to True.\n    transaction_mode: Transaction behavior:\n        - 'all_or_none': Rollback if any record fails\n        - 'best_effort': Create valid records, skip invalid\n        Defaults to 'all_or_none'.\n    background_tasks: FastAPI background tasks for post-processing.\n\nReturns:\n    BulkCreateResponse containing:\n        - created: List of successfully created record IDs\n        - failed: List of failed records with error details\n        - total_processed: Number of records processed\n        - transaction_id: Unique ID for this bulk operation\n\nRaises:\n    HTTPException: Status 400 if validate_all=True and any validation fails.\n    HTTPException: Status 413 if more than 1000 records.\n    HTTPException: Status 507 if database capacity exceeded.\n\nExample:\n    Request:\n        POST /records/bulk?validate_all=false&transaction_mode=best_effort\n        Body: [\n            {\"name\": \"Record 1\", \"value\": 100},\n            {\"name\": \"Record 2\", \"value\": \"invalid\"},\n            {\"name\": \"Record 3\", \"value\": 300}\n        ]\n    \n    Response:\n        207 Multi-Status\n        {\n            \"created\": [\"id_1\", \"id_3\"],\n            \"failed\": [\n                {\n                    \"index\": 1,\n                    \"error\": \"value must be numeric\",\n                    \"record\": {\"name\": \"Record 2\", \"value\": \"invalid\"}\n                }\n            ],\n            \"total_processed\": 3,\n            \"transaction_id\": \"bulk_20240110_abc123\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "get_user_profile",
      "module_path": "api/profiles.py",
      "function_signature": "def get_user_profile(user_id: int = Path(..., ge=1), include_stats: bool = Query(False), cache_control: str | None = Header(None)) -> UserProfileResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Get detailed user profile by ID.\n\nRetrieves user profile with optional statistics and proper cache headers.\n\nArgs:\n    user_id: User ID to retrieve. Must be positive integer.\n    include_stats: Whether to include usage statistics.\n        Adds extra database queries. Defaults to False.\n    cache_control: Client cache control header.\n        Respects 'no-cache' and 'max-age' directives.\n\nReturns:\n    UserProfileResponse with:\n        - user: Core user information\n        - profile: Extended profile data\n        - stats: Usage statistics (if requested)\n        - last_updated: Profile modification timestamp\n\nRaises:\n    HTTPException: Status 404 if user not found.\n    HTTPException: Status 403 if profile is private.\n\nExample:\n    Request:\n        GET /users/123/profile?include_stats=true\n        Headers:\n            Cache-Control: max-age=300\n    \n    Response:\n        200 OK\n        Headers:\n            Cache-Control: public, max-age=300\n            ETag: \"33a64df551\"\n            Last-Modified: Mon, 10 Jan 2024 10:00:00 GMT\n        Body:\n        {\n            \"user\": {\"id\": 123, \"username\": \"johndoe\"},\n            \"profile\": {\"bio\": \"Software developer\", \"location\": \"NYC\"},\n            \"stats\": {\"posts_count\": 42, \"followers_count\": 150},\n            \"last_updated\": \"2024-01-10T10:00:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "update_settings",
      "module_path": "api/settings.py",
      "function_signature": "def update_settings(settings: SettingsUpdateSchema, setting_type: str = Path(..., regex='^(user|app|notification)$'), user: User = Depends(get_current_user), if_match: str | None = Header(None)) -> SettingsResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Update user or application settings.\n\nApplies partial updates to settings with optimistic locking support\nvia ETag headers.\n\nArgs:\n    settings: Settings to update. Only provided fields are modified.\n        Supports nested updates via dot notation.\n    setting_type: Type of settings to update.\n        Must be 'user', 'app', or 'notification'.\n    user: Current authenticated user.\n    if_match: Optional ETag for optimistic locking.\n        Prevents concurrent modification conflicts.\n\nReturns:\n    SettingsResponse containing:\n        - settings: Updated settings object\n        - version: New version number\n        - etag: New ETag for future updates\n        - applied_changes: List of fields that were modified\n\nRaises:\n    HTTPException: Status 400 if invalid setting key.\n    HTTPException: Status 409 if ETag mismatch (concurrent update).\n    HTTPException: Status 422 if setting value fails validation.\n\nExample:\n    Request:\n        PUT /settings/notification\n        Headers:\n            If-Match: \"a3c9f021\"\n        Body:\n        {\n            \"email_notifications\": true,\n            \"notification_frequency\": \"daily\",\n            \"channels.slack.enabled\": false\n        }\n    \n    Response:\n        200 OK\n        Headers:\n            ETag: \"b4d8e132\"\n        Body:\n        {\n            \"settings\": {\n                \"email_notifications\": true,\n                \"notification_frequency\": \"daily\",\n                \"channels\": {\"slack\": {\"enabled\": false}, \"email\": {\"enabled\": true}}\n            },\n            \"version\": 5,\n            \"etag\": \"b4d8e132\",\n            \"applied_changes\": [\"email_notifications\", \"notification_frequency\", \"channels.slack.enabled\"]\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "stream_events",
      "module_path": "api/events.py",
      "function_signature": "async def stream_events(event_types: list[str] = Query(['*']), since: datetime | None = Query(None), user: User = Depends(get_current_user)) -> EventSourceResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Stream real-time events using Server-Sent Events (SSE).\n\nEstablishes long-lived connection for streaming events to client\nwith automatic reconnection support.\n\nArgs:\n    event_types: List of event types to subscribe to.\n        Use ['*'] for all events. Defaults to all.\n        Options: 'user_update', 'message', 'notification', 'system'.\n    since: Optional timestamp to replay events from.\n        Useful for reconnection. Max 1 hour in past.\n    user: Authenticated user receiving events.\n\nReturns:\n    EventSourceResponse streaming events in SSE format:\n        - id: Event ID for resumption\n        - event: Event type name\n        - data: JSON event payload\n        - retry: Reconnection delay in milliseconds\n\nRaises:\n    HTTPException: Status 400 if invalid event type.\n    HTTPException: Status 400 if 'since' too far in past.\n\nExample:\n    Request:\n        GET /events/stream?event_types=notification&event_types=message\n        Headers:\n            Accept: text/event-stream\n    \n    Response:\n        200 OK\n        Content-Type: text/event-stream\n        Cache-Control: no-cache\n        \n        id: 1704891234567\n        event: notification\n        data: {\"type\": \"mention\", \"from_user\": \"alice\", \"message\": \"Check this out!\"}\n        \n        id: 1704891235001\n        event: message\n        data: {\"chat_id\": \"room_123\", \"text\": \"Hello everyone!\", \"timestamp\": \"2024-01-10T12:00:35Z\"}\n        \n        :heartbeat\n\nNote:\n    Sends ':heartbeat' every 30 seconds to keep connection alive.\n    Client should reconnect with last received ID on disconnect.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "export_data",
      "module_path": "api/export.py",
      "function_signature": "def export_data(export_format: str = Query(..., regex='^(csv|json|excel|pdf)$'), filters: ExportFilters = Depends(), columns: list[str] | None = Query(None), async_export: bool = Query(False)) -> ExportResponse | TaskResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Export data in various formats with filtering.\n\nSupports synchronous and asynchronous export with progress tracking\nfor large datasets.\n\nArgs:\n    export_format: Output format.\n        Options: 'csv', 'json', 'excel', 'pdf'.\n    filters: Export filters injected via dependency:\n        - date_from: Start date for data range\n        - date_to: End date for data range\n        - categories: List of categories to include\n        - include_archived: Whether to include archived items\n    columns: Specific columns to include in export.\n        None exports all available columns.\n    async_export: If True, returns task ID for background processing.\n        Recommended for exports over 10k records.\n\nReturns:\n    If async_export=False: ExportResponse with download URL\n    If async_export=True: TaskResponse with task tracking info\n\nRaises:\n    HTTPException: Status 400 if invalid column specified.\n    HTTPException: Status 413 if export too large for sync mode.\n    HTTPException: Status 429 if export rate limit exceeded.\n\nExample:\n    Sync Request:\n        GET /data/export?export_format=csv&filters[date_from]=2024-01-01&columns=id&columns=name\n    \n    Response:\n        200 OK\n        {\n            \"download_url\": \"https://downloads.example.com/exports/abc123.csv\",\n            \"expires_at\": \"2024-01-11T00:00:00Z\",\n            \"size_bytes\": 1048576,\n            \"row_count\": 5000\n        }\n    \n    Async Request:\n        GET /data/export?export_format=excel&async_export=true\n    \n    Response:\n        202 Accepted\n        {\n            \"task_id\": \"task_xyz789\",\n            \"status\": \"pending\",\n            \"progress_url\": \"/tasks/task_xyz789/progress\",\n            \"estimated_completion\": \"2024-01-10T12:05:00Z\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "handle_webhook",
      "module_path": "api/webhooks.py",
      "function_signature": "async def handle_webhook(webhook_id: str, request: Request, signature: str = Header(..., alias='X-Webhook-Signature'), timestamp: str = Header(..., alias='X-Webhook-Timestamp')) -> WebhookResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Handle incoming webhook notifications.\n\nValidates webhook signatures, processes payloads, and handles retries\nwith idempotency support.\n\nArgs:\n    webhook_id: Unique identifier for webhook configuration.\n    request: Raw request object containing webhook payload.\n    signature: HMAC signature for payload verification.\n        Must match expected signature.\n    timestamp: Request timestamp for replay protection.\n        Must be within 5 minutes of server time.\n\nReturns:\n    WebhookResponse with:\n        - status: Processing status ('success', 'queued', 'duplicate')\n        - message_id: Unique ID for this webhook delivery\n        - processed_at: Processing timestamp\n\nRaises:\n    HTTPException: Status 401 if signature validation fails.\n    HTTPException: Status 400 if timestamp too old (replay attack).\n    HTTPException: Status 404 if webhook_id not registered.\n    HTTPException: Status 409 if duplicate delivery (already processed).\n\nExample:\n    Request:\n        POST /webhooks/wh_payment_provider\n        Headers:\n            X-Webhook-Signature: sha256=3b5c3a8f9d2e1a0c...\n            X-Webhook-Timestamp: 1704891600\n            Content-Type: application/json\n        Body:\n        {\n            \"event\": \"payment.completed\",\n            \"payment_id\": \"pay_123\",\n            \"amount\": 99.99,\n            \"currency\": \"USD\"\n        }\n    \n    Response:\n        200 OK\n        {\n            \"status\": \"success\",\n            \"message_id\": \"msg_abc123xyz\",\n            \"processed_at\": \"2024-01-10T12:00:00Z\"\n        }\n\nNote:\n    Webhook handlers should be idempotent.\n    Failed webhooks are automatically retried with exponential backoff.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "graphql_resolver",
      "module_path": "api/graphql.py",
      "function_signature": "@strawberry.field\nasync def get_user_posts(self, first: int = 10, after: str | None = None, order_by: PostOrderBy = PostOrderBy.CREATED_AT_DESC) -> Connection[Post]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"GraphQL resolver for paginated user posts.\n\nImplements Relay-style cursor pagination with sorting options for\nefficient data fetching in GraphQL queries.\n\nArgs:\n    first: Number of posts to return. Max 100.\n        Defaults to 10.\n    after: Cursor for pagination. Opaque string from previous\n        response's endCursor.\n    order_by: Sort order for posts. Enum with options:\n        CREATED_AT_DESC, CREATED_AT_ASC, POPULARITY_DESC.\n        Defaults to CREATED_AT_DESC.\n\nReturns:\n    Connection[Post] containing:\n        - edges: List of PostEdge with node and cursor\n        - pageInfo: Pagination metadata\n            - hasNextPage: More results available\n            - hasPreviousPage: Can paginate backwards\n            - startCursor: First item's cursor\n            - endCursor: Last item's cursor\n        - totalCount: Total posts for this user\n\nRaises:\n    GraphQLError: If first > 100 or < 1.\n    GraphQLError: If after cursor is invalid.\n\nExample:\n    Query:\n        query GetUserPosts($userId: ID!, $first: Int, $after: String) {\n            user(id: $userId) {\n                posts(first: $first, after: $after, orderBy: CREATED_AT_DESC) {\n                    edges {\n                        node {\n                            id\n                            title\n                            content\n                            createdAt\n                        }\n                        cursor\n                    }\n                    pageInfo {\n                        hasNextPage\n                        endCursor\n                    }\n                    totalCount\n                }\n            }\n        }\n    \n    Variables:\n        {\n            \"userId\": \"123\",\n            \"first\": 20,\n            \"after\": \"eyJpZCI6NDV9\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "batch_update",
      "module_path": "api/batch.py",
      "function_signature": "def batch_update(updates: list[ResourceUpdate], validation_mode: str = Query('strict', regex='^(strict|lenient)$'), partial: bool = Query(True)) -> BatchUpdateResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Batch update multiple resources in a single request.\n\nEfficiently updates multiple resources with configurable validation\nand partial update support.\n\nArgs:\n    updates: List of update operations. Each contains:\n        - resource_id: ID of resource to update\n        - fields: Dictionary of fields to update\n        - version: Optional version for optimistic locking\n        Max 500 updates per request.\n    validation_mode: How to handle validation:\n        - 'strict': Fail entire batch on any validation error\n        - 'lenient': Skip invalid updates, process valid ones\n        Defaults to 'strict'.\n    partial: If True, only update provided fields.\n        If False, replace entire resource. Defaults to True.\n\nReturns:\n    BatchUpdateResponse containing:\n        - updated: List of successfully updated resource IDs\n        - failed: List of failed updates with reasons\n        - warnings: List of non-fatal issues\n        - batch_version: Version identifier for this batch\n\nRaises:\n    HTTPException: Status 400 if validation_mode='strict' and any update invalid.\n    HTTPException: Status 413 if more than 500 updates.\n    HTTPException: Status 409 if version conflicts detected.\n\nExample:\n    Request:\n        PATCH /resources/batch?validation_mode=lenient\n        Body:\n        [\n            {\n                \"resource_id\": \"res_001\",\n                \"fields\": {\"status\": \"active\", \"priority\": \"high\"},\n                \"version\": 3\n            },\n            {\n                \"resource_id\": \"res_002\",\n                \"fields\": {\"status\": \"invalid_status\"}\n            }\n        ]\n    \n    Response:\n        207 Multi-Status\n        {\n            \"updated\": [\"res_001\"],\n            \"failed\": [\n                {\n                    \"resource_id\": \"res_002\",\n                    \"reason\": \"Invalid status value\",\n                    \"code\": \"VALIDATION_ERROR\"\n                }\n            ],\n            \"warnings\": [],\n            \"batch_version\": \"batch_v4_20240110\"\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "health_check",
      "module_path": "api/health.py",
      "function_signature": "async def health_check(include_details: bool = Query(False), check_dependencies: bool = Query(True)) -> HealthCheckResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Health check endpoint for monitoring and load balancers.\n\nReturns service health status with optional detailed diagnostics\nand dependency checks.\n\nArgs:\n    include_details: If True, include detailed component status.\n        May increase response time. Defaults to False.\n    check_dependencies: If True, check external dependencies.\n        Set False for faster checks. Defaults to True.\n\nReturns:\n    HealthCheckResponse with:\n        - status: Overall status ('healthy', 'degraded', 'unhealthy')\n        - timestamp: Check timestamp\n        - version: API version\n        - uptime_seconds: Service uptime\n        - details: Component details (if requested)\n        - dependencies: External service status (if checked)\n\nRaises:\n    Never raises exceptions - returns unhealthy status instead.\n\nExample:\n    Simple check:\n        GET /health\n    \n    Response:\n        200 OK\n        {\n            \"status\": \"healthy\",\n            \"timestamp\": \"2024-01-10T12:00:00Z\",\n            \"version\": \"1.2.3\",\n            \"uptime_seconds\": 86400\n        }\n    \n    Detailed check:\n        GET /health?include_details=true&check_dependencies=true\n    \n    Response:\n        200 OK (degraded) or 503 Service Unavailable (unhealthy)\n        {\n            \"status\": \"degraded\",\n            \"timestamp\": \"2024-01-10T12:00:00Z\",\n            \"version\": \"1.2.3\",\n            \"uptime_seconds\": 86400,\n            \"details\": {\n                \"api\": \"healthy\",\n                \"database_pool\": \"healthy\",\n                \"cache\": \"degraded\",\n                \"queue\": \"healthy\"\n            },\n            \"dependencies\": {\n                \"auth_service\": \"healthy\",\n                \"payment_gateway\": \"timeout\",\n                \"email_service\": \"healthy\"\n            }\n        }\n\nNote:\n    Returns 200 for healthy/degraded, 503 for unhealthy.\n    Degraded means non-critical components have issues.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "websocket_chat",
      "module_path": "api/websocket.py",
      "function_signature": "async def websocket_chat(websocket: WebSocket, room_id: str = Path(...), token: str = Query(...)) -> None",
      "docstring_format": "google",
      "docstring_content": "\"\"\"WebSocket endpoint for real-time chat functionality.\n\nHandles bidirectional communication for chat rooms with authentication,\nmessage broadcasting, and presence tracking.\n\nArgs:\n    websocket: WebSocket connection from client.\n    room_id: Chat room identifier to join.\n    token: Authentication token for user verification.\n\nRaises:\n    WebSocketException: If authentication fails.\n    WebSocketException: If room doesn't exist or user lacks access.\n    ConnectionError: If WebSocket connection fails.\n\nExample:\n    Client connection:\n        const ws = new WebSocket('wss://api.example.com/ws/chat/room123?token=abc...');\n        \n        ws.onopen = () => {\n            // Send message\n            ws.send(JSON.stringify({\n                type: 'message',\n                content: 'Hello everyone!'\n            }));\n        };\n        \n        ws.onmessage = (event) => {\n            const data = JSON.parse(event.data);\n            switch(data.type) {\n                case 'message':\n                    console.log(`${data.user}: ${data.content}`);\n                    break;\n                case 'user_joined':\n                    console.log(`${data.user} joined the room`);\n                    break;\n                case 'user_left':\n                    console.log(`${data.user} left the room`);\n                    break;\n                case 'error':\n                    console.error(`Error: ${data.message}`);\n                    break;\n            }\n        };\n    \n    Server messages:\n        {\"type\": \"message\", \"user\": \"alice\", \"content\": \"Hi!\", \"timestamp\": \"2024-01-10T12:00:00Z\"}\n        {\"type\": \"user_joined\", \"user\": \"bob\", \"timestamp\": \"2024-01-10T12:01:00Z\"}\n        {\"type\": \"typing\", \"user\": \"alice\", \"is_typing\": true}\n        {\"type\": \"error\", \"message\": \"Rate limit exceeded\", \"code\": \"RATE_LIMIT\"}\n\nNote:\n    - Automatically sends 'user_joined' and 'user_left' events\n    - Implements rate limiting (max 100 messages/minute per user)\n    - Disconnects idle connections after 5 minutes\n    - Messages are persisted for history retrieval\n\"\"\"",
      "has_params": true,
      "has_returns": false,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "create_checkout_session",
      "module_path": "api/payments.py",
      "function_signature": "async def create_checkout_session(checkout_data: CheckoutSessionCreate, user: User = Depends(get_current_user), idempotency_key: str | None = Header(None)) -> CheckoutSessionResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create a payment checkout session.\n\nInitiates a payment flow with support for multiple payment methods\nand idempotency for safe retries.\n\nArgs:\n    checkout_data: Checkout session configuration:\n        - items: List of {product_id, quantity, price_override?}\n        - success_url: URL to redirect after success\n        - cancel_url: URL to redirect on cancellation\n        - payment_methods: Allowed methods ['card', 'bank', 'wallet']\n        - metadata: Optional key-value pairs for reference\n    user: Authenticated user creating the session.\n    idempotency_key: Optional key to prevent duplicate charges\n        on retry. Recommended for all payment operations.\n\nReturns:\n    CheckoutSessionResponse containing:\n        - session_id: Unique checkout session identifier\n        - payment_url: URL to redirect user for payment\n        - expires_at: Session expiration time (30 minutes)\n        - amount_total: Total amount including tax\n        - currency: Payment currency code\n\nRaises:\n    HTTPException: Status 400 if invalid product or pricing.\n    HTTPException: Status 402 if payment method not available.\n    HTTPException: Status 409 if idempotency key already used.\n    HTTPException: Status 503 if payment service unavailable.\n\nExample:\n    Request:\n        POST /checkout/sessions\n        Headers:\n            Idempotency-Key: order_12345_attempt_1\n        Body:\n        {\n            \"items\": [\n                {\"product_id\": \"prod_abc\", \"quantity\": 2},\n                {\"product_id\": \"prod_xyz\", \"quantity\": 1, \"price_override\": 49.99}\n            ],\n            \"success_url\": \"https://myapp.com/success?session_id={CHECKOUT_SESSION_ID}\",\n            \"cancel_url\": \"https://myapp.com/cart\",\n            \"payment_methods\": [\"card\", \"wallet\"],\n            \"metadata\": {\"order_id\": \"12345\"}\n        }\n    \n    Response:\n        201 Created\n        {\n            \"session_id\": \"cs_live_abc123xyz\",\n            \"payment_url\": \"https://checkout.example.com/pay/cs_live_abc123xyz\",\n            \"expires_at\": \"2024-01-10T12:30:00Z\",\n            \"amount_total\": 199.97,\n            \"currency\": \"USD\"\n        }\n\nNote:\n    Sessions expire after 30 minutes. Monitor webhook for completion.\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "analyze_logs",
      "module_path": "api/analytics.py",
      "function_signature": "def analyze_logs(time_range: TimeRange = Depends(), aggregations: list[str] = Query(['count', 'errors']), group_by: str | None = Query(None), filters: LogFilters = Depends()) -> AnalyticsResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Analyze application logs with aggregations and filtering.\n\nProvides real-time log analysis with various aggregation functions\nand flexible filtering options.\n\nArgs:\n    time_range: Time range for analysis (injected):\n        - start: Beginning of range\n        - end: End of range (defaults to now)\n        - timezone: Timezone for bucketing\n    aggregations: List of aggregation functions:\n        Options: 'count', 'errors', 'response_time_p50',\n        'response_time_p95', 'response_time_p99',\n        'unique_users', 'requests_per_second'.\n        Defaults to ['count', 'errors'].\n    group_by: Optional grouping field:\n        Options: 'endpoint', 'status_code', 'user_id',\n        'hour', 'day', 'error_type'.\n    filters: Log filters (injected):\n        - level: Minimum log level\n        - endpoint_prefix: Filter by endpoint\n        - status_codes: List of HTTP status codes\n        - user_ids: Specific users to analyze\n\nReturns:\n    AnalyticsResponse containing:\n        - aggregations: Dict of aggregation results\n        - time_series: Time-bucketed data (if applicable)\n        - groups: Grouped results (if group_by specified)\n        - query_time_ms: Analysis execution time\n        - total_events: Number of events analyzed\n\nRaises:\n    HTTPException: Status 400 if invalid aggregation or group_by.\n    HTTPException: Status 400 if time range exceeds 30 days.\n\nExample:\n    Request:\n        GET /logs/analyze?time_range[start]=2024-01-10T00:00:00Z&aggregations=errors&aggregations=response_time_p95&group_by=endpoint\n    \n    Response:\n        200 OK\n        {\n            \"aggregations\": {\n                \"errors\": 42,\n                \"response_time_p95\": 234.5\n            },\n            \"groups\": [\n                {\n                    \"endpoint\": \"/api/users\",\n                    \"errors\": 10,\n                    \"response_time_p95\": 150.2\n                },\n                {\n                    \"endpoint\": \"/api/orders\",\n                    \"errors\": 32,\n                    \"response_time_p95\": 450.8\n                }\n            ],\n            \"query_time_ms\": 45,\n            \"total_events\": 50000\n        }\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "refresh_token",
      "module_path": "api/auth.py",
      "function_signature": "def refresh_token(refresh_token: str = Body(...), grant_type: str = Body('refresh_token', regex='^refresh_token$')) -> TokenResponse",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Refresh access token using refresh token.\n\nExchanges a valid refresh token for a new access token and optionally\na new refresh token (rotation).\n\nArgs:\n    refresh_token: Valid refresh token from previous authentication.\n        Must not be expired or revoked.\n    grant_type: OAuth2 grant type. Must be 'refresh_token'.\n\nReturns:\n    TokenResponse with:\n        - access_token: New JWT access token\n        - refresh_token: New refresh token (if rotation enabled)\n        - token_type: Always 'bearer'\n        - expires_in: Access token lifetime in seconds\n        - scope: Granted scopes (may be subset of original)\n\nRaises:\n    HTTPException: Status 401 if refresh token invalid/expired.\n    HTTPException: Status 401 if refresh token revoked.\n    HTTPException: Status 429 if too many refresh attempts.\n\nExample:\n    Request:\n        POST /auth/refresh\n        Content-Type: application/json\n        \n        {\n            \"refresh_token\": \"7f3a9c2d5e1b4a8f...\",\n            \"grant_type\": \"refresh_token\"\n        }\n    \n    Response:\n        200 OK\n        {\n            \"access_token\": \"eyJhbGciOiJIUzI1NiIs...\",\n            \"refresh_token\": \"8g4b0d3e6f2c5b9g...\",\n            \"token_type\": \"bearer\",\n            \"expires_in\": 3600,\n            \"scope\": \"read write\"\n        }\n\nNote:\n    - Old refresh token is invalidated after successful rotation\n    - Refresh tokens have longer expiry (30 days) than access tokens\n    - Each refresh token can only be used once when rotation is enabled\n\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "rest_api_patterns"
    },
    {
      "function_name": "preprocess_features",
      "module_path": "ml/preprocessing.py",
      "function_signature": "def preprocess_features(X: pd.DataFrame, numerical_features: list[str], categorical_features: list[str], scaling_method: str = 'standard', handle_missing: str = 'mean') -> tuple[pd.DataFrame, dict[str, Any]]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Preprocess numerical and categorical features for machine learning.\n\nApplies scaling to numerical features and encoding to categorical features\nwith configurable strategies for handling missing values.\n\nParameters\n----------\nX : pd.DataFrame\n    Input features with shape (n_samples, n_features).\n    Can contain mixed numerical and categorical columns.\nnumerical_features : list[str]\n    Column names of numerical features to scale.\ncategorical_features : list[str]\n    Column names of categorical features to encode.\nscaling_method : {'standard', 'minmax', 'robust'}, default='standard'\n    Scaling method for numerical features:\n    - 'standard': Zero mean, unit variance\n    - 'minmax': Scale to [0, 1] range\n    - 'robust': Median and IQR based (outlier resistant)\nhandle_missing : {'mean', 'median', 'drop', 'zero'}, default='mean'\n    Strategy for handling missing values:\n    - 'mean': Replace with column mean (numerical only)\n    - 'median': Replace with column median\n    - 'drop': Remove rows with missing values\n    - 'zero': Replace with zero\n\nReturns\n-------\nX_preprocessed : pd.DataFrame\n    Preprocessed features with original column names preserved.\n    Categorical features are one-hot encoded with new column names.\npreprocessors : dict[str, Any]\n    Fitted preprocessor objects for transforming new data:\n    - 'scaler': Fitted scaler for numerical features\n    - 'encoder': Fitted encoder for categorical features\n    - 'feature_names': List of all output feature names\n\nRaises\n------\nValueError\n    If scaling_method or handle_missing not recognized.\n    If feature lists overlap or contain non-existent columns.\n\nExamples\n--------\n>>> data = pd.DataFrame({\n...     'age': [25, 30, None, 35],\n...     'income': [50000, 60000, 55000, None],\n...     'category': ['A', 'B', 'A', 'C']\n... })\n>>> X_processed, preprocessors = preprocess_features(\n...     data, \n...     numerical_features=['age', 'income'],\n...     categorical_features=['category'],\n...     scaling_method='standard',\n...     handle_missing='mean'\n... )\n>>> X_processed.shape\n(4, 5)  # 2 numerical + 3 one-hot encoded\n\nNotes\n-----\nPreprocessors should be saved for consistent transformation\nof test data and production inference.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "engineer_time_features",
      "module_path": "ml/feature_engineering.py",
      "function_signature": "def engineer_time_features(df: pd.DataFrame, datetime_col: str, features: list[str] = ['hour', 'dayofweek', 'month', 'quarter'], cyclical_encoding: bool = True) -> pd.DataFrame",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Extract time-based features from datetime column.\n\nCreates numerical features from datetime data with optional\ncyclical encoding for periodic patterns.\n\nArgs:\n    df: DataFrame containing datetime column.\n    datetime_col: Name of datetime column to process.\n    features: List of time features to extract.\n        Options: 'hour', 'dayofweek', 'day', 'month',\n        'quarter', 'year', 'weekofyear', 'is_weekend',\n        'is_holiday', 'hour_sin', 'hour_cos', 'month_sin',\n        'month_cos'. Defaults to common features.\n    cyclical_encoding: Whether to use sine/cosine encoding\n        for cyclical features (hour, day, month).\n        Helps models understand 23:00 is close to 00:00.\n\nReturns:\n    pd.DataFrame: Original dataframe with new time features added.\n        Feature names follow pattern: {datetime_col}_{feature_name}.\n\nRaises:\n    ValueError: If datetime_col not found or not datetime type.\n    ValueError: If unknown feature requested.\n\nExample:\n    >>> df = pd.DataFrame({\n    ...     'timestamp': pd.date_range('2024-01-01', periods=100, freq='H'),\n    ...     'value': np.random.randn(100)\n    ... })\n    >>> df_features = engineer_time_features(\n    ...     df, \n    ...     'timestamp',\n    ...     features=['hour', 'dayofweek', 'is_weekend'],\n    ...     cyclical_encoding=True\n    ... )\n    >>> df_features.columns.tolist()\n    ['timestamp', 'value', 'timestamp_hour_sin', 'timestamp_hour_cos',\n     'timestamp_dayofweek', 'timestamp_is_weekend']\n\nNote:\n    Cyclical encoding prevents discontinuity at period boundaries.\n    Consider domain knowledge when selecting features.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "train_model_with_validation",
      "module_path": "ml/training.py",
      "function_signature": "def train_model_with_validation(model: BaseEstimator, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, param_grid: dict[str, list[Any]] | None = None, scoring: str = 'accuracy', n_jobs: int = -1, verbose: int = 1) -> tuple[BaseEstimator, dict[str, Any]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Train model with hyperparameter tuning and validation.\n\nPerforms grid search over hyperparameters and returns the best model\nwith training history and metrics.\n\nArgs:\n    model: Scikit-learn compatible estimator instance.\n        Must implement fit() and predict() methods.\n    X_train: Training features with shape (n_train_samples, n_features).\n    y_train: Training labels with shape (n_train_samples,).\n    X_val: Validation features with shape (n_val_samples, n_features).\n    y_val: Validation labels with shape (n_val_samples,).\n    param_grid: Dictionary mapping parameter names to lists of values.\n        If None, trains with default parameters.\n        Example: {'n_estimators': [100, 200], 'max_depth': [10, 20]}.\n    scoring: Scoring metric for model selection.\n        Options: 'accuracy', 'f1', 'precision', 'recall',\n        'roc_auc' (binary), 'f1_macro' (multiclass).\n    n_jobs: Number of parallel jobs for grid search.\n        -1 uses all available cores.\n    verbose: Verbosity level (0=silent, 1=progress, 2=detailed).\n\nReturns:\n    best_model: Fitted model with best hyperparameters.\n    training_report: Dictionary containing:\n        - 'best_params': Optimal hyperparameters found\n        - 'best_score': Best cross-validation score\n        - 'validation_score': Score on validation set\n        - 'training_time': Total training time in seconds\n        - 'cv_results': Full grid search results\n        - 'feature_importance': If model supports it\n\nRaises:\n    ValueError: If param_grid contains invalid parameters.\n    ValueError: If scoring metric incompatible with task type.\n\nExample:\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> model = RandomForestClassifier(random_state=42)\n    >>> param_grid = {\n    ...     'n_estimators': [100, 200],\n    ...     'max_depth': [10, 20, None],\n    ...     'min_samples_split': [2, 5]\n    ... }\n    >>> best_model, report = train_model_with_validation(\n    ...     model, X_train, y_train, X_val, y_val,\n    ...     param_grid=param_grid,\n    ...     scoring='f1_macro'\n    ... )\n    >>> print(f\"Best params: {report['best_params']}\")\n    >>> print(f\"Validation F1: {report['validation_score']:.3f}\")\n\nNote:\n    Consider using RandomizedSearchCV for large parameter spaces.\n    Monitor for overfitting by comparing training and validation scores.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "calculate_classification_metrics",
      "module_path": "ml/metrics.py",
      "function_signature": "def calculate_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray | None = None, average: str = 'weighted', labels: list[int] | None = None) -> dict[str, float | np.ndarray]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Calculate comprehensive classification metrics.\n\nComputes various metrics for binary and multiclass classification\nincluding threshold-dependent and threshold-independent metrics.\n\nParameters\n----------\ny_true : np.ndarray\n    True labels with shape (n_samples,).\ny_pred : np.ndarray\n    Predicted labels with shape (n_samples,).\ny_proba : np.ndarray, optional\n    Predicted probabilities with shape (n_samples, n_classes).\n    Required for ROC-AUC and probability-based metrics.\naverage : {'micro', 'macro', 'weighted', 'binary'}, default='weighted'\n    Averaging strategy for multiclass metrics:\n    - 'micro': Global counting of TP, FP, FN\n    - 'macro': Unweighted mean of per-class metrics\n    - 'weighted': Weighted mean by class support\n    - 'binary': Only for binary classification\nlabels : list[int], optional\n    List of labels to include in metrics.\n    Default includes all labels in y_true.\n\nReturns\n-------\ndict[str, float | np.ndarray]\n    Dictionary containing:\n    - 'accuracy': Overall accuracy\n    - 'precision': Precision score(s)\n    - 'recall': Recall score(s)\n    - 'f1': F1 score(s)\n    - 'confusion_matrix': Confusion matrix array\n    - 'classification_report': Per-class metrics dict\n    - 'roc_auc': ROC-AUC score (if y_proba provided)\n    - 'pr_auc': Precision-Recall AUC (if y_proba provided)\n    - 'log_loss': Logarithmic loss (if y_proba provided)\n\nRaises\n------\nValueError\n    If average='binary' but more than 2 classes present.\n    If y_proba shape doesn't match number of classes.\n\nExamples\n--------\n>>> y_true = np.array([0, 1, 2, 2, 1, 0])\n>>> y_pred = np.array([0, 1, 2, 1, 1, 0])\n>>> y_proba = np.array([[0.9, 0.1, 0.0], [0.1, 0.8, 0.1],\n...                     [0.0, 0.2, 0.8], [0.1, 0.7, 0.2],\n...                     [0.1, 0.9, 0.0], [0.8, 0.1, 0.1]])\n>>> metrics = calculate_classification_metrics(\n...     y_true, y_pred, y_proba, average='macro'\n... )\n>>> print(f\"Accuracy: {metrics['accuracy']:.3f}\")\nAccuracy: 0.833\n>>> print(f\"Macro F1: {metrics['f1']:.3f}\")\nMacro F1: 0.822\n\nNotes\n-----\nFor imbalanced datasets, focus on precision, recall, and F1\nrather than accuracy alone.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "cross_validate_model",
      "module_path": "ml/evaluation.py",
      "function_signature": "def cross_validate_model(estimator: BaseEstimator, X: pd.DataFrame | np.ndarray, y: pd.Series | np.ndarray, cv: int = 5, scoring: str | list[str] = 'accuracy', n_jobs: int = -1) -> dict[str, np.ndarray]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Perform k-fold cross-validation on a machine learning model.\n\nEvaluates model performance using stratified k-fold for classification\nor regular k-fold for regression tasks.\n\nParameters\n----------\nestimator : BaseEstimator\n    Scikit-learn compatible estimator with fit and predict methods.\nX : pd.DataFrame or np.ndarray\n    Feature matrix with shape (n_samples, n_features).\n    Can be pandas DataFrame or numpy array.\ny : pd.Series or np.ndarray\n    Target values with shape (n_samples,).\n    Automatically stratified for classification tasks.\ncv : int, default=5\n    Number of cross-validation folds.\n    Must be at least 2.\nscoring : str or list of str, default='accuracy'\n    Scoring metric(s) to evaluate.\n    Common options: 'accuracy', 'f1', 'roc_auc', 'mse', 'r2'.\nn_jobs : int, default=-1\n    Number of parallel jobs. -1 uses all processors.\n\nReturns\n-------\ndict[str, np.ndarray]\n    Dictionary mapping metric names to arrays of scores.\n    Each array has length equal to cv parameter.\n\nRaises\n------\nValueError\n    If cv < 2 or scoring metric not recognized.\nTypeError\n    If estimator doesn't have required methods.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> rf = RandomForestClassifier(n_estimators=100)\n>>> scores = cross_validate_model(rf, X, y, cv=3)\n>>> print(f\"Mean accuracy: {scores['accuracy'].mean():.3f}\")\nMean accuracy: 0.953\n\nNotes\n-----\nFor imbalanced datasets, consider using stratified sampling\nor balanced scoring metrics like 'balanced_accuracy'.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "create_ml_pipeline",
      "module_path": "ml/pipelines.py",
      "function_signature": "def create_ml_pipeline(preprocessor: BaseEstimator | None, feature_selector: BaseEstimator | None, model: BaseEstimator, pipeline_name: str = 'ml_pipeline') -> Pipeline",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Create a scikit-learn pipeline with preprocessing and modeling steps.\n\nCombines preprocessing, feature selection, and model into a single\npipeline for consistent transformation and prediction.\n\nArgs:\n    preprocessor: Preprocessing transformer (e.g., StandardScaler,\n        ColumnTransformer). Can be None to skip preprocessing.\n    feature_selector: Feature selection transformer (e.g.,\n        SelectKBest, RFE). Can be None to use all features.\n    model: Final estimator for prediction. Must implement\n        fit and predict methods.\n    pipeline_name: Name for the pipeline. Used in string\n        representation and logging.\n\nReturns:\n    Pipeline: Scikit-learn pipeline with named steps:\n        - 'preprocessor': Data preprocessing (if provided)\n        - 'feature_selector': Feature selection (if provided)\n        - 'model': Final prediction model\n\nRaises:\n    ValueError: If model is None or doesn't have required methods.\n\nExample:\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.feature_selection import SelectKBest, f_classif\n    >>> from sklearn.svm import SVC\n    >>> \n    >>> pipeline = create_ml_pipeline(\n    ...     preprocessor=StandardScaler(),\n    ...     feature_selector=SelectKBest(f_classif, k=10),\n    ...     model=SVC(kernel='rbf', C=1.0),\n    ...     pipeline_name='svm_classifier'\n    ... )\n    >>> \n    >>> # Pipeline can be used like any estimator\n    >>> pipeline.fit(X_train, y_train)\n    >>> predictions = pipeline.predict(X_test)\n    >>> \n    >>> # Access individual steps\n    >>> selected_features = pipeline['feature_selector'].get_support()\n\nNote:\n    Pipelines ensure consistent preprocessing between training\n    and inference, preventing data leakage.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 3,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "plot_feature_importance",
      "module_path": "ml/visualization.py",
      "function_signature": "def plot_feature_importance(model: BaseEstimator, feature_names: list[str], top_n: int = 20, importance_type: str = 'default', figsize: tuple[int, int] = (10, 6), save_path: str | None = None) -> plt.Figure",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Plot feature importance from a trained model.\n\nCreates a horizontal bar plot showing the most important features\nfor tree-based models or linear models with coefficients.\n\nArgs:\n    model: Trained model with feature_importances_ or coef_ attribute.\n        Common models: RandomForest, GradientBoosting, LinearRegression.\n    feature_names: List of feature names corresponding to model inputs.\n        Length must match number of features.\n    top_n: Number of top features to display. Defaults to 20.\n        Use -1 to show all features.\n    importance_type: Type of importance to plot:\n        - 'default': Use model's default (feature_importances_ or coef_)\n        - 'permutation': Calculate permutation importance\n        - 'shap': Use SHAP values (requires shap library)\n    figsize: Figure size as (width, height) in inches.\n    save_path: Optional path to save the plot. Supports .png, .pdf, .svg.\n\nReturns:\n    plt.Figure: Matplotlib figure object for further customization.\n\nRaises:\n    AttributeError: If model lacks feature importance attributes.\n    ValueError: If feature_names length doesn't match model features.\n\nExample:\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> # Assume model is trained\n    >>> feature_names = ['age', 'income', 'education', 'experience']\n    >>> fig = plot_feature_importance(\n    ...     model,\n    ...     feature_names,\n    ...     top_n=10,\n    ...     figsize=(12, 8),\n    ...     save_path='feature_importance.png'\n    ... )\n    >>> # Customize further\n    >>> fig.suptitle('Top 10 Most Important Features', fontsize=16)\n    >>> plt.tight_layout()\n    >>> plt.show()\n\nNote:\n    For linear models, importance is absolute value of coefficients.\n    Consider using SHAP for more detailed feature explanations.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "perform_statistical_test",
      "module_path": "ml/statistics.py",
      "function_signature": "def perform_statistical_test(data1: np.ndarray, data2: np.ndarray | None = None, test_type: str = 't_test', alternative: str = 'two-sided', alpha: float = 0.05) -> dict[str, float | str | bool]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Perform statistical hypothesis testing.\n\nConducts various statistical tests for comparing distributions\nor testing assumptions about data.\n\nParameters\n----------\ndata1 : np.ndarray\n    First data sample or full dataset for single-sample tests.\ndata2 : np.ndarray, optional\n    Second data sample for two-sample tests.\n    Not required for normality or single-sample tests.\ntest_type : str, default='t_test'\n    Type of statistical test to perform:\n    - 't_test': Two-sample t-test (parametric)\n    - 'wilcoxon': Wilcoxon rank-sum test (non-parametric)\n    - 'ks_test': Kolmogorov-Smirnov test\n    - 'chi_square': Chi-square test of independence\n    - 'anova': One-way ANOVA (requires grouped data)\n    - 'normality': Shapiro-Wilk normality test\nalternative : {'two-sided', 'greater', 'less'}, default='two-sided'\n    Alternative hypothesis for applicable tests.\nalpha : float, default=0.05\n    Significance level for hypothesis testing.\n\nReturns\n-------\ndict[str, float | str | bool]\n    Test results containing:\n    - 'statistic': Test statistic value\n    - 'p_value': Probability value\n    - 'reject_null': Whether to reject null hypothesis\n    - 'interpretation': Human-readable result explanation\n    - 'effect_size': Cohen's d or other effect size (if applicable)\n    - 'confidence_interval': CI for difference (if applicable)\n\nRaises\n------\nValueError\n    If test_type not recognized or data2 required but not provided.\n    If data shapes incompatible for chosen test.\n\nExamples\n--------\n>>> # Test if two groups have different means\n>>> group1 = np.random.normal(100, 15, 50)\n>>> group2 = np.random.normal(105, 15, 50)\n>>> result = perform_statistical_test(\n...     group1, group2,\n...     test_type='t_test',\n...     alternative='two-sided'\n... )\n>>> print(f\"p-value: {result['p_value']:.4f}\")\np-value: 0.0234\n>>> print(result['interpretation'])\n'Significant difference detected (p=0.0234). Reject null hypothesis.'\n\n>>> # Test normality\n>>> data = np.random.exponential(scale=2, size=100)\n>>> result = perform_statistical_test(data, test_type='normality')\n>>> print(f\"Data is normal: {not result['reject_null']}\")\nData is normal: False\n\nNotes\n-----\nAlways check test assumptions before interpreting results.\nFor multiple comparisons, consider Bonferroni correction.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "analyze_time_series",
      "module_path": "ml/timeseries.py",
      "function_signature": "def analyze_time_series(ts_data: pd.Series, freq: str = 'D', decompose_method: str = 'additive', test_stationarity: bool = True, seasonal_period: int | None = None) -> dict[str, Any]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Analyze time series data for patterns and characteristics.\n\nPerforms decomposition, stationarity testing, and basic forecasting\nmetrics for time series data.\n\nArgs:\n    ts_data: Time series data with datetime index.\n        Values should be numeric. Index must be datetime.\n    freq: Frequency of time series.\n        Common: 'D' (daily), 'W' (weekly), 'M' (monthly),\n        'H' (hourly), 'Q' (quarterly).\n    decompose_method: Decomposition method:\n        - 'additive': Y = Trend + Seasonal + Residual\n        - 'multiplicative': Y = Trend * Seasonal * Residual\n    test_stationarity: Whether to perform ADF test for stationarity.\n    seasonal_period: Seasonal period for decomposition.\n        If None, attempts automatic detection.\n\nReturns:\n    dict[str, Any]: Analysis results containing:\n        - 'summary_stats': Basic statistics (mean, std, etc.)\n        - 'trend': Trend component array\n        - 'seasonal': Seasonal component array\n        - 'residual': Residual component array\n        - 'is_stationary': Stationarity test result\n        - 'adf_statistic': Augmented Dickey-Fuller statistic\n        - 'adf_p_value': ADF test p-value\n        - 'seasonality_strength': Measure of seasonal pattern\n        - 'trend_strength': Measure of trend pattern\n        - 'suggested_transformations': List of recommendations\n\nRaises:\n    ValueError: If ts_data doesn't have datetime index.\n    ValueError: If insufficient data for decomposition.\n\nExample:\n    >>> dates = pd.date_range('2023-01-01', periods=365, freq='D')\n    >>> values = 100 + 10*np.sin(2*np.pi*np.arange(365)/7) + np.random.randn(365)\n    >>> ts = pd.Series(values, index=dates)\n    >>> \n    >>> analysis = analyze_time_series(\n    ...     ts,\n    ...     freq='D',\n    ...     decompose_method='additive',\n    ...     seasonal_period=7\n    ... )\n    >>> \n    >>> print(f\"Stationary: {analysis['is_stationary']}\")\n    >>> print(f\"Seasonality strength: {analysis['seasonality_strength']:.2f}\")\n    >>> print(\"Suggested transforms:\", analysis['suggested_transformations'])\n\nNote:\n    For multiplicative decomposition, ensure all values are positive.\n    Consider differencing for non-stationary series.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "build_neural_network",
      "module_path": "ml/neural_nets.py",
      "function_signature": "def build_neural_network(input_shape: tuple[int, ...], output_units: int, hidden_layers: list[int], activation: str = 'relu', dropout_rate: float = 0.0, batch_norm: bool = False, optimizer: str = 'adam', loss: str = 'mse', metrics: list[str] | None = None) -> keras.Model",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Build a customizable feedforward neural network.\n\nCreates a sequential neural network with configurable architecture\nincluding hidden layers, activation functions, and regularization.\n\nParameters\n----------\ninput_shape : tuple[int, ...]\n    Shape of input data excluding batch dimension.\n    For tabular data: (n_features,)\n    For images: (height, width, channels)\noutput_units : int\n    Number of output neurons.\n    Use 1 for regression/binary classification,\n    n_classes for multiclass.\nhidden_layers : list[int]\n    List of hidden layer sizes.\n    Example: [128, 64, 32] creates 3 hidden layers.\nactivation : str, default='relu'\n    Activation function for hidden layers.\n    Options: 'relu', 'tanh', 'sigmoid', 'elu', 'leaky_relu'.\ndropout_rate : float, default=0.0\n    Dropout rate after each hidden layer.\n    Must be between 0 and 1.\nbatch_norm : bool, default=False\n    Whether to apply batch normalization after\n    each hidden layer (before activation).\noptimizer : str, default='adam'\n    Optimizer for training.\n    Options: 'adam', 'sgd', 'rmsprop', 'adamax'.\nloss : str, default='mse'\n    Loss function.\n    Options: 'mse', 'mae', 'binary_crossentropy',\n    'categorical_crossentropy', 'sparse_categorical_crossentropy'.\nmetrics : list[str], optional\n    List of metrics to track during training.\n    Default: ['accuracy'] for classification, ['mae'] for regression.\n\nReturns\n-------\nkeras.Model\n    Compiled neural network ready for training.\n    Use model.summary() to see architecture.\n\nRaises\n------\nValueError\n    If dropout_rate not in [0, 1].\n    If hidden_layers is empty.\n    If activation or optimizer not recognized.\n\nExamples\n--------\n>>> # Regression network\n>>> model = build_neural_network(\n...     input_shape=(10,),\n...     output_units=1,\n...     hidden_layers=[64, 32, 16],\n...     activation='relu',\n...     dropout_rate=0.2,\n...     loss='mse',\n...     metrics=['mae']\n... )\n>>> model.summary()\n\n>>> # Classification network with batch norm\n>>> model = build_neural_network(\n...     input_shape=(28, 28, 1),\n...     output_units=10,\n...     hidden_layers=[128, 64],\n...     activation='relu',\n...     dropout_rate=0.3,\n...     batch_norm=True,\n...     loss='categorical_crossentropy',\n...     metrics=['accuracy']\n... )\n\nNotes\n-----\nFor better performance, normalize input data before training.\nConsider using callbacks for early stopping and learning rate scheduling.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "encode_categorical_features",
      "module_path": "ml/encoding.py",
      "function_signature": "def encode_categorical_features(df: pd.DataFrame, categorical_cols: list[str], encoding_type: str = 'onehot', handle_unknown: str = 'ignore', min_frequency: int | None = None) -> tuple[pd.DataFrame, dict[str, Any]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Encode categorical variables for machine learning.\n\nTransforms categorical columns using various encoding strategies\nwith proper handling of unseen categories.\n\nArgs:\n    df: DataFrame containing features to encode.\n    categorical_cols: List of column names to encode.\n        Must be present in df.\n    encoding_type: Encoding strategy to use:\n        - 'onehot': One-hot encoding (dummy variables)\n        - 'label': Label encoding (ordinal integers)\n        - 'target': Target encoding (mean of target)\n        - 'binary': Binary encoding (bit representation)\n        - 'hashing': Feature hashing (fixed dimensionality)\n    handle_unknown: Strategy for unknown categories:\n        - 'ignore': Treat as zero vector (onehot)\n        - 'error': Raise error on unknown category\n        - 'default': Use default value (label/target)\n    min_frequency: Minimum frequency to keep category.\n        Less frequent categories grouped as 'Other'.\n\nReturns:\n    encoded_df: DataFrame with encoded features.\n        Original columns replaced with encoded versions.\n    encoders: Dictionary of fitted encoders for each column.\n        Use for transforming new data consistently.\n\nRaises:\n    ValueError: If encoding_type not recognized.\n    KeyError: If categorical_cols not found in df.\n\nExample:\n    >>> df = pd.DataFrame({\n    ...     'color': ['red', 'blue', 'red', 'green', 'blue'],\n    ...     'size': ['S', 'M', 'L', 'M', 'S'],\n    ...     'price': [10, 20, 15, 25, 12]\n    ... })\n    >>> encoded_df, encoders = encode_categorical_features(\n    ...     df,\n    ...     categorical_cols=['color', 'size'],\n    ...     encoding_type='onehot',\n    ...     min_frequency=2\n    ... )\n    >>> encoded_df.columns.tolist()\n    ['price', 'color_blue', 'color_red', 'color_Other', 'size_M', 'size_S', 'size_Other']\n\nNote:\n    One-hot encoding can create many features for high-cardinality variables.\n    Consider target encoding or hashing for such cases.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "detect_outliers",
      "module_path": "ml/outliers.py",
      "function_signature": "def detect_outliers(X: pd.DataFrame | np.ndarray, method: str = 'isolation_forest', contamination: float = 0.1, return_scores: bool = False) -> tuple[np.ndarray, np.ndarray | None]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Detect outliers in multivariate data.\n\nIdentifies anomalous data points using various outlier detection\nalgorithms suitable for different data characteristics.\n\nParameters\n----------\nX : pd.DataFrame or np.ndarray\n    Data matrix with shape (n_samples, n_features).\n    Should be scaled for distance-based methods.\nmethod : str, default='isolation_forest'\n    Outlier detection algorithm:\n    - 'isolation_forest': Tree-based anomaly detection\n    - 'lof': Local Outlier Factor (density-based)\n    - 'one_class_svm': One-Class SVM\n    - 'elliptic': Elliptic Envelope (Gaussian)\n    - 'iqr': Interquartile Range (univariate)\ncontamination : float, default=0.1\n    Expected proportion of outliers in the dataset.\n    Must be between 0 and 0.5.\nreturn_scores : bool, default=False\n    Whether to return anomaly scores along with labels.\n\nReturns\n-------\nis_outlier : np.ndarray\n    Boolean array where True indicates outlier.\n    Shape: (n_samples,)\nscores : np.ndarray, optional\n    Anomaly scores for each sample (if return_scores=True).\n    Higher scores indicate more anomalous.\n\nRaises\n------\nValueError\n    If method not recognized.\n    If contamination not in valid range.\n\nExamples\n--------\n>>> # Generate data with outliers\n>>> np.random.seed(42)\n>>> X_normal = np.random.randn(100, 2)\n>>> X_outliers = np.random.uniform(-6, 6, (10, 2))\n>>> X = np.vstack([X_normal, X_outliers])\n>>> \n>>> # Detect outliers\n>>> outliers, scores = detect_outliers(\n...     X,\n...     method='isolation_forest',\n...     contamination=0.1,\n...     return_scores=True\n... )\n>>> print(f\"Found {outliers.sum()} outliers\")\nFound 11 outliers\n>>> \n>>> # Most anomalous points\n>>> top_outliers = np.argsort(scores)[-5:]\n>>> print(\"Top 5 outlier indices:\", top_outliers)\n\nNotes\n-----\nScale features before using distance-based methods (LOF, One-Class SVM).\nIsolation Forest works well without scaling and handles mixed types.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "perform_feature_selection",
      "module_path": "ml/feature_selection.py",
      "function_signature": "def perform_feature_selection(X: pd.DataFrame, y: pd.Series, method: str = 'mutual_info', n_features: int | float = 0.5, task_type: str = 'classification') -> tuple[pd.DataFrame, list[str], dict[str, float]]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Select most informative features for machine learning.\n\nReduces feature dimensionality by selecting the most relevant\nfeatures based on various statistical and model-based methods.\n\nArgs:\n    X: Feature matrix with columns as features.\n    y: Target variable for supervised selection.\n    method: Feature selection method:\n        - 'mutual_info': Mutual information (non-linear)\n        - 'chi2': Chi-squared test (categorical features)\n        - 'f_test': ANOVA F-test (linear)\n        - 'rfe': Recursive Feature Elimination\n        - 'lasso': L1 regularization coefficients\n        - 'tree': Tree-based feature importance\n    n_features: Number of features to select.\n        If int: Select exactly n features.\n        If float (0-1): Select percentage of features.\n    task_type: Type of ML task:\n        - 'classification': For categorical targets\n        - 'regression': For continuous targets\n\nReturns:\n    X_selected: DataFrame with selected features only.\n    selected_features: List of selected feature names.\n    feature_scores: Dictionary mapping feature names to\n        importance scores (method-dependent).\n\nRaises:\n    ValueError: If method not recognized or n_features invalid.\n    ValueError: If task_type incompatible with method.\n\nExample:\n    >>> # Select top 10 features for classification\n    >>> X_selected, features, scores = perform_feature_selection(\n    ...     X_train,\n    ...     y_train,\n    ...     method='mutual_info',\n    ...     n_features=10,\n    ...     task_type='classification'\n    ... )\n    >>> print(f\"Selected features: {features}\")\n    >>> \n    >>> # Select top 50% using tree importance\n    >>> X_selected, features, scores = perform_feature_selection(\n    ...     X_train,\n    ...     y_train,\n    ...     method='tree',\n    ...     n_features=0.5,\n    ...     task_type='regression'\n    ... )\n    >>> \n    >>> # Plot feature scores\n    >>> pd.Series(scores).sort_values(ascending=False).plot(kind='bar')\n\nNote:\n    Different methods make different assumptions about data.\n    Consider trying multiple methods and comparing results.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "evaluate_regression_model",
      "module_path": "ml/regression_metrics.py",
      "function_signature": "def evaluate_regression_model(y_true: np.ndarray, y_pred: np.ndarray, sample_weights: np.ndarray | None = None, multioutput: str = 'uniform_average') -> dict[str, float]",
      "docstring_format": "numpy",
      "docstring_content": "\"\"\"Calculate comprehensive regression evaluation metrics.\n\nComputes various regression metrics to assess model performance\nfrom different perspectives including accuracy and error distribution.\n\nParameters\n----------\ny_true : np.ndarray\n    True target values with shape (n_samples,) or\n    (n_samples, n_outputs) for multi-output.\ny_pred : np.ndarray\n    Predicted values with same shape as y_true.\nsample_weights : np.ndarray, optional\n    Sample weights with shape (n_samples,).\n    If None, all samples weighted equally.\nmultioutput : {'raw_values', 'uniform_average', 'variance_weighted'}, default='uniform_average'\n    Aggregation for multi-output regression:\n    - 'raw_values': Return array of scores\n    - 'uniform_average': Simple mean of scores\n    - 'variance_weighted': Weighted by variance\n\nReturns\n-------\ndict[str, float]\n    Dictionary containing:\n    - 'mae': Mean Absolute Error\n    - 'mse': Mean Squared Error\n    - 'rmse': Root Mean Squared Error\n    - 'r2': R-squared coefficient of determination\n    - 'mape': Mean Absolute Percentage Error\n    - 'explained_variance': Explained variance score\n    - 'max_error': Maximum residual error\n    - 'median_absolute_error': Median of absolute errors\n\nRaises\n------\nValueError\n    If shapes of y_true and y_pred don't match.\n    If multioutput not recognized.\n\nExamples\n--------\n>>> # Single output regression\n>>> y_true = np.array([3.0, -0.5, 2.0, 7.0])\n>>> y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n>>> metrics = evaluate_regression_model(y_true, y_pred)\n>>> print(f\"RMSE: {metrics['rmse']:.3f}\")\nRMSE: 0.612\n>>> print(f\"R: {metrics['r2']:.3f}\")\nR: 0.948\n\n>>> # Multi-output with sample weights\n>>> y_true = np.array([[0.5, 1], [-1, 1], [7, -6]])\n>>> y_pred = np.array([[0, 2], [-1, 2], [8, -5]])\n>>> weights = np.array([0.5, 1.0, 1.0])\n>>> metrics = evaluate_regression_model(\n...     y_true, y_pred,\n...     sample_weights=weights,\n...     multioutput='variance_weighted'\n... )\n\nNotes\n-----\nMAPE can be infinite if y_true contains zeros.\nConsider using RMSE for normally distributed errors\nand MAE for data with outliers.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 4,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    },
    {
      "function_name": "tune_hyperparameters",
      "module_path": "ml/hyperparameter_tuning.py",
      "function_signature": "def tune_hyperparameters(estimator: BaseEstimator, param_distributions: dict[str, Any], X: np.ndarray, y: np.ndarray, search_method: str = 'random', n_iter: int = 50, cv: int = 5, scoring: str = 'neg_mean_squared_error', n_jobs: int = -1, random_state: int | None = None) -> tuple[BaseEstimator, dict[str, Any], pd.DataFrame]",
      "docstring_format": "google",
      "docstring_content": "\"\"\"Perform hyperparameter optimization for machine learning models.\n\nSearches for optimal hyperparameters using various search strategies\nwith cross-validation.\n\nArgs:\n    estimator: Scikit-learn compatible estimator to tune.\n    param_distributions: Search space definition.\n        For grid search: dict of lists\n        For random/bayesian: dict of distributions\n        Example: {'n_estimators': [100, 200],\n                 'max_depth': scipy.stats.randint(1, 20)}\n    X: Training features with shape (n_samples, n_features).\n    y: Training targets with shape (n_samples,).\n    search_method: Search strategy to use:\n        - 'grid': Exhaustive grid search\n        - 'random': Random sampling of parameters\n        - 'bayesian': Bayesian optimization (requires scikit-optimize)\n        - 'halving': Successive halving for resource efficiency\n    n_iter: Number of parameter settings sampled (random/bayesian).\n        Ignored for grid search.\n    cv: Number of cross-validation folds.\n    scoring: Scoring metric for optimization.\n        Classification: 'accuracy', 'f1', 'roc_auc'\n        Regression: 'neg_mean_squared_error', 'r2'\n    n_jobs: Number of parallel jobs. -1 uses all cores.\n    random_state: Random seed for reproducibility.\n\nReturns:\n    best_estimator: Model with optimal parameters fitted on all data.\n    best_params: Dictionary of best parameter values found.\n    results_df: DataFrame with all trials and scores for analysis.\n        Columns include params, mean_test_score, std_test_score,\n        rank_test_score, and timing information.\n\nRaises:\n    ValueError: If search_method not recognized.\n    ImportError: If bayesian selected but scikit-optimize not installed.\n\nExample:\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from scipy.stats import randint, uniform\n    >>> \n    >>> param_dist = {\n    ...     'n_estimators': randint(50, 500),\n    ...     'max_depth': randint(5, 50),\n    ...     'min_samples_split': randint(2, 20),\n    ...     'min_samples_leaf': randint(1, 10),\n    ...     'max_features': uniform(0.1, 0.9)\n    ... }\n    >>> \n    >>> best_model, best_params, results = tune_hyperparameters(\n    ...     RandomForestRegressor(random_state=42),\n    ...     param_dist,\n    ...     X_train, y_train,\n    ...     search_method='random',\n    ...     n_iter=100,\n    ...     scoring='neg_mean_squared_error'\n    ... )\n    >>> \n    >>> print(f\"Best RMSE: {np.sqrt(-results.loc[0, 'mean_test_score']):.3f}\")\n    >>> print(\"Best parameters:\", best_params)\n\nNote:\n    Bayesian optimization typically finds good parameters\n    with fewer iterations than random search.\"\"\"",
      "has_params": true,
      "has_returns": true,
      "has_examples": true,
      "complexity_score": 5,
      "quality_score": 5,
      "source": "curated",
      "category": "data_science_patterns"
    }
  ]
}
